{
  "generated": "2025-08-27T01:09:59.002297",
  "notebooks": [
    {
      "notebook": "pixeltable-basics.ipynb",
      "title": "Pixeltable Basics",
      "description": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/pixeltable-basics.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/pixeltable-basics.ipynb) Welcome to Pixeltable! In this tutorial, we'll survey how to create tables, populate them with data, and enhance them with built-in and user-defined transformations and AI operations. If you want to follow along with this tutorial interactively, there are two ways to go.",
      "workflows": [
        {
          "title": "Creating a Table",
          "explanation": [
            "We can use `t.describe()` to examine the table schema. We see that it now contains a single column, as expected.",
            "The new table is initially empty, with no rows:",
            "Now let's put an image into it! We can add images simply by giving Pixeltable their URLs. The example images in this demo come from the [COCO dataset](https://cocodataset.org/), and we'll be referencing copies of them in the Pixeltable github repo. But in practice, the images can come from anywhere: an S3 bucket, say, or the local file system.\n\nWhen we add the image, we see that Pixeltable gives us some useful status updates indicating that the operation was successful.",
            "We can use `t.show()` to examine the contents of the table.",
            "## Adding Computed Columns\n\nGreat! Now we have a table containing some data. Let's add an object detection model to our workflow. Specifically, we're going to use the ResNet-50 object detection model, which runs using the Huggingface DETR (\"DEtection TRansformer\") model class. Pixeltable contains a built-in adapter for this model family, so all we have to do is call the `detr_for_object_detection` Pixeltable function. A nice thing about the Huggingface models is that they run locally, so you don't need an account with a service provider in order to use them.\n\nThis is our first example of a __computed column__, a key concept in Pixeltable. Recall that when we created the `input_image` column, we specified a type, `ImageType`, indicating our intent to populate it with data in the future. When we create a _computed_ column, we instead specify a function that operates on other columns of the table. By default, when we add the new computed column, Pixeltable immediately evaluates it against all existing data in the table - in this case, by calling the `detr_for_object_detection` function on the image.\n\nDepending on your setup, it may take a minute for the function to execute. In the background, Pixeltable is downloading the model from Huggingface (if necessary), instantiating it, and caching it for later use.",
            "Let's examine the results.",
            "We see that the model returned a JSON structure containing a lot of information. In particular, it has the following fields:\n- `label_text`: Descriptions of the objects detected\n- `boxes`: Bounding boxes for each detected object\n- `scores`: Confidence scores for each detection\n- `labels`: The DETR model's internal IDs for the detected objects\n\nPerhaps this is more than we need, and all we really want are the text labels. We could add another computed column to extract `label_text` from the JSON struct:",
            "If we inspect the table schema now, we see how Pixeltable distinguishes between ordinary and computed columns.",
            "Now let's add some more images to our table. This demonstrates another important feature of computed columns: by default, they update incrementally any time new data shows up on their inputs. In this case, Pixeltable will run the ResNet-50 model against each new image that is added, then extract the labels into the `detect_text` column. Pixeltable will orchestrate the execution of any sequence (or DAG) of computed columns.\n\nNote how we can pass multiple rows to `t.insert` with a single statement, which will insert them more efficiently.",
            "Let's see what the model came up with. We'll use `t.select` to suppress the display of the `detect` column, since right now we're only interested in the text labels.",
            "## Pixeltable Is Persistent\n\nAn important feature of Pixeltable is that _everything is persistent_. Unlike in-memory Python libraries such as Pandas, Pixeltable is a database: all your data, transformations, and computed columns are stored and preserved between sessions. To see this, let's clear all the variables in our notebook and start fresh. You can optionally restart your notebook kernel at this point, to demonstrate how Pixeltable data persists across sessions.",
            "## GPT-4o\n\nFor comparison, let's try running our examples through a generative model, Open AI's `gpt-4o-mini`. For this section, you'll need an OpenAI account with an API key. You can use the following command to add your API key to the environment (just enter your API key when prompted):",
            "Now we can connect to OpenAI through Pixeltable. This may take some time, depending on how long OpenAI takes to process the query.",
            "Let's see how GPT-4's responses compare to the traditional discriminative (DETR) model.",
            "In addition to adapters for local models and inference APIs, Pixeltable can perform a range of more basic image operations. These image operations can be seamlessly chained with API calls, and Pixeltable will keep track of the sequence of operations, constructing new images and caching when necessary to keep things running smoothly. Just for fun (and to demonstrate the power of computed columns), let's see what OpenAI thinks of our sample images when we rotate them by 180 degrees.",
            "## UDFs: Enhancing Pixeltable's Capabilities\n\nAnother important principle of Pixeltable is that, although Pixeltable has a built-in library of useful operations and adapters, it will never prescribe a particular way of doing things. Pixeltable is built from the ground up to be extensible.\n\nLet's take a specific example. Recall our use of the ResNet-50 detection model, in which the `detect` column contains a JSON blob with bounding boxes, scores, and labels. Suppose we want to create a column containing the single label with the highest confidence score. There's no built-in Pixeltable function to do this, but it's easy to write our own. In fact, all we have to do is write a Python function that does the thing we want, and mark it with the `@pxt.udf` decorator.",
            "Congratulations! You've reached the end of the tutorial. Hopefully, this gives a good overview of the capabilities of Pixeltable, but there's much more to explore. As a next step, you might check out one of the other tutorials, depending on your interests:\n- [Object Detection in Videos](https://pixeltable.readme.io/docs/object-detection-in-videos/)\n- [RAG Operations in Pixeltable](https://pixeltable.readme.io/docs/rag-operations/)\n- [Working with OpenAI in Pixeltable](https://pixeltable.readme.io/docs/working-with-openai/)"
          ],
          "code_blocks": [
            {
              "code": "import pixeltable as pxt\n\n# Create the directory `demo` (if it doesn't already exist)\npxt.drop_dir('demo', force=True)  # First drop `demo` to ensure a clean environment\npxt.create_dir('demo')\n\n# Create the table `demo.first` with a single column `input_image`\nt = pxt.create_table('demo.first', {'input_image': pxt.Image})",
              "output": "Connected to Pixeltable database at: postgresql://postgres:@/pixeltable?host=/Users/asiegel/.pixeltable/pgdata\nCreated directory `demo`.\nCreated table `first`.\n",
              "explanation": ""
            },
            {
              "code": "t.describe()",
              "output": null,
              "explanation": ""
            },
            {
              "code": "t.count()",
              "output": "0",
              "explanation": ""
            },
            {
              "code": "t.insert(input_image='https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000025.jpg')",
              "output": "Inserting rows into `first`: 1 rows [00:00, 336.92 rows/s]\nInserted 1 row with 0 errors.\n",
              "explanation": ""
            },
            {
              "code": "t.show()",
              "output": "                                         input_image\n0  <PIL.JpegImagePlugin.JpegImageFile image mode=...",
              "explanation": ""
            },
            {
              "code": "from pixeltable.functions import huggingface\n\nt.add_computed_column(detections=huggingface.detr_for_object_detection(\n    t.input_image, model_id='facebook/detr-resnet-50'\n))",
              "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:02<00:00,  2.03s/ cells]\nAdded 1 column value with 0 errors.\n",
              "explanation": ""
            },
            {
              "code": "t.show()",
              "output": "                                         input_image  \\\n0  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n\n                                          detections  \n0  {'boxes': [[51.94154739379883, 356.17449951171...  ",
              "explanation": ""
            },
            {
              "code": "t.add_computed_column(detections_text=t.detections.label_text)\nt.show()",
              "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 281.61 cells/s]\nAdded 1 column value with 0 errors.\n",
              "explanation": ""
            },
            {
              "code": "t.describe()",
              "output": null,
              "explanation": ""
            },
            {
              "code": "more_images = [\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000030.jpg',\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000034.jpg',\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000042.jpg',\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000061.jpg'\n]\nt.insert({'input_image': image} for image in more_images)",
              "output": "Computing cells:  50%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                      | 4/8 [00:01<00:01,  3.67 cells/s]\nInserting rows into `first`: 4 rows [00:00, 3478.59 rows/s]\nComputing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 8/8 [00:01<00:00,  7.32 cells/s]\nInserted 4 rows with 0 errors.\n",
              "explanation": ""
            },
            {
              "code": "t.select(t.input_image, t.detections_text).show()",
              "output": "                                         input_image  \\\n0  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n1  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n2  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n3  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n4  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n\n                                     detections_text  \n0                                 [giraffe, giraffe]  \n1                               [vase, potted plant]  \n2                                            [zebra]  \n3                                         [dog, dog]  \n4  [person, person, bench, person, elephant, elep...  ",
              "explanation": ""
            },
            {
              "code": "# Clear all variables in the notebook\n%reset -f\n\n# Instantiate a new client object\nimport pixeltable as pxt\nt = pxt.get_table('demo.first')\n\n# Display just the first two rows, to avoid cluttering the tutorial\nt.select(t.input_image, t.detections_text).show(2)",
              "output": "                                         input_image       detections_text\n0  <PIL.JpegImagePlugin.JpegImageFile image mode=...    [giraffe, giraffe]\n1  <PIL.JpegImagePlugin.JpegImageFile image mode=...  [vase, potted plant]",
              "explanation": ""
            },
            {
              "code": "import os\nimport getpass\nif 'OPENAI_API_KEY' not in os.environ:\n    os.environ['OPENAI_API_KEY'] = getpass.getpass('Enter your OpenAI API key:')",
              "output": "Enter your OpenAI API key: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n",
              "explanation": ""
            },
            {
              "code": "from pixeltable.functions import openai\n\nt.add_computed_column(vision=openai.vision(\n    prompt=\"Describe what's in this image.\",\n    image=t.input_image,\n    model='gpt-4o-mini'\n))",
              "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:28<00:00,  5.64s/ cells]\nComputing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 647.69 cells/s]\nAdded 5 column values with 0 errors.\n",
              "explanation": ""
            },
            {
              "code": "t.select(t.input_image, t.detections_text, t.vision).show()",
              "output": "                                         input_image  \\\n0  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n1  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n2  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n3  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n4  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n\n                                     detections_text  \\\n0                                 [giraffe, giraffe]   \n1                               [vase, potted plant]   \n2                                         [dog, dog]   \n3                                            [zebra]   \n4  [person, person, bench, person, elephant, elep...   \n\n                                              vision  \n0  The image shows two giraffes in a natural sett...  \n1  The image features a white vase with a shell-l...  \n2  The image shows a small, curly-haired dog lyin...  \n3  The image depicts a zebra grazing on green gra...  \n4  The image shows a lush, green forest scene whe...  ",
              "explanation": ""
            },
            {
              "code": "t.add_computed_column(rot_image=t.input_image.rotate(180))\nt.add_computed_column(rot_vision=openai.vision(\n    prompt=\"Describe what's in this image.\",\n    image=t.rot_image,\n    model='gpt-4o-mini'\n))",
              "output": "Added 5 column values with 0 errors.\nComputing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:26<00:00,  5.24s/ cells]\nComputing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 661.02 cells/s]\nAdded 5 column values with 0 errors.\n",
              "explanation": ""
            },
            {
              "code": "t.select(t.rot_image, t.rot_vision).show()",
              "output": "                                           rot_image  \\\n0  <PIL.Image.Image image mode=RGB size=640x425 a...   \n1  <PIL.Image.Image image mode=RGB size=640x426 a...   \n2  <PIL.Image.Image image mode=RGB size=640x478 a...   \n3  <PIL.Image.Image image mode=RGB size=640x488 a...   \n4  <PIL.Image.Image image mode=RGB size=640x428 a...   \n\n                                          rot_vision  \n0  The image features a zebra lying on green gras...  \n1  The image features a giraffe in a natural sett...  \n2  The image shows a pile of shoes and a small, f...  \n3  The image depicts a lush, green forest scene w...  \n4  The image features a white vase hanging upside...  ",
              "explanation": ""
            },
            {
              "code": "@pxt.udf\ndef top_detection(detect: dict) -> str:\n    scores = detect['scores']\n    label_text = detect['label_text']\n    # Get the index of the object with the highest confidence\n    i = scores.index(max(scores))\n    # Return the corresponding label\n    return label_text[i]",
              "output": null,
              "explanation": ""
            },
            {
              "code": "t.add_computed_column(top=top_detection(t.detections))",
              "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 495.50 cells/s]\nComputing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 1096.21 cells/s]\nAdded 5 column values with 0 errors.\n",
              "explanation": ""
            },
            {
              "code": "t.select(t.detections_text, t.top).show()",
              "output": "                                     detections_text       top\n0                                 [giraffe, giraffe]   giraffe\n1                                            [zebra]     zebra\n2                                         [dog, dog]       dog\n3  [person, person, bench, person, elephant, elep...  elephant\n4                               [vase, potted plant]      vase",
              "explanation": ""
            }
          ]
        }
      ],
      "api_usage": {},
      "key_concepts": [
        "incremental",
        "computed column",
        "multimodal",
        "udf"
      ],
      "code_snippets": [
        {
          "code": "%pip install -qU torch transformers openai pixeltable",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/pixeltable-basics.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/pixeltable-basics.ipynb)\n\n# Pixeltable Basics\n\nWelcome to Pixeltable! In this tutorial, we'll survey how to create tables, populate them with data, and enhance them with built-in and user-defined transformations and AI operations.\n\nIf you want to follow along with this tutorial interactively, there are two ways to go.\n- Use a Kaggle or Colab container (easiest): Click on one of the badges above.\n- Locally in a self-managed Python environment: You'll probably want to create your own empty notebook, then copy-paste each command from the website. Be sure your Jupyter kernel is running in a Python virtual environment; you can check out the [Getting Started with Pixeltable](https://docs.pixeltable.com/docs/pixeltable-basics) guide for step-by-step instructions.\n\n## Install Python Packages\n\nFirst run the following command to install Pixeltable and related libraries needed for this tutorial.",
          "output": null
        }
      ]
    },
    {
      "notebook": "fundamentals/computed-columns.ipynb",
      "title": "Pixeltable Fundamentals",
      "description": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/fundamentals/computed-columns.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/fundamentals/computed-columns.ipynb) Welcome to Section 2 of the __Pixeltable Fundamentals__ tutorial, __Computed Columns__. In the previous section, [Tables and Data Operations](https://pixeltable.readme.io/docs/tables-and-data-operations), we learned how to create tables, populate them with data, and query and manipulate their contents. In this section, we'll introduce one of Pixeltable's most essential and powerful concepts: computed columns. We'll learn how to:",
      "workflows": [],
      "api_usage": {},
      "key_concepts": [
        "incremental",
        "udf",
        "iterator",
        "computed column",
        "multimodal"
      ],
      "code_snippets": [
        {
          "code": "%pip install -qU pixeltable torch transformers",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/fundamentals/computed-columns.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/fundamentals/computed-columns.ipynb)\n# Pixeltable Fundamentals\n\n## Section 2: Computed Columns\n\nWelcome to Section 2 of the __Pixeltable Fundamentals__ tutorial, __Computed Columns__.\n\nIn the previous section, [Tables and Data Operations](https://pixeltable.readme.io/docs/tables-and-data-operations), we learned how to create tables, populate them with data, and query and manipulate their contents. In this section, we'll introduce one of Pixeltable's most essential and powerful concepts: computed columns. We'll learn how to:\n\n- Add computed columns to a table\n- Use computed columns for complex operations such as image processing and model inference\n\n**If you are running this tutorial in Colab:**\nIn order to make the tutorial run a bit snappier, let's switch to a GPU-equipped instance for this Colab session. To do that, click on the `Runtime -> Change runtime type` menu item at the top, then select the `GPU` radio button and click on `Save`.\n\nNext, let's ensure the Pixeltable library is installed in your environment, along with the Huggingface `transformers` library, which we'll need for this tutorial section.",
          "output": null
        },
        {
          "code": "import pixeltable as pxt\n\npxt.drop_dir('fundamentals', force=True)\npxt.create_dir('fundamentals')\npop_t = pxt.io.import_csv(\n    'fundamentals.population',\n    'https://github.com/pixeltable/pixeltable/raw/release/docs/resources/world-population-data.csv'\n)",
          "context": "### Computed Columns\nLet's start with a simple example that illustrates the basic concepts behind computed columns. We'll use a table of world population data for our example. Remember that you can import datasets into a Pixeltable table by providing a URL or file path to `pxt.io.import_csv()`.",
          "output": "Connected to Pixeltable database at: postgresql+psycopg://postgres:@/pixeltable?host=/Users/asiegel/.pixeltable/pgdata\nCreated directory `fundamentals`.\nCreated table `population`.\nInserting rows into `population`: 234 rows [00:00, 15845.96 rows/s]\nInserted 234 rows with 0 errors.\n"
        },
        {
          "code": "pop_t.head(5)",
          "context": "Let's start with a simple example that illustrates the basic concepts behind computed columns. We'll use a table of world population data for our example. Remember that you can import datasets into a Pixeltable table by providing a URL or file path to `pxt.io.import_csv()`.\nAlso recall that `pop_t.head()` returns the first few rows of a table, and typing the table name `pop_t` by itself gives the schema.",
          "output": "  cca3        country      continent    pop_2023    pop_2022    pop_2000  \\\n0  IND          India           Asia  1428627663  1417173173  1059633675   \n1  CHN          China           Asia  1425671352  1425887337  1264099069   \n2  USA  United States  North America   339996563   338289857   282398554   \n3  IDN      Indonesia           Asia   277534122   275501339   214072421   \n4  PAK       Pakistan           Asia   240485658   235824862   154369924   \n\n   area__km__  \n0   3287590.0  \n1   9706961.0  \n2   9372610.0  \n3   1904569.0  \n4    881912.0  "
        },
        {
          "code": "pop_t",
          "context": "Let's start with a simple example that illustrates the basic concepts behind computed columns. We'll use a table of world population data for our example. Remember that you can import datasets into a Pixeltable table by providing a URL or file path to `pxt.io.import_csv()`.\nAlso recall that `pop_t.head()` returns the first few rows of a table, and typing the table name `pop_t` by itself gives the schema.",
          "output": "table 'population'\n\nColumn Name   Type Computed With\n       cca3 String              \n    country String              \n  continent String              \n   pop_2023    Int              \n   pop_2022    Int              \n   pop_2000    Int              \n area__km__  Float              "
        },
        {
          "code": "pop_t.select(pop_t.country, yoy_change=(pop_t.pop_2023 - pop_t.pop_2022)).head(5)",
          "context": "Also recall that `pop_t.head()` returns the first few rows of a table, and typing the table name `pop_t` by itself gives the schema.\nNow let's suppose we want to add a new column for the year-over-year population change from 2022 to 2023. In the previous tutorial section, [Tables and Data Operations](https://docs.pixeltable.com/docs/tables-and-data-operations), we saw how one might `select()` such a quantity into a Pixeltable `DataFrame`, giving it the name `yoy_change` (year-over-year change):",
          "output": "         country  yoy_change\n0          India    11454490\n1          China     -215985\n2  United States     1706706\n3      Indonesia     2032783\n4       Pakistan     4660796"
        },
        {
          "code": "pop_t.add_computed_column(yoy_change=(pop_t.pop_2023 - pop_t.pop_2022))",
          "context": "Now let's suppose we want to add a new column for the year-over-year population change from 2022 to 2023. In the previous tutorial section, [Tables and Data Operations](https://docs.pixeltable.com/docs/tables-and-data-operations), we saw how one might `select()` such a quantity into a Pixeltable `DataFrame`, giving it the name `yoy_change` (year-over-year change):\nA __computed column__ is a way of turning such a selection into a new, permanent column of the table. Here's how it works:",
          "output": "Added 234 column values with 0 errors.\n"
        },
        {
          "code": "pop_t",
          "context": "A __computed column__ is a way of turning such a selection into a new, permanent column of the table. Here's how it works:\nAs soon as the column is added, Pixeltable will (by default) automatically compute its value for all rows in the table, storing the results in the new column. If we now inspect the schema of `pop_t`, we see the new column and its definition.",
          "output": "table 'population'\n\nColumn Name   Type       Computed With\n       cca3 String                    \n    country String                    \n  continent String                    \n   pop_2023    Int                    \n   pop_2022    Int                    \n   pop_2000    Int                    \n area__km__  Float                    \n yoy_change    Int pop_2023 - pop_2022"
        },
        {
          "code": "pop_t.select(pop_t.country, pop_t.yoy_change).head(5)",
          "context": "As soon as the column is added, Pixeltable will (by default) automatically compute its value for all rows in the table, storing the results in the new column. If we now inspect the schema of `pop_t`, we see the new column and its definition.\nThe new column can be queried in the usual manner.",
          "output": "         country  yoy_change\n0          India    11454490\n1          China     -215985\n2  United States     1706706\n3      Indonesia     2032783\n4       Pakistan     4660796"
        },
        {
          "code": "pop_t.add_computed_column(\n    yoy_percent_change=(100 * pop_t.yoy_change / pop_t.pop_2022)\n)",
          "context": "The new column can be queried in the usual manner.\nThe output is identical to the previous example, but now we're retrieving the computed output from the database, instead of computing it on-the-fly.\n\nComputed columns can be \"chained\" with other computed columns. Here's an example that expresses population change as a percentage:",
          "output": "Added 234 column values with 0 errors.\n"
        },
        {
          "code": "pop_t",
          "context": "The new column can be queried in the usual manner.\nThe output is identical to the previous example, but now we're retrieving the computed output from the database, instead of computing it on-the-fly.\n\nComputed columns can be \"chained\" with other computed columns. Here's an example that expresses population change as a percentage:",
          "output": "table 'population'\n\n       Column Name   Type                 Computed With\n              cca3 String                              \n           country String                              \n         continent String                              \n          pop_2023    Int                              \n          pop_2022    Int                              \n          pop_2000    Int                              \n        area__km__  Float                              \n        yoy_change    Int           pop_2023 - pop_2022\nyoy_percent_change  Float (100 * yoy_change) / pop_2022"
        },
        {
          "code": "pop_t.select(pop_t.country, pop_t.yoy_change, pop_t.yoy_percent_change).head(5)",
          "context": "The new column can be queried in the usual manner.\nThe output is identical to the previous example, but now we're retrieving the computed output from the database, instead of computing it on-the-fly.\n\nComputed columns can be \"chained\" with other computed columns. Here's an example that expresses population change as a percentage:",
          "output": "         country  yoy_change  yoy_percent_change\n0          India    11454490            0.808263\n1          China     -215985           -0.015147\n2  United States     1706706            0.504510\n3      Indonesia     2032783            0.737849\n4       Pakistan     4660796            1.976380"
        },
        {
          "code": "pop_t.insert(\n    country='California',\n    pop_2023=39110000,\n    pop_2022=39030000,\n)",
          "context": "The output is identical to the previous example, but now we're retrieving the computed output from the database, instead of computing it on-the-fly.\n\nComputed columns can be \"chained\" with other computed columns. Here's an example that expresses population change as a percentage:\nAlthough computed columns appear superficially similar to DataFrames, there is a key difference. Because computed columns are a permanent part of the table, they will be automatically updated any time new data is added to the table. These updates will propagate through any other computed columns that are \"downstream\" of the new data, ensuring that the state of the entire data is kept up-to-date.\n\n<div class=\"alert alert-block alert-info\">\nIn traditional data workflows, it is commonplace to recompute entire pipelines when the input dataset is changed or enlarged. In Pixeltable, by contrast, <b>all updates are applied incrementally</b>. When new data appear in a table or existing data are altered, Pixeltable will recompute only those rows that are dependent on the changed data.\n</div>\n\nLet's see how this works in practice. For purposes of illustration, we'll add an entry for California to the table, as if it were a country.",
          "output": "Computing cells:   0%|                                                    | 0/5 [00:00<?, ? cells/s]\nInserting rows into `population`: 1 rows [00:00, 198.14 rows/s]\nComputing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 645.24 cells/s]\nInserted 1 row with 0 errors.\n"
        },
        {
          "code": "pop_t.tail(5)",
          "context": "Although computed columns appear superficially similar to DataFrames, there is a key difference. Because computed columns are a permanent part of the table, they will be automatically updated any time new data is added to the table. These updates will propagate through any other computed columns that are \"downstream\" of the new data, ensuring that the state of the entire data is kept up-to-date.\n\n<div class=\"alert alert-block alert-info\">\nIn traditional data workflows, it is commonplace to recompute entire pipelines when the input dataset is changed or enlarged. In Pixeltable, by contrast, <b>all updates are applied incrementally</b>. When new data appear in a table or existing data are altered, Pixeltable will recompute only those rows that are dependent on the changed data.\n</div>\n\nLet's see how this works in practice. For purposes of illustration, we'll add an entry for California to the table, as if it were a country.\nObserve that the computed columns `yoy_growth` and `yoy_percent_growth` have been automatically updated in response to the new data.",
          "output": "   cca3           country      continent  pop_2023  pop_2022  pop_2000  \\\n0   FLK  Falkland Islands  South America      3791      3780    3080.0   \n1   NIU              Niue        Oceania      1935      1934    2074.0   \n2   TKL           Tokelau        Oceania      1893      1871    1666.0   \n3   VAT      Vatican City         Europe       518       510     651.0   \n4  None        California           None  39110000  39030000       NaN   \n\n   area__km__  yoy_change  yoy_percent_change  \n0    12173.00          11            0.291005  \n1      261.00           1            0.051706  \n2       12.00          22            1.175842  \n3        0.44           8            1.568627  \n4         NaN       80000            0.204971  "
        },
        {
          "code": "t = pxt.create_table('fundamentals.image_ops', {'source': pxt.Image})",
          "context": "### A More Complex Example: Image Processing\nIn the __Tables and Data Operations__ tutorial, we saw how media data such as images can be inserted into Pixeltable tables, alongside more traditional structured data. Let's explore another example that uses computed columns for image processing operations.\n\nIn this example, we'll create the table directly by providing a schema, rather than importing it from a CSV like before.",
          "output": "Created table `image_ops`.\n"
        },
        {
          "code": "url_prefix = 'https://github.com/pixeltable/pixeltable/raw/release/docs/resources/images'\nimages = ['000000000139.jpg', '000000000632.jpg', '000000000872.jpg']\nt.insert({'source': f'{url_prefix}/{image}'} for image in images)",
          "context": "### A More Complex Example: Image Processing\nIn the __Tables and Data Operations__ tutorial, we saw how media data such as images can be inserted into Pixeltable tables, alongside more traditional structured data. Let's explore another example that uses computed columns for image processing operations.\n\nIn this example, we'll create the table directly by providing a schema, rather than importing it from a CSV like before.",
          "output": "Inserting rows into `image_ops`: 3 rows [00:00, 1092.08 rows/s]\nInserted 3 rows with 0 errors.\n"
        },
        {
          "code": "t.collect()",
          "context": "### A More Complex Example: Image Processing\nIn the __Tables and Data Operations__ tutorial, we saw how media data such as images can be inserted into Pixeltable tables, alongside more traditional structured data. Let's explore another example that uses computed columns for image processing operations.\n\nIn this example, we'll create the table directly by providing a schema, rather than importing it from a CSV like before.",
          "output": "                                              source\n0  <PIL.JpegImagePlugin.JpegImageFile image mode=...\n1  <PIL.JpegImagePlugin.JpegImageFile image mode=...\n2  <PIL.JpegImagePlugin.JpegImageFile image mode=..."
        },
        {
          "code": "t.add_computed_column(metadata=t.source.get_metadata())\nt.collect()",
          "context": "In the __Tables and Data Operations__ tutorial, we saw how media data such as images can be inserted into Pixeltable tables, alongside more traditional structured data. Let's explore another example that uses computed columns for image processing operations.\n\nIn this example, we'll create the table directly by providing a schema, rather than importing it from a CSV like before.\nWhat are some things we might want to do with these images? A fairly basic one is to extract metadata. Pixeltable provides the built-in UDF `get_metadata()`, which returns a dictionary with various metadata about the image. Let's go ahead and make this a computed column.\n\n<div class=\"alert alert-block alert-info\">\n\"UDF\" is standard terminology in databases, meaning \"User-Defined Function\". Technically speaking, the <code>get_metadata()</code> function isn't user-defined, it's built in to the Pixeltable library. But we'll consistently refer to Pixeltable functions as \"UDFs\" in order to clearly distinguish them from ordinary Python functions. Later in this tutorial, we'll see how to turn (almost) any Python function into a Pixeltable UDF.\n</div>",
          "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 113.06 cells/s]\nAdded 3 column values with 0 errors.\n"
        },
        {
          "code": "t.add_computed_column(rotated=t.source.rotate(10))",
          "context": "What are some things we might want to do with these images? A fairly basic one is to extract metadata. Pixeltable provides the built-in UDF `get_metadata()`, which returns a dictionary with various metadata about the image. Let's go ahead and make this a computed column.\n\n<div class=\"alert alert-block alert-info\">\n\"UDF\" is standard terminology in databases, meaning \"User-Defined Function\". Technically speaking, the <code>get_metadata()</code> function isn't user-defined, it's built in to the Pixeltable library. But we'll consistently refer to Pixeltable functions as \"UDFs\" in order to clearly distinguish them from ordinary Python functions. Later in this tutorial, we'll see how to turn (almost) any Python function into a Pixeltable UDF.\n</div>\nImage operations, of course, can also return new images.",
          "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 89.02 cells/s]\nAdded 3 column values with 0 errors.\n"
        },
        {
          "code": "t.collect()",
          "context": "What are some things we might want to do with these images? A fairly basic one is to extract metadata. Pixeltable provides the built-in UDF `get_metadata()`, which returns a dictionary with various metadata about the image. Let's go ahead and make this a computed column.\n\n<div class=\"alert alert-block alert-info\">\n\"UDF\" is standard terminology in databases, meaning \"User-Defined Function\". Technically speaking, the <code>get_metadata()</code> function isn't user-defined, it's built in to the Pixeltable library. But we'll consistently refer to Pixeltable functions as \"UDFs\" in order to clearly distinguish them from ordinary Python functions. Later in this tutorial, we'll see how to turn (almost) any Python function into a Pixeltable UDF.\n</div>\nImage operations, of course, can also return new images.",
          "output": "                                              source  \\\n0  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n1  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n2  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n\n                                            metadata  \\\n0  {'bits': 8, 'mode': 'RGB', 'width': 640, 'form...   \n1  {'bits': 8, 'mode': 'RGB', 'width': 640, 'form...   \n2  {'bits': 8, 'mode': 'RGB', 'width': 621, 'form...   \n\n                                             rotated  \n0  <PIL.JpegImagePlugin.JpegImageFile image mode=...  \n1  <PIL.JpegImagePlugin.JpegImageFile image mode=...  \n2  <PIL.JpegImagePlugin.JpegImageFile image mode=...  "
        },
        {
          "code": "t.add_computed_column(rotated_transparent=t.source.convert('RGBA').rotate(10))\nt.collect()",
          "context": "Image operations, of course, can also return new images.\nOr, perhaps we want to rotate our images and fill them in with a transparent background rather than black. We can do this by chaining image operations, adding a transparency layer before doing the rotation.",
          "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 24.34 cells/s]\nAdded 3 column values with 0 errors.\n"
        },
        {
          "code": "t",
          "context": "<div class=\"alert alert-block alert-info\">\nIn addition to <code>get_metadata()</code>, <code>convert()</code>, and <code>rotate()</code>, Pixeltable has a sizable library of other common image operations that can be used as UDFs in computed columns. For the most part, the image UDFs are analogs of the operations provided by the <a href=\"https://pillow.readthedocs.io/en/stable/\">Pillow</a> library (in fact, Pixeltable is just using Pillow under the covers). You can read more about the provided image (and other) UDFs in the <a href=\"https://pixeltable.github.io/pixeltable/api/functions/image/\">Pixeltable API Documentation</a>.\n</div>\nLet's have a look at our table schema.",
          "output": "table 'image_ops'\n\n        Column Name           Type                     Computed With\n             source          Image                                  \n           metadata Required[Json]             source.get_metadata()\n            rotated          Image                 source.rotate(10)\nrotated_transparent  Image['RGBA'] source.convert('RGBA').rotate(10)"
        },
        {
          "code": "from pixeltable.functions.huggingface import detr_for_object_detection\n\nt.add_computed_column(detections=detr_for_object_detection(\n    t.source,\n    model_id='facebook/detr-resnet-50',\n    threshold=0.8\n))",
          "context": "### Image Detection\nIn addition to simple operations like `rotate()` and `convert()`, the Pixeltable API includes UDFs for various off-the-shelf image models. Let's look at one example: object detection using the ResNet-50 model. Model inference is a UDF too, and it can be inserted into a computed column like any other.\n\nThis one may take a little more time to compute, since it involves first downloading the ResNet-50 model (if it isn't already cached), then running inference on the images in our table.",
          "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:03<00:00,  1.05s/ cells]\nAdded 3 column values with 0 errors.\n"
        },
        {
          "code": "t.select(t.source, t.detections).collect()",
          "context": "### Image Detection\nIn addition to simple operations like `rotate()` and `convert()`, the Pixeltable API includes UDFs for various off-the-shelf image models. Let's look at one example: object detection using the ResNet-50 model. Model inference is a UDF too, and it can be inserted into a computed column like any other.\n\nThis one may take a little more time to compute, since it involves first downloading the ResNet-50 model (if it isn't already cached), then running inference on the images in our table.",
          "output": "                                              source  \\\n0  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n1  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n2  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n\n                                          detections  \n0  {'boxes': [[243.32928466796875, 198.9383697509...  \n1  {'boxes': [[0.6772994995117188, 280.9590454101...  \n2  {'boxes': [[270.3342590332031, 353.47186279296...  "
        },
        {
          "code": "from pixeltable.functions.vision import draw_bounding_boxes\n\nt.add_computed_column(image_with_bb=draw_bounding_boxes(\n    t.source, t.detections.boxes, t.detections.label_text, fill=True\n))\nt.select(t.source, t.image_with_bb).collect()",
          "context": "In addition to simple operations like `rotate()` and `convert()`, the Pixeltable API includes UDFs for various off-the-shelf image models. Let's look at one example: object detection using the ResNet-50 model. Model inference is a UDF too, and it can be inserted into a computed column like any other.\n\nThis one may take a little more time to compute, since it involves first downloading the ResNet-50 model (if it isn't already cached), then running inference on the images in our table.\nIt's great that the DETR model gave us so much information about the images, but it's not exactly in human-readable form. Those are JSON structures that encode bounding boxes, confidence scores, and categories for each detected object. Let's do something more useful with them: we'll use Pixeltable's `draw_bounding_boxes()` API to superimpose bounding boxes on the images, using different colors to distinguish different object categories.",
          "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 55.74 cells/s]\nAdded 3 column values with 0 errors.\n"
        },
        {
          "code": "t.select(t.image_with_bb).head(1)",
          "context": "It's great that the DETR model gave us so much information about the images, but it's not exactly in human-readable form. Those are JSON structures that encode bounding boxes, confidence scores, and categories for each detected object. Let's do something more useful with them: we'll use Pixeltable's `draw_bounding_boxes()` API to superimpose bounding boxes on the images, using different colors to distinguish different object categories.\nIt can be a little hard to see what's going on, so let's zoom in on just one image. If you select a single image in a notebook, Pixeltable will enlarge its display:",
          "output": "                                       image_with_bb\n0  <PIL.JpegImagePlugin.JpegImageFile image mode=..."
        },
        {
          "code": "t",
          "context": "It can be a little hard to see what's going on, so let's zoom in on just one image. If you select a single image in a notebook, Pixeltable will enlarge its display:\nLet's check in on our schema. We now have five computed columns, all derived from the single source column.",
          "output": "table 'image_ops'\n\n        Column Name            Type                                                                        Computed With\n             source           Image                                                                                     \n           metadata  Required[Json]                                                                source.get_metadata()\n            rotated           Image                                                                    source.rotate(10)\nrotated_transparent   Image['RGBA']                                                    source.convert('RGBA').rotate(10)\n         detections  Required[Json] detr_for_object_detection(source, model_id='facebook/detr-resnet-50', threshold=0.8)\n      image_with_bb Required[Image]      draw_bounding_boxes(source, detections.boxes, detections.label_text, fill=True)"
        },
        {
          "code": "more_images = ['000000000108.jpg', '000000000885.jpg']\nt.insert({'source': f'{url_prefix}/{image}'} for image in more_images)",
          "context": "Let's check in on our schema. We now have five computed columns, all derived from the single source column.\nAnd as always, when we add new data to the table, its computed columns are updated automatically. Let's try this on a few more images.",
          "output": "Computing cells:  20%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c                                  | 2/10 [00:01<00:05,  1.45 cells/s]\nInserting rows into `image_ops`: 2 rows [00:00, 1118.03 rows/s]\nComputing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:01<00:00,  6.96 cells/s]\nInserted 2 rows with 0 errors.\n"
        },
        {
          "code": "t.select(t.source, t.image_with_bb, t.detections.label_text, t.metadata).tail(2)",
          "context": "Let's check in on our schema. We now have five computed columns, all derived from the single source column.\nAnd as always, when we add new data to the table, its computed columns are updated automatically. Let's try this on a few more images.",
          "output": "                                              source  \\\n0  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n1  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n\n                                       image_with_bb  \\\n0  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n1  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n\n                                detections_labeltext  \\\n0                                            [train]   \n1  [person, person, tennis racket, person, person...   \n\n                                            metadata  \n0  {'bits': 8, 'mode': 'RGB', 'width': 640, 'form...  \n1  {'bits': 8, 'mode': 'RGB', 'width': 640, 'form...  "
        }
      ]
    },
    {
      "notebook": "fundamentals/queries-and-expressions.ipynb",
      "title": "Pixeltable Fundamentals",
      "description": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/fundamentals/queries-and-expressions.ipynb) [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/fundamentals/queries-and-expressions.ipynb) Welcome to Section 3 of the __Pixeltable Fundamentals__ tutorial, __Queries and Expressions__.",
      "workflows": [],
      "api_usage": {},
      "key_concepts": [
        "incremental",
        "embedding",
        "iterator",
        "computed column",
        "multimodal"
      ],
      "code_snippets": [
        {
          "code": "%pip install -qU pixeltable datasets torch transformers",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/fundamentals/queries-and-expressions.ipynb)\n[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/fundamentals/queries-and-expressions.ipynb)\n\n# Pixeltable Fundamentals\n\n## Section 3: Queries and Expressions\n\nWelcome to Section 3 of the __Pixeltable Fundamentals__ tutorial, __Queries and Expressions__.\n\nIn the previous section of this tutorial, [Computed Columns](https://docs.pixeltable.com/docs/computed-columns), we saw how to issue queries over Pixeltable tables, such as:\n\n```python\npop_t.select(yoy_change=(pop_t.pop_2023 - pop_t.pop_2022)).collect()\n```\n\nWe also saw how to define __computed columns__ that become part of the table and are updated automatically when new rows are inserted:\n\n```python\npop_t.add_column(yoy_change=(pop_t.pop_2023 - pop_t.pop_2022))\n```\n\nBoth these examples use the __Pixeltable expression__ `pop_t.pop_2023 - pop_t.pop_2022`. We've seen a number of other expressions as well, such as the chain of image operations\n\n```python\nt.source.convert('RGBA').rotate(10)\n```\n\nand the model invocation\n\n```python\ndetr_for_object_detection(\n    t.source,\n    model_id='facebook/detr-resnet-50',\n    threshold=0.8\n)\n```\n\nExpressions are the basic building blocks of Pixeltable workloads. An expression can be included in a `select()` statement, which will cause it to be evaluated dynamically, or in an `add_column()` statement, which will add it to the table schema as a computed column. In this section, we'll dive deeper into the different kinds of Pixeltable expressions and their uses. We'll:\n\n- Understand the relationship between Pixeltable expressions and query execution\n- Survey the different types of expressions\n- Learn more about the Pixeltable type system\n\nTo get started, let's import the necessary libraries for this tutorial and set up a demo directory.",
          "output": null
        },
        {
          "code": "import pixeltable as pxt\n\npxt.drop_dir('demo', force=True)\npxt.create_dir('demo')",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/fundamentals/queries-and-expressions.ipynb)\n[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/fundamentals/queries-and-expressions.ipynb)\n\n# Pixeltable Fundamentals\n\n## Section 3: Queries and Expressions\n\nWelcome to Section 3 of the __Pixeltable Fundamentals__ tutorial, __Queries and Expressions__.\n\nIn the previous section of this tutorial, [Computed Columns](https://docs.pixeltable.com/docs/computed-columns), we saw how to issue queries over Pixeltable tables, such as:\n\n```python\npop_t.select(yoy_change=(pop_t.pop_2023 - pop_t.pop_2022)).collect()\n```\n\nWe also saw how to define __computed columns__ that become part of the table and are updated automatically when new rows are inserted:\n\n```python\npop_t.add_column(yoy_change=(pop_t.pop_2023 - pop_t.pop_2022))\n```\n\nBoth these examples use the __Pixeltable expression__ `pop_t.pop_2023 - pop_t.pop_2022`. We've seen a number of other expressions as well, such as the chain of image operations\n\n```python\nt.source.convert('RGBA').rotate(10)\n```\n\nand the model invocation\n\n```python\ndetr_for_object_detection(\n    t.source,\n    model_id='facebook/detr-resnet-50',\n    threshold=0.8\n)\n```\n\nExpressions are the basic building blocks of Pixeltable workloads. An expression can be included in a `select()` statement, which will cause it to be evaluated dynamically, or in an `add_column()` statement, which will add it to the table schema as a computed column. In this section, we'll dive deeper into the different kinds of Pixeltable expressions and their uses. We'll:\n\n- Understand the relationship between Pixeltable expressions and query execution\n- Survey the different types of expressions\n- Learn more about the Pixeltable type system\n\nTo get started, let's import the necessary libraries for this tutorial and set up a demo directory.",
          "output": "Connected to Pixeltable database at: postgresql+psycopg://postgres:@/pixeltable?host=/Users/asiegel/.pixeltable/pgdata\nCreated directory 'demo'.\n"
        },
        {
          "code": "import datasets\n\n# Download the first 50 images of the MNIST dataset\nds = datasets.load_dataset('ylecun/mnist', split='train[:50]')\n\n# Import them into a Pixeltable table\nt = pxt.io.import_huggingface_dataset('demo.mnist', ds)",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/fundamentals/queries-and-expressions.ipynb)\n[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/fundamentals/queries-and-expressions.ipynb)\n\n# Pixeltable Fundamentals\n\n## Section 3: Queries and Expressions\n\nWelcome to Section 3 of the __Pixeltable Fundamentals__ tutorial, __Queries and Expressions__.\n\nIn the previous section of this tutorial, [Computed Columns](https://docs.pixeltable.com/docs/computed-columns), we saw how to issue queries over Pixeltable tables, such as:\n\n```python\npop_t.select(yoy_change=(pop_t.pop_2023 - pop_t.pop_2022)).collect()\n```\n\nWe also saw how to define __computed columns__ that become part of the table and are updated automatically when new rows are inserted:\n\n```python\npop_t.add_column(yoy_change=(pop_t.pop_2023 - pop_t.pop_2022))\n```\n\nBoth these examples use the __Pixeltable expression__ `pop_t.pop_2023 - pop_t.pop_2022`. We've seen a number of other expressions as well, such as the chain of image operations\n\n```python\nt.source.convert('RGBA').rotate(10)\n```\n\nand the model invocation\n\n```python\ndetr_for_object_detection(\n    t.source,\n    model_id='facebook/detr-resnet-50',\n    threshold=0.8\n)\n```\n\nExpressions are the basic building blocks of Pixeltable workloads. An expression can be included in a `select()` statement, which will cause it to be evaluated dynamically, or in an `add_column()` statement, which will add it to the table schema as a computed column. In this section, we'll dive deeper into the different kinds of Pixeltable expressions and their uses. We'll:\n\n- Understand the relationship between Pixeltable expressions and query execution\n- Survey the different types of expressions\n- Learn more about the Pixeltable type system\n\nTo get started, let's import the necessary libraries for this tutorial and set up a demo directory.\nIn this tutorial we're going to work with a subset of the MNIST dataset, a classic reference database of hand-drawn digits. A copy of the MNIST dataset is hosted on the Hugging Face datasets repository, so wecan  use Pixeltable's built-in Hugging Face data importer to load it into a Pixeltable table.",
          "output": "Created table `mnist_tmp_32592145`.\nInserting rows into `mnist_tmp_32592145`: 50 rows [00:00, 1071.83 rows/s]\nInserted 50 rows with 0 errors.\n"
        },
        {
          "code": "t.head(5)",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/fundamentals/queries-and-expressions.ipynb)\n[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/fundamentals/queries-and-expressions.ipynb)\n\n# Pixeltable Fundamentals\n\n## Section 3: Queries and Expressions\n\nWelcome to Section 3 of the __Pixeltable Fundamentals__ tutorial, __Queries and Expressions__.\n\nIn the previous section of this tutorial, [Computed Columns](https://docs.pixeltable.com/docs/computed-columns), we saw how to issue queries over Pixeltable tables, such as:\n\n```python\npop_t.select(yoy_change=(pop_t.pop_2023 - pop_t.pop_2022)).collect()\n```\n\nWe also saw how to define __computed columns__ that become part of the table and are updated automatically when new rows are inserted:\n\n```python\npop_t.add_column(yoy_change=(pop_t.pop_2023 - pop_t.pop_2022))\n```\n\nBoth these examples use the __Pixeltable expression__ `pop_t.pop_2023 - pop_t.pop_2022`. We've seen a number of other expressions as well, such as the chain of image operations\n\n```python\nt.source.convert('RGBA').rotate(10)\n```\n\nand the model invocation\n\n```python\ndetr_for_object_detection(\n    t.source,\n    model_id='facebook/detr-resnet-50',\n    threshold=0.8\n)\n```\n\nExpressions are the basic building blocks of Pixeltable workloads. An expression can be included in a `select()` statement, which will cause it to be evaluated dynamically, or in an `add_column()` statement, which will add it to the table schema as a computed column. In this section, we'll dive deeper into the different kinds of Pixeltable expressions and their uses. We'll:\n\n- Understand the relationship between Pixeltable expressions and query execution\n- Survey the different types of expressions\n- Learn more about the Pixeltable type system\n\nTo get started, let's import the necessary libraries for this tutorial and set up a demo directory.\nIn this tutorial we're going to work with a subset of the MNIST dataset, a classic reference database of hand-drawn digits. A copy of the MNIST dataset is hosted on the Hugging Face datasets repository, so wecan  use Pixeltable's built-in Hugging Face data importer to load it into a Pixeltable table.",
          "output": "                                               image label\n0  <PIL.JpegImagePlugin.JpegImageFile image mode=...     5\n1  <PIL.JpegImagePlugin.JpegImageFile image mode=...     0\n2  <PIL.JpegImagePlugin.JpegImageFile image mode=...     4\n3  <PIL.JpegImagePlugin.JpegImageFile image mode=...     1\n4  <PIL.JpegImagePlugin.JpegImageFile image mode=...     9"
        },
        {
          "code": "t.image",
          "context": "### Column References\nThe most basic type of expression is a __column reference__: that's what you get when you type, say, `t.image`. An expression such as `t.image` by itself is just a Python object; it doesn't contain any actual data, and no data will be loaded until you use the expression in a `select()` query or `add_column()` statement. Here's what we get if we type `t.image` by itself:",
          "output": "Column\n'image'\n(of table 'demo.mnist')\n\n Column Name   Type Computed With\n       image  Image              "
        },
        {
          "code": "from pixeltable.functions.huggingface import vit_for_image_classification\n\nt.add_computed_column(classification=vit_for_image_classification(\n    t.image,\n    model_id='farleyknight-org-username/vit-base-mnist'\n))",
          "context": "### JSON Collections (Dicts and Lists)\nData is commonly presented in JSON format: for example, API responses and model output often take the shape of JSON dictionaries or lists of dictionaries. Pixeltable has native support for JSON accessors. To demonstrate this, let's add a computed column that runs an image classification model against the images in our dataset.",
          "output": "Added 50 column values with 0 errors.\n"
        },
        {
          "code": "t.select(t.image, t.classification).head(3)",
          "context": "### JSON Collections (Dicts and Lists)\nData is commonly presented in JSON format: for example, API responses and model output often take the shape of JSON dictionaries or lists of dictionaries. Pixeltable has native support for JSON accessors. To demonstrate this, let's add a computed column that runs an image classification model against the images in our dataset.",
          "output": "                                               image  \\\n0  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n1  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n2  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n\n                                      classification  \n0  {'labels': [5, 3, 2, 8, 7], 'scores': [0.98086...  \n1  {'labels': [0, 6, 9, 8, 1], 'scores': [0.99701...  \n2  {'labels': [4, 1, 9, 7, 0], 'scores': [0.99723...  "
        },
        {
          "code": "t",
          "context": "Data is commonly presented in JSON format: for example, API responses and model output often take the shape of JSON dictionaries or lists of dictionaries. Pixeltable has native support for JSON accessors. To demonstrate this, let's add a computed column that runs an image classification model against the images in our dataset.\nWe see that the output is returned as a dict containing three lists: the five most likely labels (classes) for the image, the corresponding text labels (in this case, just the string form of the class number), and the scores (confidences) of each prediction. The Pixeltable type of the `classification` column is `pxt.Json`:",
          "output": "Table\n'demo.mnist'\n\n     Column Name    Type                                      Computed With\n           image   Image                                                   \n           label  String                                                   \n  classification    Json  vit_for_image_classification(image, model_id='..."
        },
        {
          "code": "t.select(t.classification['labels']).head(3)",
          "context": "We see that the output is returned as a dict containing three lists: the five most likely labels (classes) for the image, the corresponding text labels (in this case, just the string form of the class number), and the scores (confidences) of each prediction. The Pixeltable type of the `classification` column is `pxt.Json`:\nPixeltable provides a range of operators on `Json`-typed output that behave just as you'd expect. To look up a key in a dictionary, use the syntax `t.classification['labels']`:",
          "output": "  classification_labels\n0       [5, 3, 2, 8, 7]\n1       [0, 6, 9, 8, 1]\n2       [4, 1, 9, 7, 0]"
        },
        {
          "code": "t.select(t.classification.labels).head(3)",
          "context": "Pixeltable provides a range of operators on `Json`-typed output that behave just as you'd expect. To look up a key in a dictionary, use the syntax `t.classification['labels']`:\nYou can also use a convenient \"attribute\" syntax for dictionary lookups. This follows the standard [JSONPath](https://en.wikipedia.org/wiki/JSONPath) expression syntax.",
          "output": "  classification_labels\n0       [5, 3, 2, 8, 7]\n1       [0, 6, 9, 8, 1]\n2       [4, 1, 9, 7, 0]"
        },
        {
          "code": "t.classification.labels",
          "context": "You can also use a convenient \"attribute\" syntax for dictionary lookups. This follows the standard [JSONPath](https://en.wikipedia.org/wiki/JSONPath) expression syntax.\nThe \"attribute\" syntax isn't fully general (it won't work for dictionary keys that are not valid Python identifiers), but it's handy when it works.\n\n\n`t.classification.labels` is another Pixeltable expression; you can think of it as saying, \"do the `'labels'` lookup from every dictionary in the column `t.classification`, and return the result as a new column.\" As before, the expression by itself contains no data; it's the query that does the actual work of retrieving data. Here's what we see if we just give the expression by itself, without a query:",
          "output": "classification.labels"
        },
        {
          "code": "t.select(t.classification.labels[0]).head(3)",
          "context": "The \"attribute\" syntax isn't fully general (it won't work for dictionary keys that are not valid Python identifiers), but it's handy when it works.\n\n\n`t.classification.labels` is another Pixeltable expression; you can think of it as saying, \"do the `'labels'` lookup from every dictionary in the column `t.classification`, and return the result as a new column.\" As before, the expression by itself contains no data; it's the query that does the actual work of retrieving data. Here's what we see if we just give the expression by itself, without a query:\nSimilarly, one can pull out a specific item in a list (for this model, we're probably mostly interested in the first item anyway):",
          "output": "   classification_labels0\n0                       5\n1                       0\n2                       4"
        },
        {
          "code": "t.select(t.classification.labels[:2]).head(3)",
          "context": "Similarly, one can pull out a specific item in a list (for this model, we're probably mostly interested in the first item anyway):\nOr slice a list in the usual manner:",
          "output": "  classification_labels2\n0                 [5, 3]\n1                 [0, 6]\n2                 [4, 1]"
        },
        {
          "code": "t.select(t.classification.not_a_key).head(3)",
          "context": "Or slice a list in the usual manner:\nPixeltable is resilient against out-of-bounds indices or dictionary keys. If an index or key doesn't exist for a particular row, you'll get a `None` output for that row.",
          "output": "  classification_notakey\n0                   None\n1                   None\n2                   None"
        },
        {
          "code": "# Use label_text to be consistent with t.label, which was given\n# to us as a string\n\nt.add_computed_column(pred_label=t.classification.label_text[0])\nt",
          "context": "Pixeltable is resilient against out-of-bounds indices or dictionary keys. If an index or key doesn't exist for a particular row, you'll get a `None` output for that row.\nAs always, any expression can be used to create a computed column.",
          "output": "Added 50 column values with 0 errors.\n"
        },
        {
          "code": "custom_dict = {\n    # Keys must be strings; values can be any expressions\n    'ground_truth': t.label,\n    'prediction': t.pred_label,\n    'is_correct': t.label == t.pred_label,\n    # You can also use constants as values\n    'engine': 'pixeltable',\n}\n\nt.select(t.image, custom_dict).head(5)",
          "context": "As always, any expression can be used to create a computed column.\nFinally, just as it's possible to extract items from lists and dictionaries using Pixeltable expressions, you can also construct new lists and dictionaries: just package them up in the usual way.",
          "output": "                                               image  \\\n0  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n1  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n2  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n3  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n4  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n\n                                               col_1  \n0  {'ground_truth': '5', 'prediction': '5', 'is_c...  \n1  {'ground_truth': '0', 'prediction': '0', 'is_c...  \n2  {'ground_truth': '4', 'prediction': '4', 'is_c...  \n3  {'ground_truth': '1', 'prediction': '1', 'is_c...  \n4  {'ground_truth': '9', 'prediction': '9', 'is_c...  "
        },
        {
          "code": "rot_model_result = vit_for_image_classification(\n    t.image.rotate(90),\n    model_id='farleyknight-org-username/vit-base-mnist'\n)\n\nt.select(t.image, rot_label=rot_model_result.labels[0]).head(5)   ",
          "context": "Finally, just as it's possible to extract items from lists and dictionaries using Pixeltable expressions, you can also construct new lists and dictionaries: just package them up in the usual way.\n### UDF Calls\n\nUDF calls are another common type of expression. We've seen them throughout the tutorial; in fact, we used one a moment ago when we added a model invocation to our workload:\n\n```python\nvit_for_image_classification(\n    t.image,\n    model_id='farleyknight-org-username/vit-base-mnist'\n)\n```\n\nThis calls the `vit_for_image_classification` UDF in the `pxt.functions.huggingface` module. Note that `vit_for_image_classification` is a Pixeltable UDF, not an ordinary Python function. (UDFs were first discussed in the [Tables and Data Operations](https://docs.pixeltable.com/docs/computed-columns) section of this tutorial.) You can think of a Pixeltable UDF as a function that operates on columns of data, iteratively applying an underlying operation to each row in the column (or columns). In this case, `vit_for_image_classification` operates on `t.image`, running the model against every image in the column.\n\nNotice that in addition to the column `t.image`, this call to `vit_for_image_classification` also takes a constant argument specifying the `model_id`. Any UDF call argument may be a constant, and the constant value simply means \"use this value for every row being evaluated\".\n\nYou can always compose Pixeltable expressions to form more complicated ones; here's an example that runs the model against a 90-degree rotation of every image in the sample and extracts the label. Not surprisingly, the model doesn't perform as well on the rotated images.",
          "output": "                                               image  rot_label\n0  <PIL.JpegImagePlugin.JpegImageFile image mode=...          4\n1  <PIL.JpegImagePlugin.JpegImageFile image mode=...          0\n2  <PIL.JpegImagePlugin.JpegImageFile image mode=...          5\n3  <PIL.JpegImagePlugin.JpegImageFile image mode=...          1\n4  <PIL.JpegImagePlugin.JpegImageFile image mode=...          8"
        },
        {
          "code": "a = t.image.rotate(90)\nb = pxt.functions.image.rotate(t.image, 90)",
          "context": "<div class=\"alert alert-block alert-info\">\nNote that we employed a useful trick here: we assigned an expression to the variable <code>rot_model_result</code> for later reuse. Every Pixeltable expression is a Python object, so you can freely assign them to variables, reuse them, compose them, and so on. Remember that nothing actually happens until the expression is used in a query - so in this example, setting the variable <code>rot_model_result</code> doesn't itself result in any data being retrieved; that only happens later, when we actually use it in the <code>select()</code> query.\n</div>\n\nThere are a large number of built-in UDFs that ship with Pixeltable; you can always refer back to the [API Documentation](https://pixeltable.github.io/pixeltable/) for details.\n### Method Calls\n\nMany built-in UDFs allow a convenient alternate syntax. The following two expressions are exactly equivalent:",
          "output": null
        },
        {
          "code": "t.select(\n    t.image,\n    t.label,\n    t.label == '4',\n    t.label < '5',\n).head(5)",
          "context": "### Arithmetic and Boolean Operations\nExpressions can also be combined using standard arithmetic and boolean operators. As with everything else, arithmetic and boolean expressions are operations on columns that (when used in a query) are applied to every row.",
          "output": "                                               image label  col_2  col_3\n0  <PIL.JpegImagePlugin.JpegImageFile image mode=...     5  False  False\n1  <PIL.JpegImagePlugin.JpegImageFile image mode=...     0  False   True\n2  <PIL.JpegImagePlugin.JpegImageFile image mode=...     4   True   True\n3  <PIL.JpegImagePlugin.JpegImageFile image mode=...     1  False   True\n4  <PIL.JpegImagePlugin.JpegImageFile image mode=...     9  False  False"
        },
        {
          "code": "t.where(t.label == '4').select(t.image).show()",
          "context": "Expressions can also be combined using standard arithmetic and boolean operators. As with everything else, arithmetic and boolean expressions are operations on columns that (when used in a query) are applied to every row.\nWhen you use a `where` clause in a query, you're giving it a Pixeltable expression, too (a boolean-valued one).",
          "output": "                                               image\n0  <PIL.JpegImagePlugin.JpegImageFile image mode=...\n1  <PIL.JpegImagePlugin.JpegImageFile image mode=...\n2  <PIL.JpegImagePlugin.JpegImageFile image mode=...\n3  <PIL.JpegImagePlugin.JpegImageFile image mode=..."
        },
        {
          "code": "# Reuse `rot_model_result` from above, extracting\n# the dominant label as a new expression\n\nrot_label = rot_model_result.label_text[0]\n\n# Select all the rows where the ground truth label is '5',\n# and the \"rotated\" version of the model got it wrong\n# (by returning something other than a '5')\n\nt.where((t.label == '5') & (rot_label != '5')).select(\n    t.image, t.label, rot_label=rot_label\n).show()",
          "context": "When you use a `where` clause in a query, you're giving it a Pixeltable expression, too (a boolean-valued one).\nThe following example shows how boolean expressions can be assigned to variables and used to form more complex expressions.",
          "output": "                                               image label rot_label\n0  <PIL.JpegImagePlugin.JpegImageFile image mode=...     5         4"
        },
        {
          "code": "from pixeltable.functions.huggingface import clip\n\n# Add a computed column that computes a CLIP embedding for each image\nt.add_computed_column(clip=clip(t.image, model_id='openai/clip-vit-base-patch32'))\nt.select(t.image, t.clip).head(5)",
          "context": "Notice that to form a logical \"and\", we wrote\n\n```python\n(t.label == '5') & (rot_label != '5')\n```\n\nusing the operator `&` rather than `and`. Likewise, to form a logical \"or\", we'd use `|` rather than `or`:\n\n```python\n(t.label == '5') | (rot_label != '5')\n```\n\nFor logical negation:\n\n```python\n~(t.label == '5')\n```\n\nThis follows the convention used by other popular data-manipulation frameworks such as Pandas, and it's necessary because the Python language does not allow the meanings of `and`, `or`, and `not` to be customized. There is one more instance of this to be aware of: to check whether an expression is `None`, it's necessary to write (say)\n\n```python\nt.label == None\n```\n\nrather than `t.label is None`, for the same reason.\n### Arrays\n\nIn addition to lists and dicts, Pixeltable also has built-in support for numerical arrays. A typical place where arrays show up is as the output of an embedding.",
          "output": "Added 50 column values with 0 errors.\n"
        },
        {
          "code": "t.select(t.clip[0], t.clip[5:10], t.clip[-3:]).head(5)",
          "context": "### Arrays\n\nIn addition to lists and dicts, Pixeltable also has built-in support for numerical arrays. A typical place where arrays show up is as the output of an embedding.\nThe underlying Python type of `pxt.Array` is an ordinary NumPy array (`np.ndarray`), so that an array-typed column is a column of NumPy arrays (in this example, representing the embedding output of each image in the table). As with lists, arrays can be sliced in all the usual ways.",
          "output": "      col_0                                              col_1  \\\n0 -0.052472  [-0.03184929, 0.06963743, 1.1165005, -0.037148...   \n1  0.062565  [-0.3323701, -0.20095566, 1.0229394, -0.002611...   \n2 -0.059083  [-0.23313646, 0.23840791, 0.8635649, -0.135253...   \n3  0.098077  [-0.106710635, 0.038182013, 1.0814098, -0.1106...   \n4 -0.046089  [-0.11528104, 0.06613226, 1.0892408, -0.016736...   \n\n                                    col_2  \n0  [0.90351915, 0.036779515, -0.16105242]  \n1  [0.82265085, -0.14863057, -0.16314176]  \n2   [0.76470613, -0.36314437, 0.20581008]  \n3     [1.0230694, -0.1362458, 0.09243362]  \n4     [0.7860199, 0.09811185, 0.17112087]  "
        },
        {
          "code": "import numpy as np\n\nt.select(t.clip.apply(np.ndarray.dumps, col_type=pxt.String)).head(2)",
          "context": "The underlying Python type of `pxt.Array` is an ordinary NumPy array (`np.ndarray`), so that an array-typed column is a column of NumPy arrays (in this example, representing the embedding output of each image in the table). As with lists, arrays can be sliced in all the usual ways.\n### Ad hoc UDFs with `apply`\n\nWe've now seen the most commonly encountered Pixeltable expression types. There are a few other less commonly encountered expressions that are occasionally useful.\n\nYou can use `apply` to map any Python function onto a column of data. You can think of `apply` as a quick way of constructing an \"on-the-fly\" UDF for one-off use.",
          "output": null
        },
        {
          "code": "# Select the text in position 0 of `t.classification.label_text`; since\n# `t.classification.label_text` has type `pxt.Json`, so does\n# `t.classification.label_text[0]`\n\nt.classification.label_text[0].col_type",
          "context": "Note, however, that if the function you're `apply`ing doesn't have type hints (as in the example here), you'll need to specify the output column type explicitly.\n### Type Conversion with `astype`\n\nSometimes it's useful to transform an expression of one type into a different type. For example, you can use `astype` to turn an expression of type `pxt.Json` into one of type `pxt.String`. This assumes that the value being converted is actually a string; otherwise, you'll get an exception. Here's an example:",
          "output": "Optional[Json]"
        },
        {
          "code": "# Select the text in position 0 of `t.classification.label_text`, this time\n# cast as a `pxt.String`\n\nt.classification.label_text[0].astype(pxt.String).col_type",
          "context": "Note, however, that if the function you're `apply`ing doesn't have type hints (as in the example here), you'll need to specify the output column type explicitly.\n### Type Conversion with `astype`\n\nSometimes it's useful to transform an expression of one type into a different type. For example, you can use `astype` to turn an expression of type `pxt.Json` into one of type `pxt.String`. This assumes that the value being converted is actually a string; otherwise, you'll get an exception. Here's an example:",
          "output": "Optional[String]"
        },
        {
          "code": "t.select(t.image, t.image.localpath).head(5)",
          "context": "### Type Conversion with `astype`\n\nSometimes it's useful to transform an expression of one type into a different type. For example, you can use `astype` to turn an expression of type `pxt.Json` into one of type `pxt.String`. This assumes that the value being converted is actually a string; otherwise, you'll get an exception. Here's an example:\n### Column Properties\n\nSome `ColumnRef` expressions have additional useful properties. A media column (image, video, audio, or document) has the following two properties:\n\n- `localpath`: the media location on the local filesystem\n- `fileurl`: the original URL where the media resides (could be the same as `localpath`)",
          "output": "                                               image  \\\n0  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n1  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n2  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n3  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n4  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n\n                                     image_localpath  \n0  /Users/asiegel/.pixeltable/media/26d0c4b761ea4...  \n1  /Users/asiegel/.pixeltable/media/26d0c4b761ea4...  \n2  /Users/asiegel/.pixeltable/media/26d0c4b761ea4...  \n3  /Users/asiegel/.pixeltable/media/26d0c4b761ea4...  \n4  /Users/asiegel/.pixeltable/media/26d0c4b761ea4...  "
        },
        {
          "code": "t.add_computed_column(channel=t.image.getchannel(1))",
          "context": "### Column Properties\n\nSome `ColumnRef` expressions have additional useful properties. A media column (image, video, audio, or document) has the following two properties:\n\n- `localpath`: the media location on the local filesystem\n- `fileurl`: the original URL where the media resides (could be the same as `localpath`)\nAny computed column will have two additional properties, `errortype` and `errormsg`. These properties will usually be `None`. However, if the computed column was created with `on_error='ignore'` and an exception was encountered during column execution, then the properties will contain additional information about the exception.\n\nTo demonstrate this feature, we're going to deliberately trigger an exception in a computed column. The images in our example table are black and white, meaning they have only one color channel. If we try to extract a channel other than channel number `0`, we'll get an exception. Ordinarily when we call `add_computed_column`, the exception is raised and the `add_computed_column` operation is aborted.",
          "output": null
        },
        {
          "code": "t.add_computed_column(channel=t.image.getchannel(1), on_error='ignore')",
          "context": "Any computed column will have two additional properties, `errortype` and `errormsg`. These properties will usually be `None`. However, if the computed column was created with `on_error='ignore'` and an exception was encountered during column execution, then the properties will contain additional information about the exception.\n\nTo demonstrate this feature, we're going to deliberately trigger an exception in a computed column. The images in our example table are black and white, meaning they have only one color channel. If we try to extract a channel other than channel number `0`, we'll get an exception. Ordinarily when we call `add_computed_column`, the exception is raised and the `add_computed_column` operation is aborted.\nBut if we use `on_error='ignore'`, the exception will be logged in the column properties instead.",
          "output": "Added 50 column values with 50 errors.\n"
        },
        {
          "code": "t.select(t.image, t.channel, t.channel.errortype, t.channel.errormsg).head(5)",
          "context": "But if we use `on_error='ignore'`, the exception will be logged in the column properties instead.\nNotice that the update status informs us that there were 50 errors. If we query the table, we see that the column contains only `None` values, but the `errortype` and `errormsg` fields contain details of the error.",
          "output": "                                               image channel  \\\n0  <PIL.JpegImagePlugin.JpegImageFile image mode=...    None   \n1  <PIL.JpegImagePlugin.JpegImageFile image mode=...    None   \n2  <PIL.JpegImagePlugin.JpegImageFile image mode=...    None   \n3  <PIL.JpegImagePlugin.JpegImageFile image mode=...    None   \n4  <PIL.JpegImagePlugin.JpegImageFile image mode=...    None   \n\n  channel_errortype         channel_errormsg  \n0        ValueError  band index out of range  \n1        ValueError  band index out of range  \n2        ValueError  band index out of range  \n3        ValueError  band index out of range  \n4        ValueError  band index out of range  "
        },
        {
          "code": "t",
          "context": "We've seen that every column and every expression in Pixeltable has an associated __Pixeltable type__. In this section, we'll briefly survey the various Pixeltable types and their uses.\n\nHere are all the supported types and their corresponding Python types:\n\n| Pixeltable type | Python type |\n|-----------------|-------------|\n| pxt.String | str |\n| pxt.Int | int |\n| pxt.Float | float |\n| pxt.Bool | bool |\n| pxt.Timestamp | datetime.datetime |\n| pxt.Json | ** |\n| pxt.Array | np.ndarray |\n| pxt.Image | PIL.Image.Image |\n| pxt.Video | str |\n| pxt.Audio | str |\n| pxt.Document | str |\n\nThe Python type is what you'll get back if you query an expression of the given Pixeltable type. For `pxt.Json`, it can be any of `str`, `int`, `float`, `bool`, `list`, or `dict`.\n\n<div class=\"alert alert-block alert-info\">\n<code>pxt.Audio</code>, <code>pxt.Video</code>, and <code>pxt.Document</code> all correspond to the Python type <code>str</code>. This is because those types are represented by file paths that reference the media in question. When you query for, say, <code>t.select(t.video_col)</code>, you're guaranteed to get a file path on the <i>local</i> filesystem (Pixeltable will download and cache a local copy of the video if necessary to ensure this). If you want the original URL, use <code>t.video_col.fileurl</code> instead.\n</div>\nSeveral types can be __specialized__ to constrain the allowable data in a column.\n\n- `pxt.Image` can be specialized with a resolution and/or an image mode:\n  - `pxt.Image[(300,200)]` - images with width 300 and height 200\n  - `pxt.Image['RGB']` - images with mode `'RGB'`; see the [PIL Documentation](https://pillow.readthedocs.io/en/stable/handbook/concepts.html) for the full list\n  - `pxt.Image[(300,200), 'RGB']` - combines the above constraints\n- `pxt.Array` can be specialized with a shape and/or a dtype:\n  - `pxt.Array[pxt.Float]` - arrays with dtype `pxt.Float`\n  - `pxt.Array[(64,64,3), pxt.Float]` - 3-dimensional arrays with dtype `pxt.Float` and 64x64x3 shape\n\nIf we look at the structure of our table now, we see examples of specialized image and array types.",
          "output": "Table\n'demo.mnist'\n\n     Column Name                  Type                                      Computed With\n           image                 Image                                                   \n           label                String                                                   \n  classification                  Json  vit_for_image_classification(image, model_id='...\n      pred_label                  Json                       classification.label_text[0]\n            clip  Array[(512,), Float]  clip(image, model_id='openai/clip-vit-base-pat...\n         channel            Image['L']                                image.getchannel(1)"
        }
      ]
    },
    {
      "notebook": "fundamentals/tables-and-data-operations.ipynb",
      "title": "Pixeltable Fundamentals",
      "description": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/fundamentals/tables-and-data-operations.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/fundamentals/tables-and-data-operations.ipynb) Welcome to Section 1 of the Pixeltable Fundamentals tutorial, __Tables and Data Operations__. In this section, we'll learn how to: - Create and manage tables: Understand Pixeltable's table structure, create and modify tables, and work with table schemas",
      "workflows": [],
      "api_usage": {},
      "key_concepts": [
        "incremental",
        "computed column",
        "multimodal",
        "iterator"
      ],
      "code_snippets": [
        {
          "code": "%pip install -qU pixeltable",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/fundamentals/tables-and-data-operations.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/fundamentals/tables-and-data-operations.ipynb)\n# Pixeltable Fundamentals\n\n## Section 1: Tables and Data Operations\n\nWelcome to Section 1 of the Pixeltable Fundamentals tutorial, __Tables and Data Operations__. In this section, we'll learn how to:\n\n- Create and manage tables: Understand Pixeltable's table structure, create and modify tables, and work with table schemas\n- Manipulate data: Insert, update, and delete data within tables, and retrieve data from tables into Python variables\n- Filter and select data: Use `where()`, `select()`, and `order_by()` to query for specific rows and columns\n- Import data from CSV files and other file types\n\nFirst, let's ensure the Pixeltable library is installed in your environment.",
          "output": null
        },
        {
          "code": "import pixeltable as pxt\n\n# First we delete the `fundamentals` directory and all its contents (if\n# it exists), in order to ensure a clean environment for the tutorial.\npxt.drop_dir('fundamentals', force=True)\n\n# Now we create the directory.\npxt.create_dir('fundamentals')",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/fundamentals/tables-and-data-operations.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/fundamentals/tables-and-data-operations.ipynb)\n# Pixeltable Fundamentals\n\n## Section 1: Tables and Data Operations\n\nWelcome to Section 1 of the Pixeltable Fundamentals tutorial, __Tables and Data Operations__. In this section, we'll learn how to:\n\n- Create and manage tables: Understand Pixeltable's table structure, create and modify tables, and work with table schemas\n- Manipulate data: Insert, update, and delete data within tables, and retrieve data from tables into Python variables\n- Filter and select data: Use `where()`, `select()`, and `order_by()` to query for specific rows and columns\n- Import data from CSV files and other file types\n\nFirst, let's ensure the Pixeltable library is installed in your environment.\n### Tables\n\nAll data in Pixeltable is stored in tables. At a high level, a Pixeltable table behaves similarly to an ordinary SQL database table, but with many additional capabilities to support complex AI workflows. We'll introduce those advanced capabilities gradually throughout this tutorial; in this section, the focus is on basic table and data operations.\n\nTables in Pixeltable are grouped into __directories__, which are simply user-defined namespaces. The following command creates a new directory, `fundamentals`, which we'll use to store the tables in our tutorial.",
          "output": "Connected to Pixeltable database at: postgresql+psycopg://postgres:@/pixeltable?host=/Users/asiegel/.pixeltable/pgdata\nCreated directory `fundamentals`.\n"
        },
        {
          "code": "films_t = pxt.create_table('fundamentals.films', {\n    'film_name': pxt.String,\n    'year': pxt.Int,\n    'revenue': pxt.Float\n})",
          "context": "### Tables\n\nAll data in Pixeltable is stored in tables. At a high level, a Pixeltable table behaves similarly to an ordinary SQL database table, but with many additional capabilities to support complex AI workflows. We'll introduce those advanced capabilities gradually throughout this tutorial; in this section, the focus is on basic table and data operations.\n\nTables in Pixeltable are grouped into __directories__, which are simply user-defined namespaces. The following command creates a new directory, `fundamentals`, which we'll use to store the tables in our tutorial.\nNow let's create our first table. To create a table, we must give it a name and a __schema__ that describes the table structure. Note that prefacing the name with `fundamentals` causes it to be placed in our newly-created directory.",
          "output": "Created table `films`.\n"
        },
        {
          "code": "films_t.insert([\n    {'film_name': 'Jurassic Park', 'year': 1993, 'revenue': 1037.5},\n    {'film_name': 'Titanic', 'year': 1997, 'revenue': 2257.8},\n    {'film_name': 'Avengers: Endgame', 'year': 2019, 'revenue': 2797.5}\n])",
          "context": "Now let's create our first table. To create a table, we must give it a name and a __schema__ that describes the table structure. Note that prefacing the name with `fundamentals` causes it to be placed in our newly-created directory.\nTo insert data into a table, we use the `insert()` method, passing it a list of Python dicts.",
          "output": "Inserting rows into `films`: 3 rows [00:00, 490.68 rows/s]\nInserted 3 rows with 0 errors.\n"
        },
        {
          "code": "films_t.insert(film_name='Inside Out 2', year=2024, revenue=1462.7)",
          "context": "To insert data into a table, we use the `insert()` method, passing it a list of Python dicts.\nIf you're inserting just a single row, you can use an alternate syntax that is sometimes more convenient.",
          "output": "Inserting rows into `films`: 1 rows [00:00, 386.54 rows/s]\nInserted 1 row with 0 errors.\n"
        },
        {
          "code": "films_t.collect()",
          "context": "If you're inserting just a single row, you can use an alternate syntax that is sometimes more convenient.\nWe can peek at the data in our table with the `collect()` method, which retrieves all the rows in the table.",
          "output": "           film_name  year  revenue\n0      Jurassic Park  1993   1037.5\n1            Titanic  1997   2257.8\n2  Avengers: Endgame  2019   2797.5\n3       Inside Out 2  2024   1462.7"
        },
        {
          "code": "films_t.where(films_t.revenue >= 2000.0).collect()",
          "context": "Pixeltable also provides `update()` and `delete()` methods for modifying and removing data from a table; we'll see examples of them shortly.\n### Filtering and Selecting Data\n\nOften you want to select only certain rows and/or certain columns in a table. You can do this with the `where()` and `select()` methods.",
          "output": "           film_name  year  revenue\n0            Titanic  1997   2257.8\n1  Avengers: Endgame  2019   2797.5"
        },
        {
          "code": "films_t.select(films_t.film_name, films_t.year).collect()",
          "context": "Pixeltable also provides `update()` and `delete()` methods for modifying and removing data from a table; we'll see examples of them shortly.\n### Filtering and Selecting Data\n\nOften you want to select only certain rows and/or certain columns in a table. You can do this with the `where()` and `select()` methods.",
          "output": "           film_name  year\n0      Jurassic Park  1993\n1            Titanic  1997\n2  Avengers: Endgame  2019\n3       Inside Out 2  2024"
        },
        {
          "code": "films_t.select(films_t['film_name'], films_t['year']).collect()",
          "context": "### Filtering and Selecting Data\n\nOften you want to select only certain rows and/or certain columns in a table. You can do this with the `where()` and `select()` methods.\nNote the expressions that appear inside the calls to `where()` and `select()`, such as `films_t.year`. These are __column references__ that point to specific columns within a table. In place of `films_t.year`, you can also use dictionary syntax and type `films_t['year']`, which means exactly the same thing but is sometimes more convenient.",
          "output": "           film_name  year\n0      Jurassic Park  1993\n1            Titanic  1997\n2  Avengers: Endgame  2019\n3       Inside Out 2  2024"
        },
        {
          "code": "films_t.select(films_t.film_name, films_t.revenue * 1000).collect()",
          "context": "Note the expressions that appear inside the calls to `where()` and `select()`, such as `films_t.year`. These are __column references__ that point to specific columns within a table. In place of `films_t.year`, you can also use dictionary syntax and type `films_t['year']`, which means exactly the same thing but is sometimes more convenient.\nIn addition to selecting columns directly, you can use column references inside various kinds of expressions. For example, our `revenue` numbers are given in millions of dollars. Let's say we wanted to select revenue in thousands of dollars instead; we could do that as follows:",
          "output": "           film_name      col_1\n0      Jurassic Park  1037500.0\n1            Titanic  2257800.0\n2  Avengers: Endgame  2797500.0\n3       Inside Out 2  1462700.0"
        },
        {
          "code": "films_t.select(films_t.film_name, revenue_thousands=films_t.revenue * 1000).collect()",
          "context": "In addition to selecting columns directly, you can use column references inside various kinds of expressions. For example, our `revenue` numbers are given in millions of dollars. Let's say we wanted to select revenue in thousands of dollars instead; we could do that as follows:\nNote that since we selected an abstract expression rather than a specific column, Pixeltable gave it the generic name `col_1`. You can assign it a more informative name with Python keyword syntax:",
          "output": "           film_name  revenue_thousands\n0      Jurassic Park          1037500.0\n1            Titanic          2257800.0\n2  Avengers: Endgame          2797500.0\n3       Inside Out 2          1462700.0"
        },
        {
          "code": "%reset -f\nfilms_t.collect()  # Throws an exception now",
          "context": "### Tables are Persistent\nThis is a good time to mention a few key differences between Pixeltable tables and other familiar datastructures, such as Python dicts or Pandas dataframes.\n\nFirst, **Pixeltable is persistent. Unlike in-memory Python libraries such as Pandas, Pixeltable is a database**. When you reset a notebook kernel or start a new Python session, you'll have access to all the data you've stored previously in Pixeltable. Let's demonstrate this by using the IPython `%reset -f` command to clear out all our notebook variables, so that `films_t` is no longer defined.",
          "output": null
        },
        {
          "code": "import pixeltable as pxt\n\nfilms_t = pxt.get_table('fundamentals.films')\nfilms_t.collect()",
          "context": "This is a good time to mention a few key differences between Pixeltable tables and other familiar datastructures, such as Python dicts or Pandas dataframes.\n\nFirst, **Pixeltable is persistent. Unlike in-memory Python libraries such as Pandas, Pixeltable is a database**. When you reset a notebook kernel or start a new Python session, you'll have access to all the data you've stored previously in Pixeltable. Let's demonstrate this by using the IPython `%reset -f` command to clear out all our notebook variables, so that `films_t` is no longer defined.\nThe `films_t` variable (along with all other variables in our Python session) has been cleared out - but that's ok, because it wasn't the source of record for our data. The `films_t` variable is just a reference to the underlying database table. We can recover it with the `get_table` command, referencing the `films` table by name.",
          "output": "           film_name  year  revenue\n0      Jurassic Park  1993   1037.5\n1            Titanic  1997   2257.8\n2  Avengers: Endgame  2019   2797.5\n3       Inside Out 2  2024   1462.7"
        },
        {
          "code": "pxt.list_tables()",
          "context": "The `films_t` variable (along with all other variables in our Python session) has been cleared out - but that's ok, because it wasn't the source of record for our data. The `films_t` variable is just a reference to the underlying database table. We can recover it with the `get_table` command, referencing the `films` table by name.\nYou can always get a list of all existing tables with the Pixeltable `list_tables` command.",
          "output": "['fundamentals.films']"
        },
        {
          "code": "films_t.describe()",
          "context": "<div class=\"alert alert-block alert-info\">\nNote that if you're running Pixeltable on colab or kaggle, the database will persist only for as long as your colab/kaggle session remains active. If you're running it locally or on your own server, then your database will persist indefinitely (until you actively delete it).\n</div>\n### Tables are Typed\n\nThe second major difference is that **Pixeltable is strongly typed**. Because Pixeltable is a database, every column has a data type: that's why we specified `String`, `Int`, and `Float` for the three columns when we created the table. These __type specifiers__ are _mandatory_ when creating tables, and they become part of the table schema. You can always see the table schema with the `describe()` method.",
          "output": null
        },
        {
          "code": "films_t",
          "context": "### Tables are Typed\n\nThe second major difference is that **Pixeltable is strongly typed**. Because Pixeltable is a database, every column has a data type: that's why we specified `String`, `Int`, and `Float` for the three columns when we created the table. These __type specifiers__ are _mandatory_ when creating tables, and they become part of the table schema. You can always see the table schema with the `describe()` method.\nIn a notebook, you can also just type `films_t` to see the schema; its output is identical to `films_t.describe()`.",
          "output": "table 'films'\n\nColumn Name   Type Computed With\n  film_name String              \n       year    Int              \n    revenue  Float              "
        },
        {
          "code": "eq_t = pxt.io.import_csv(\n    'fundamentals.earthquakes',  # Name for the new table\n    'https://raw.githubusercontent.com/pixeltable/pixeltable/release/docs/resources/earthquakes.csv',\n    primary_key='id',  # Column 'id' is the primary key\n    parse_dates=[3]  # Interpret column 3 as a timestamp\n)",
          "context": "All of the methods we've discussed so far, such as `insert()` and `get_table()`, are documented in the [Pixeltable API](https://pixeltable.github.io/pixeltable/) Documentation. The following pages are particularly relevant to this section of the tutorial:\n- [pixeltable](https://pixeltable.github.io/pixeltable/api/pixeltable/) package reference\n- [pxt.Table](https://pixeltable.github.io/pixeltable/api/table/) class reference\n- [API Cheat Sheet](https://pixeltable.github.io/pixeltable/api-cheat-sheet/)\n### A Real-World Example: Earthquake Data\n\nNow let's dive a little deeper into Pixeltable's data operations. To showcase all the features, it'll be helpful to have a real-world dataset, rather than our toy dataset with four movies. The dataset we'll be using consists of Earthquake data drawn from the US Geological Survey: all recorded Earthquakes that occurred within 100 km of Seattle, Washington, between January 1, 2023 and June 30, 2024.\n\nThe dataset is in CSV format, and we can load it into Pixeltable with the handy `import_csv()` function, which creates a new Pixeltable table from the contents of a CSV file.",
          "output": "Created table `earthquakes`.\nInserting rows into `earthquakes`: 1823 rows [00:00, 25263.30 rows/s]\nInserted 1823 rows with 0 errors.\n"
        },
        {
          "code": "eq_t.limit(5).collect()",
          "context": "### A Real-World Example: Earthquake Data\n\nNow let's dive a little deeper into Pixeltable's data operations. To showcase all the features, it'll be helpful to have a real-world dataset, rather than our toy dataset with four movies. The dataset we'll be using consists of Earthquake data drawn from the US Geological Survey: all recorded Earthquakes that occurred within 100 km of Seattle, Washington, between January 1, 2023 and June 30, 2024.\n\nThe dataset is in CSV format, and we can load it into Pixeltable with the handy `import_csv()` function, which creates a new Pixeltable table from the contents of a CSV file.\n<div class=\"alert alert-block alert-info\">\nIn Pixeltable, you can always import external data by giving a URL instead of a local file path. This applies to CSV datasets, media files (such images and video), and other types of content. The URL will often be an <code>http://</code> URL, but it can also be an <code>s3://</code> URL referencing an S3 bucket.\n</div>\n\n<div class=\"alert alert-block alert-info\">\nIn addition to <code>import_csv</code>, Pixeltable provides a variety of other importers for structured data, such as <code>import_pandas</code> (which creates a table from a Pandas dataframe) and <code>import_excel</code>. These importers are described in the <a href=\"https://pixeltable.github.io/pixeltable/api/io/\">pixeltable.io package reference</a>.\n</div>\n\nLet's have a peek at our new dataset. The dataset contains 1823 rows, and we probably don't want to display them all at once. We can limit our query to fewer rows with the `limit()` method.",
          "output": "   id  magnitude                           location  \\\n0   0       1.15    10 km NW of Belfair, Washington   \n1   1       0.29   23 km ENE of Ashford, Washington   \n2   2       0.20   23 km ENE of Ashford, Washington   \n3   3       0.52   15 km NNE of Ashford, Washington   \n4   4       1.56  0 km WSW of Esperance, Washington   \n\n                         timestamp  longitude  latitude  \n0 2023-01-01 08:10:37.050000-08:00    -122.93     47.51  \n1 2023-01-02 01:02:43.950000-08:00    -121.76     46.85  \n2 2023-01-02 12:05:01.420000-08:00    -121.75     46.86  \n3 2023-01-02 12:45:14.220000-08:00    -121.95     46.89  \n4 2023-01-02 13:19:27.200000-08:00    -122.36     47.79  "
        },
        {
          "code": "eq_t.head(5)",
          "context": "<div class=\"alert alert-block alert-info\">\nIn Pixeltable, you can always import external data by giving a URL instead of a local file path. This applies to CSV datasets, media files (such images and video), and other types of content. The URL will often be an <code>http://</code> URL, but it can also be an <code>s3://</code> URL referencing an S3 bucket.\n</div>\n\n<div class=\"alert alert-block alert-info\">\nIn addition to <code>import_csv</code>, Pixeltable provides a variety of other importers for structured data, such as <code>import_pandas</code> (which creates a table from a Pandas dataframe) and <code>import_excel</code>. These importers are described in the <a href=\"https://pixeltable.github.io/pixeltable/api/io/\">pixeltable.io package reference</a>.\n</div>\n\nLet's have a peek at our new dataset. The dataset contains 1823 rows, and we probably don't want to display them all at once. We can limit our query to fewer rows with the `limit()` method.\nA different way of achieving something similar is to use the `head()` and `tail()` methods. Pixeltable keeps track of the insertion order of all its data, and `head()` and `tail()` will always return the _earliest inserted_ and _most recently inserted_ rows in a table, respectively.",
          "output": "   id  magnitude                           location  \\\n0   0       1.15    10 km NW of Belfair, Washington   \n1   1       0.29   23 km ENE of Ashford, Washington   \n2   2       0.20   23 km ENE of Ashford, Washington   \n3   3       0.52   15 km NNE of Ashford, Washington   \n4   4       1.56  0 km WSW of Esperance, Washington   \n\n                         timestamp  longitude  latitude  \n0 2023-01-01 08:10:37.050000-08:00    -122.93     47.51  \n1 2023-01-02 01:02:43.950000-08:00    -121.76     46.85  \n2 2023-01-02 12:05:01.420000-08:00    -121.75     46.86  \n3 2023-01-02 12:45:14.220000-08:00    -121.95     46.89  \n4 2023-01-02 13:19:27.200000-08:00    -122.36     47.79  "
        },
        {
          "code": "eq_t.tail(5)",
          "context": "<div class=\"alert alert-block alert-info\">\nIn Pixeltable, you can always import external data by giving a URL instead of a local file path. This applies to CSV datasets, media files (such images and video), and other types of content. The URL will often be an <code>http://</code> URL, but it can also be an <code>s3://</code> URL referencing an S3 bucket.\n</div>\n\n<div class=\"alert alert-block alert-info\">\nIn addition to <code>import_csv</code>, Pixeltable provides a variety of other importers for structured data, such as <code>import_pandas</code> (which creates a table from a Pandas dataframe) and <code>import_excel</code>. These importers are described in the <a href=\"https://pixeltable.github.io/pixeltable/api/io/\">pixeltable.io package reference</a>.\n</div>\n\nLet's have a peek at our new dataset. The dataset contains 1823 rows, and we probably don't want to display them all at once. We can limit our query to fewer rows with the `limit()` method.\nA different way of achieving something similar is to use the `head()` and `tail()` methods. Pixeltable keeps track of the insertion order of all its data, and `head()` and `tail()` will always return the _earliest inserted_ and _most recently inserted_ rows in a table, respectively.",
          "output": "     id  magnitude                             location  \\\n0  1818       1.70     14 km W of Skokomish, Washington   \n1  1819       1.06  7 km E of Lake McMurray, Washington   \n2  1820       0.48         4 km E of Duvall, Washington   \n3  1821       0.46      12 km NE of Ashford, Washington   \n4  1822       0.72          6 km ENE of Oso, Washington   \n\n                         timestamp  longitude  latitude  \n0 2024-06-29 08:55:50.030000-07:00    -123.35     47.32  \n1 2024-06-29 12:15:19.130000-07:00    -122.13     48.31  \n2 2024-06-30 09:15:43.020000-07:00    -121.93     47.75  \n3 2024-06-30 10:05:15.410000-07:00    -121.93     46.84  \n4 2024-06-30 11:12:41.900000-07:00    -121.84     48.28  "
        },
        {
          "code": "eq_t.describe()",
          "context": "<div class=\"alert alert-block alert-info\">\n<code>head(n)</code> and <code>limit(n).collect()</code> appear similar in this example. But <code>head()</code> always returns the <i>earliest</i> rows in a table, whereas <code>limit()</code> makes no promises about the ordering of its results (unless you specify an <code>order_by()</code> clause - more on this below).\n</div>\nLet's also peek at the schema:",
          "output": null
        },
        {
          "code": "eq_t.count()  # Number of rows in the table",
          "context": "Let's also peek at the schema:\nNote that while specifying a schema is mandatory when _creating_ a table, it's not always required when _importing_ data. This is because Pixeltable uses the structure of the imported data to infer the column types, when feasible. You can always override the inferred column types with the `schema_overrides` parameter of `import_csv()`.\n\nThe following examples showcase some common data operations.",
          "output": "1823"
        },
        {
          "code": "# 5 highest-magnitude earthquakes\n\neq_t.order_by(eq_t.magnitude, asc=False).limit(5).collect()",
          "context": "Let's also peek at the schema:\nNote that while specifying a schema is mandatory when _creating_ a table, it's not always required when _importing_ data. This is because Pixeltable uses the structure of the imported data to infer the column types, when feasible. You can always override the inferred column types with the `schema_overrides` parameter of `import_csv()`.\n\nThe following examples showcase some common data operations.",
          "output": "     id  magnitude                            location  \\\n0  1002       4.30                   Port Townsend, WA   \n1  1226       4.04      6 km W of Quilcene, Washington   \n2   699       3.91  9 km NNE of Snoqualmie, Washington   \n3  1281       3.48  7 km SSW of River Road, Washington   \n4  1355       3.42    17 km WSW of Brinnon, Washington   \n\n                         timestamp  longitude  latitude  \n0 2023-10-09 02:21:08.960000-07:00    -122.73     48.04  \n1 2023-12-24 15:14:04.220000-08:00    -122.96     47.82  \n2 2023-08-08 10:17:23.910000-07:00    -121.77     47.60  \n3 2024-01-15 07:25:05.920000-08:00    -123.17     48.00  \n4 2024-02-16 16:30:18.830000-08:00    -123.09     47.59  "
        },
        {
          "code": "from datetime import datetime\n\n# 5 highest-magnitude earthquakes in Q3 2023\n\neq_t.where((eq_t.timestamp >= datetime(2023, 6, 1)) & (eq_t.timestamp < datetime(2023, 10, 1))) \\\n  .order_by(eq_t.magnitude, asc=False).limit(5).collect()",
          "context": "Let's also peek at the schema:\nNote that while specifying a schema is mandatory when _creating_ a table, it's not always required when _importing_ data. This is because Pixeltable uses the structure of the imported data to infer the column types, when feasible. You can always override the inferred column types with the `schema_overrides` parameter of `import_csv()`.\n\nThe following examples showcase some common data operations.",
          "output": "    id  magnitude                             location  \\\n0  699       3.91   9 km NNE of Snoqualmie, Washington   \n1  799       2.86        5 km E of Ashford, Washington   \n2  710       2.84    8 km ENE of Fall City, Washington   \n3  577       2.79  0 km NE of Maple Valley, Washington   \n4  769       2.73      16 km NE of Ashford, Washington   \n\n                         timestamp  longitude  latitude  \n0 2023-08-08 10:17:23.910000-07:00    -121.77     47.60  \n1 2023-08-27 10:10:23.770000-07:00    -121.96     46.77  \n2 2023-08-08 11:51:12.750000-07:00    -121.79     47.60  \n3 2023-07-04 15:52:54.430000-07:00    -122.04     47.40  \n4 2023-08-22 23:44:12.250000-07:00    -121.88     46.87  "
        },
        {
          "code": "# Earthquakes with specific ids\n\neq_t.where(eq_t.id.isin([123,456,789])).collect()",
          "context": "Note that while specifying a schema is mandatory when _creating_ a table, it's not always required when _importing_ data. This is because Pixeltable uses the structure of the imported data to infer the column types, when feasible. You can always override the inferred column types with the `schema_overrides` parameter of `import_csv()`.\n\nThe following examples showcase some common data operations.\nNote that Pixeltable uses Pandas-like operators for filtering data: the expression\n\n```python\n(eq_t.timestamp >= datetime(2023, 6, 1)) & (eq_t.timestamp < datetime(2023, 10, 1))\n```\n\nmeans _both_ conditions must be true; similarly (say),\n\n```python\n(eq_t.timestamp < datetime(2023, 6, 1)) | (eq_t.timestamp >= datetime(2023, 10, 1))\n```\n\nwould mean _either_ condition must be true.\n\nYou can also use the special `isin` operator to select just those values that appear within a particular list:",
          "output": "    id  magnitude                        location  \\\n0  123       1.23  7 km SW of Rainier, Washington   \n1  456       0.23                      Washington   \n2  789       1.67  Puget Sound region, Washington   \n\n                         timestamp  longitude  latitude  \n0 2023-02-17 00:28:25.460000-08:00    -122.75     46.84  \n1 2023-05-23 08:49:02.450000-07:00    -121.98     46.87  \n2 2023-08-26 04:04:11.200000-07:00    -122.57     47.60  "
        },
        {
          "code": "# All earthquakes in the vicinity of Rainier\n\neq_t.where(eq_t.location.contains('Rainier')).collect()",
          "context": "Note that Pixeltable uses Pandas-like operators for filtering data: the expression\n\n```python\n(eq_t.timestamp >= datetime(2023, 6, 1)) & (eq_t.timestamp < datetime(2023, 10, 1))\n```\n\nmeans _both_ conditions must be true; similarly (say),\n\n```python\n(eq_t.timestamp < datetime(2023, 6, 1)) | (eq_t.timestamp >= datetime(2023, 10, 1))\n```\n\nwould mean _either_ condition must be true.\n\nYou can also use the special `isin` operator to select just those values that appear within a particular list:\nIn addition to basic operators like `>=` and `isin`, a Pixeltable `where` clause can also contain more complex operations. For example, the `location` column in our dataset is a string that contains a lot of information, but in a relatively unstructured way. Suppose we wanted to see all Earthquakes in the vicinity of Rainier, Washington; one way to do this is with the `contains()` method:",
          "output": "     id  magnitude                          location  \\\n0    40       1.22  11 km SSE of Rainier, Washington   \n1    85       1.45  10 km SSE of Rainier, Washington   \n2   123       1.23    7 km SW of Rainier, Washington   \n3   467       1.09  10 km SSE of Rainier, Washington   \n4  1399       1.08    5 km SW of Rainier, Washington   \n5  1709       1.16    10 km S of Rainier, Washington   \n6  1776       1.17    12 km S of Rainier, Washington   \n\n                         timestamp  longitude  latitude  \n0 2023-01-19 21:52:29.910000-08:00    -122.65     46.79  \n1 2023-02-02 20:08:27.810000-08:00    -122.65     46.79  \n2 2023-02-17 00:28:25.460000-08:00    -122.75     46.84  \n3 2023-05-26 19:39:44.120000-07:00    -122.65     46.80  \n4 2024-03-04 22:34:25.210000-08:00    -122.74     46.85  \n5 2024-05-22 18:28:38.130000-07:00    -122.68     46.79  \n6 2024-06-17 18:25:33.400000-07:00    -122.66     46.77  "
        },
        {
          "code": "# Min and max ids\n\neq_t.select(min=pxt.functions.min(eq_t.id), max=pxt.functions.max(eq_t.id)).collect()",
          "context": "In addition to basic operators like `>=` and `isin`, a Pixeltable `where` clause can also contain more complex operations. For example, the `location` column in our dataset is a string that contains a lot of information, but in a relatively unstructured way. Suppose we wanted to see all Earthquakes in the vicinity of Rainier, Washington; one way to do this is with the `contains()` method:\nPixeltable also supports various __aggregators__; here's an example showcasing two fairly simple ones, `max()` and `min()`:",
          "output": "   min   max\n0    0  1822"
        },
        {
          "code": "result = eq_t.limit(5).collect()\nresult[0]  # Get the first row of the results as a dict",
          "context": "### Extracting Data from Tables into Python/Pandas\nSometimes it's handy to pull out data from a table into a Python object. We've actually already done this; the call to `collect()` returns an in-memory result set, which we can then dereference in various ways. For example:",
          "output": "{'id': 0,\n 'magnitude': 1.15,\n 'location': '10 km NW of Belfair, Washington',\n 'timestamp': datetime.datetime(2023, 1, 1, 8, 10, 37, 50000, tzinfo=zoneinfo.ZoneInfo(key='America/Los_Angeles')),\n 'longitude': -122.93,\n 'latitude': 47.51}"
        },
        {
          "code": "result['timestamp']  # Get a list of the `timestamp` field of all the rows that were queried",
          "context": "### Extracting Data from Tables into Python/Pandas\nSometimes it's handy to pull out data from a table into a Python object. We've actually already done this; the call to `collect()` returns an in-memory result set, which we can then dereference in various ways. For example:",
          "output": "[datetime.datetime(2023, 1, 1, 8, 10, 37, 50000, tzinfo=zoneinfo.ZoneInfo(key='America/Los_Angeles')),\n datetime.datetime(2023, 1, 2, 1, 2, 43, 950000, tzinfo=zoneinfo.ZoneInfo(key='America/Los_Angeles')),\n datetime.datetime(2023, 1, 2, 12, 5, 1, 420000, tzinfo=zoneinfo.ZoneInfo(key='America/Los_Angeles')),\n datetime.datetime(2023, 1, 2, 12, 45, 14, 220000, tzinfo=zoneinfo.ZoneInfo(key='America/Los_Angeles')),\n datetime.datetime(2023, 1, 2, 13, 19, 27, 200000, tzinfo=zoneinfo.ZoneInfo(key='America/Los_Angeles'))]"
        },
        {
          "code": "df = result.to_pandas()  # Convert the result set into a Pandas dataframe\ndf['magnitude'].describe()",
          "context": "### Extracting Data from Tables into Python/Pandas\nSometimes it's handy to pull out data from a table into a Python object. We've actually already done this; the call to `collect()` returns an in-memory result set, which we can then dereference in various ways. For example:",
          "output": "count    5.000000\nmean     0.744000\nstd      0.587988\nmin      0.200000\n25%      0.290000\n50%      0.520000\n75%      1.150000\nmax      1.560000\nName: magnitude, dtype: float64"
        },
        {
          "code": "df = eq_t.collect().to_pandas()\ndf['magnitude'].describe()",
          "context": "Sometimes it's handy to pull out data from a table into a Python object. We've actually already done this; the call to `collect()` returns an in-memory result set, which we can then dereference in various ways. For example:\n`collect()` without a preceding `limit()` returns the entire contents of a query or table. Be careful! For very large tables, this could result in out-of-memory errors. In this example, the 1823 rows in the table fit comfortably into a dataframe.",
          "output": "count    1823.000000\nmean        0.900378\nstd         0.625492\nmin        -0.830000\n25%         0.420000\n50%         0.850000\n75%         1.310000\nmax         4.300000\nName: magnitude, dtype: float64"
        },
        {
          "code": "eq_t.add_column(note=pxt.String)",
          "context": "### Adding Columns\nLike other database tables, Pixeltable tables aren't fixed entities: they're meant to evolve over time. Suppose we want to add a new column to hold user-specified comments about particular earthquake events. We can do this with the `add_column()` method:",
          "output": "Added 1823 column values with 0 errors.\n"
        },
        {
          "code": "eq_t.add_column(contact_email=pxt.String)",
          "context": "Like other database tables, Pixeltable tables aren't fixed entities: they're meant to evolve over time. Suppose we want to add a new column to hold user-specified comments about particular earthquake events. We can do this with the `add_column()` method:\nHere, `note` is the column name, and `pxt.String` specifies the type of the new column.",
          "output": "Added 1823 column values with 0 errors.\n"
        },
        {
          "code": "eq_t.describe()",
          "context": "Here, `note` is the column name, and `pxt.String` specifies the type of the new column.\nLet's have a look at the revised schema.",
          "output": null
        },
        {
          "code": "# Add a comment to records with IDs 123 and 127\n\n(\n    eq_t\n    .where(eq_t.id.isin([121,123]))\n    .update({'note': 'Still investigating.', 'contact_email': 'contact@pixeltable.com'})\n)",
          "context": "### Updating Rows in a Table\nTable rows can be modified and deleted with the SQL-like `update()` and `delete()` commands.",
          "output": "Inserting rows into `earthquakes`: 2 rows [00:00, 638.55 rows/s]\n"
        },
        {
          "code": "eq_t.where(eq_t.id >= 120).select(eq_t.id, eq_t.magnitude, eq_t.note, eq_t.contact_email).head(5)",
          "context": "### Updating Rows in a Table\nTable rows can be modified and deleted with the SQL-like `update()` and `delete()` commands.",
          "output": "    id  magnitude                  note           contact_email\n0  120       1.17                  None                    None\n1  121       1.87  Still investigating.  contact@pixeltable.com\n2  122       0.34                  None                    None\n3  123       1.23  Still investigating.  contact@pixeltable.com\n4  124       0.13                  None                    None"
        },
        {
          "code": "eq_t.update({'location': eq_t.location.replace('Washington', 'WA')})",
          "context": "Table rows can be modified and deleted with the SQL-like `update()` and `delete()` commands.\n`update()` can also accept an expression, rather than a constant value. For example, suppose we wanted to shorten the location strings by replacing every occurrence of `Washington` with `WA`. One way to do this is with an `update()` clause, using a Pixeltable expression with the `replace()` method.",
          "output": "Inserting rows into `earthquakes`: 1823 rows [00:00, 19274.07 rows/s]\n"
        },
        {
          "code": "eq_t.head(5)",
          "context": "Table rows can be modified and deleted with the SQL-like `update()` and `delete()` commands.\n`update()` can also accept an expression, rather than a constant value. For example, suppose we wanted to shorten the location strings by replacing every occurrence of `Washington` with `WA`. One way to do this is with an `update()` clause, using a Pixeltable expression with the `replace()` method.",
          "output": "   id  magnitude                   location                        timestamp  \\\n0   0       1.15    10 km NW of Belfair, WA 2023-01-01 08:10:37.050000-08:00   \n1   1       0.29   23 km ENE of Ashford, WA 2023-01-02 01:02:43.950000-08:00   \n2   2       0.20   23 km ENE of Ashford, WA 2023-01-02 12:05:01.420000-08:00   \n3   3       0.52   15 km NNE of Ashford, WA 2023-01-02 12:45:14.220000-08:00   \n4   4       1.56  0 km WSW of Esperance, WA 2023-01-02 13:19:27.200000-08:00   \n\n   longitude  latitude  note contact_email  \n0    -122.93     47.51  None          None  \n1    -121.76     46.85  None          None  \n2    -121.75     46.86  None          None  \n3    -121.95     46.89  None          None  \n4    -122.36     47.79  None          None  "
        },
        {
          "code": "updates = [\n    {'id': 500, 'note': 'This is an example note.'},\n    {'id': 501, 'note': 'This is a different note.'},\n    {'id': 502, 'note': 'A third note, unrelated to the others.'}\n]\neq_t.batch_update(updates)",
          "context": "`update()` can also accept an expression, rather than a constant value. For example, suppose we wanted to shorten the location strings by replacing every occurrence of `Washington` with `WA`. One way to do this is with an `update()` clause, using a Pixeltable expression with the `replace()` method.\nNotice that in all cases, the `update()` clause takes a Python dictionary, but its values can be either constants such as `'contact@pixeltable.com'`, or more complex expressions such as `eq_t.location.replace('Washington', 'WA')`. Also notice that if `update()` appears without a `where()` clause, then every row in the table will be updated, as in the preceding example.\n\n### Batch Updates\n\nThe `batch_update()` method provides an alternative way to update multiple rows with different values. With a `batch_update()`, the contents of each row are specified by individual `dict`s, rather than according to a formula. Here's a toy example that shows `batch_update()` in action.",
          "output": "Inserting rows into `earthquakes`: 3 rows [00:00, 1600.88 rows/s]\n"
        },
        {
          "code": "eq_t.where(eq_t.id >= 500).select(eq_t.id, eq_t.magnitude, eq_t.note, eq_t.contact_email).head(5)",
          "context": "`update()` can also accept an expression, rather than a constant value. For example, suppose we wanted to shorten the location strings by replacing every occurrence of `Washington` with `WA`. One way to do this is with an `update()` clause, using a Pixeltable expression with the `replace()` method.\nNotice that in all cases, the `update()` clause takes a Python dictionary, but its values can be either constants such as `'contact@pixeltable.com'`, or more complex expressions such as `eq_t.location.replace('Washington', 'WA')`. Also notice that if `update()` appears without a `where()` clause, then every row in the table will be updated, as in the preceding example.\n\n### Batch Updates\n\nThe `batch_update()` method provides an alternative way to update multiple rows with different values. With a `batch_update()`, the contents of each row are specified by individual `dict`s, rather than according to a formula. Here's a toy example that shows `batch_update()` in action.",
          "output": "    id  magnitude                                    note contact_email\n0  500       0.75                This is an example note.          None\n1  501       0.23               This is a different note.          None\n2  502       0.43  A third note, unrelated to the others.          None\n3  503       0.31                                    None          None\n4  504       0.35                                    None          None"
        },
        {
          "code": "# Delete all rows in 2024\n\neq_t.where(eq_t.timestamp >= datetime(2024, 1, 1)).delete()",
          "context": "Notice that in all cases, the `update()` clause takes a Python dictionary, but its values can be either constants such as `'contact@pixeltable.com'`, or more complex expressions such as `eq_t.location.replace('Washington', 'WA')`. Also notice that if `update()` appears without a `where()` clause, then every row in the table will be updated, as in the preceding example.\n\n### Batch Updates\n\nThe `batch_update()` method provides an alternative way to update multiple rows with different values. With a `batch_update()`, the contents of each row are specified by individual `dict`s, rather than according to a formula. Here's a toy example that shows `batch_update()` in action.\n### Deleting Rows\n\nTo delete rows from a table, use the `delete()` method.",
          "output": "UpdateStatus(num_rows=587, num_computed_values=0, num_excs=0, updated_cols=[], cols_with_excs=[])"
        },
        {
          "code": "eq_t.count()  # How many are left after deleting?",
          "context": "Notice that in all cases, the `update()` clause takes a Python dictionary, but its values can be either constants such as `'contact@pixeltable.com'`, or more complex expressions such as `eq_t.location.replace('Washington', 'WA')`. Also notice that if `update()` appears without a `where()` clause, then every row in the table will be updated, as in the preceding example.\n\n### Batch Updates\n\nThe `batch_update()` method provides an alternative way to update multiple rows with different values. With a `batch_update()`, the contents of each row are specified by individual `dict`s, rather than according to a formula. Here's a toy example that shows `batch_update()` in action.\n### Deleting Rows\n\nTo delete rows from a table, use the `delete()` method.",
          "output": "1236"
        },
        {
          "code": "eq_t.delete()",
          "context": "### Deleting Rows\n\nTo delete rows from a table, use the `delete()` method.\nDon't forget to specify a `where()` clause when using `delete()`! If you run `delete()` without a `where()` clause, the entire contents of the table will be deleted.",
          "output": "UpdateStatus(num_rows=1236, num_computed_values=0, num_excs=0, updated_cols=[], cols_with_excs=[])"
        },
        {
          "code": "eq_t.count()",
          "context": "### Deleting Rows\n\nTo delete rows from a table, use the `delete()` method.\nDon't forget to specify a `where()` clause when using `delete()`! If you run `delete()` without a `where()` clause, the entire contents of the table will be deleted.",
          "output": "0"
        },
        {
          "code": "eq_t.revert()",
          "context": "Don't forget to specify a `where()` clause when using `delete()`! If you run `delete()` without a `where()` clause, the entire contents of the table will be deleted.\n### Table Versioning\n\nEvery table in Pixeltable is versioned: some or all of its modification history is preserved. If you make a mistake, you can always call `revert()` to undo the previous change to a table. Let's try it out: we'll use it to revert the successive `delete()` calls that we just executed.",
          "output": null
        },
        {
          "code": "eq_t.count()",
          "context": "Don't forget to specify a `where()` clause when using `delete()`! If you run `delete()` without a `where()` clause, the entire contents of the table will be deleted.\n### Table Versioning\n\nEvery table in Pixeltable is versioned: some or all of its modification history is preserved. If you make a mistake, you can always call `revert()` to undo the previous change to a table. Let's try it out: we'll use it to revert the successive `delete()` calls that we just executed.",
          "output": "1236"
        },
        {
          "code": "eq_t.revert()",
          "context": "Don't forget to specify a `where()` clause when using `delete()`! If you run `delete()` without a `where()` clause, the entire contents of the table will be deleted.\n### Table Versioning\n\nEvery table in Pixeltable is versioned: some or all of its modification history is preserved. If you make a mistake, you can always call `revert()` to undo the previous change to a table. Let's try it out: we'll use it to revert the successive `delete()` calls that we just executed.",
          "output": null
        },
        {
          "code": "eq_t.count()",
          "context": "Don't forget to specify a `where()` clause when using `delete()`! If you run `delete()` without a `where()` clause, the entire contents of the table will be deleted.\n### Table Versioning\n\nEvery table in Pixeltable is versioned: some or all of its modification history is preserved. If you make a mistake, you can always call `revert()` to undo the previous change to a table. Let's try it out: we'll use it to revert the successive `delete()` calls that we just executed.",
          "output": "1823"
        },
        {
          "code": "# Add a new column of type `Image`\neq_t.add_column(map_image=pxt.Image)\neq_t.describe()",
          "context": "<div class=\"alert alert-block alert-info\">\nBe aware: calling <code>revert()</code> cannot be undone!\n</div>\n### Multimodal Data\n\nIn addition to the structured data we've been exploring so far in this tutorial, Pixeltable has native support for __media types__: images, video, audio, and unstructured documents such as pdfs. Media support is one of Pixeltable's core capabilities, and we'll have much more to say about it the upcoming Unstructured Data section of this tutorial. For now, we'll just give one example to show how media data lives side-by-side with structured data in Pixeltable.",
          "output": "Added 1823 column values with 0 errors.\n"
        },
        {
          "code": "# Update the row with id == 1002, adding an image to the `map_image` column\n\neq_t.where(eq_t.id == 1002).update(\n    {'map_image': 'https://raw.githubusercontent.com/pixeltable/pixeltable/release/docs/resources/port-townsend-map.jpeg'}\n)",
          "context": "<div class=\"alert alert-block alert-info\">\nBe aware: calling <code>revert()</code> cannot be undone!\n</div>\n### Multimodal Data\n\nIn addition to the structured data we've been exploring so far in this tutorial, Pixeltable has native support for __media types__: images, video, audio, and unstructured documents such as pdfs. Media support is one of Pixeltable's core capabilities, and we'll have much more to say about it the upcoming Unstructured Data section of this tutorial. For now, we'll just give one example to show how media data lives side-by-side with structured data in Pixeltable.",
          "output": "Inserting rows into `earthquakes`: 1 rows [00:00, 327.99 rows/s]\n"
        },
        {
          "code": "eq_t.where(eq_t.id >= 1000).select(eq_t.id, eq_t.magnitude, eq_t.location, eq_t.map_image).head(5)",
          "context": "### Multimodal Data\n\nIn addition to the structured data we've been exploring so far in this tutorial, Pixeltable has native support for __media types__: images, video, audio, and unstructured documents such as pdfs. Media support is one of Pixeltable's core capabilities, and we'll have much more to say about it the upcoming Unstructured Data section of this tutorial. For now, we'll just give one example to show how media data lives side-by-side with structured data in Pixeltable.\n<div class=\"alert alert-block alert-info\">\nNote that in Pixeltable, you can always insert images into a table by giving the file path or URL of the image (as a string). It's not necessary to load the image first; Pixeltable will manage the loading and caching of images in the background. The same applies to other media data such as documents and videos.\n</div>\n\nPixeltable will also embed image thumbnails in your notebook when you do a query:",
          "output": "     id  magnitude                    location  \\\n0  1000      -0.02  17 km SSE of Carbonado, WA   \n1  1001       0.82    22 km ENE of Ashford, WA   \n2  1002       4.30           Port Townsend, WA   \n3  1003       1.04                          WA   \n4  1004       0.79    24 km ENE of Ashford, WA   \n\n                                           map_image  \n0                                               None  \n1                                               None  \n2  <PIL.JpegImagePlugin.JpegImageFile image mode=...  \n3                                               None  \n4                                               None  "
        },
        {
          "code": "pxt.create_dir('fundamentals.subdir')\npxt.create_dir('fundamentals.subdir.subsubdir')\npxt.create_table('fundamentals.subdir.subsubdir.my_table', {'my_col': pxt.String})",
          "context": "<div class=\"alert alert-block alert-info\">\nNote that in Pixeltable, you can always insert images into a table by giving the file path or URL of the image (as a string). It's not necessary to load the image first; Pixeltable will manage the loading and caching of images in the background. The same applies to other media data such as documents and videos.\n</div>\n\nPixeltable will also embed image thumbnails in your notebook when you do a query:\n### Directory Hierarchies\n\nSo far we've only seen an example of a single directory with a table inside it, but one can also put directories inside other directories, in whatever fashion makes the most sense for a given application.",
          "output": "Created directory `fundamentals.subdir`.\nCreated directory `fundamentals.subdir.subsubdir`.\nCreated table `my_table`.\n"
        },
        {
          "code": "# Delete the `contact_email` column\n\neq_t.drop_column('contact_email')",
          "context": "### Deleting Columns, Tables, and Directories\n`drop_column()`, `drop_table()`, and `drop_dir()` are used to delete columns, tables, and directories, respectively.",
          "output": null
        },
        {
          "code": "eq_t.describe()",
          "context": "### Deleting Columns, Tables, and Directories\n`drop_column()`, `drop_table()`, and `drop_dir()` are used to delete columns, tables, and directories, respectively.",
          "output": null
        },
        {
          "code": "# Delete the entire table (cannot be reverted!)\n\npxt.drop_table('fundamentals.earthquakes')",
          "context": "### Deleting Columns, Tables, and Directories\n`drop_column()`, `drop_table()`, and `drop_dir()` are used to delete columns, tables, and directories, respectively.",
          "output": null
        },
        {
          "code": "# Delete the entire directory and all its contents, including any nested\n# subdirectories (cannot be reverted!)\n\npxt.drop_dir('fundamentals', force=True)",
          "context": "### Deleting Columns, Tables, and Directories\n`drop_column()`, `drop_table()`, and `drop_dir()` are used to delete columns, tables, and directories, respectively.",
          "output": null
        }
      ]
    },
    {
      "notebook": "feature-guides/embedding-indexes.ipynb",
      "title": "Working with Embedding/Vector Indexes",
      "description": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/feature-guides/embedding-indexes.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/feature-guides/embedding-indexes.ipynb) **If you are running this tutorial in Colab:** In order to make the tutorial run a bit snappier, let's switch to a GPU-equipped instance for this Colab session. To do that, click on the `Runtime -> Change runtime type` menu item at the top, then select the `GPU` radio button and click on `Save`.",
      "workflows": [
        {
          "title": "Creating an index",
          "explanation": [
            "The first parameter of `add_embedding_index()` is the name of the column being indexed; the `embed` parameter specifies the relevant embedding. Notice the notation we used:\n```python\nclip.using(model_id='openai/clip-vit-base-patch32')\n```\n`clip` is a general-purpose UDF that can accept any CLIP model available in the Hugging Face model repository. To define an embedding, however, we need to provide a specific embedding function to `add_embedding_index()`: a function that is _not_ parameterized on `model_id`. The `.using(model_id=...)` syntax tells Pixeltable to specialize the `clip` UDF by fixing the `model_id` parameter to the specific value `'openai/clip-vit-base-patch32'`.\n\n<div class=\"alert alert-block alert-info\">\nIf you're familiar with functional programming concepts, you might recognize <code>.using()</code> as a <b>partial function</b> operator. It's a general operator that can be applied to any UDF (not just embedding functions), transforming a UDF with <i>n</i> parameters into one with <i>k</i> parameters by fixing the values of <i>n</i>-<i>k</i> of its arguments. Python has something similar in the <code>functools</code> package: the <code><a href=\"https://docs.python.org/3/library/functools.html#functools.partial\">functools.partial()</a></code> operator.\n</div>",
            "`add_embedding_index()` provides a few other optional parameters:\n\n* `idx_name`: optional name for the index, which needs to be unique for the table; a default name is created if this isn't provided explicitly\n* `metric`: the metric to use to compute the similarity of two embedding vectors; one of:\n  * `'cosine'`: cosine distance (default)\n  * `'ip'`: inner product\n  * `'l2'`: L2 distance\n\nIf desired, you can create multiple indexes on the same column, using different embedding functions. This can be useful to evaluate the effectiveness of different embedding functions side-by-side, or to use embedding functions tailored to specific use cases. In that case, you can provide explicit names for those indexes and then reference them during queries. We'll illustrate that later with an example."
          ],
          "code_blocks": [
            {
              "code": "from pixeltable.functions.huggingface import clip\nimport PIL.Image\n\n# create embedding index on the 'img' column\nimgs.add_embedding_index(\n    'img',\n    embedding=clip.using(model_id='openai/clip-vit-base-patch32')\n)",
              "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:04<00:00,  2.50 cells/s]\n",
              "explanation": ""
            }
          ]
        },
        {
          "title": "Using the index in queries",
          "explanation": [
            "We then call the `similarity()` pseudo-function as a method on the indexed column and apply `order_by()` and `limit()`. We used the default cosine distance when we created the index, so we're going to order by descending similarity (`order_by(..., asc=False)`):",
            "We can combine nearest-neighbor/similarity search with standard predicates. Here's the same query, but filtering out the selected `sample_img` (which we already know has perfect similarity with itself):",
            "## Index updates\n\nIn Pixeltable, each index is kept up-to-date automatically in response to changes to the indexed table.\n\nTo illustrate this, let's insert a few more rows:",
            "When we now re-run the initial similarity query, we get a different result:",
            "## Similarity search on different types\n\nBecause CLIP models are multimodal, we can also do lookups by text."
          ],
          "code_blocks": [
            {
              "code": "# retrieve the 'img' column of some row as a PIL.Image.Image\nsample_img = imgs.select(imgs.img).collect()[6]['img']\nsample_img",
              "output": "<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x480>",
              "explanation": ""
            },
            {
              "code": "sim = imgs.img.similarity(sample_img)\nres = (\n    imgs.order_by(sim, asc=False)  # Order by descending similarity\n    .limit(2)  # Limit number of results to 2\n    .select(imgs.id, imgs.img, sim)\n    .collect()  # Retrieve results now\n)\nres",
              "output": "   id                                                img  similarity\n0   6  <PIL.JpegImagePlugin.JpegImageFile image mode=...    1.000000\n1   3  <PIL.JpegImagePlugin.JpegImageFile image mode=...    0.607202",
              "explanation": ""
            },
            {
              "code": "res = (\n    imgs.order_by(sim, asc=False)\n    .where(imgs.id != 6)  # Additional clause\n    .limit(2)\n    .select(imgs.id, imgs.img, sim)\n    .collect()\n)\nres",
              "output": "   id                                                img  similarity\n0   3  <PIL.JpegImagePlugin.JpegImageFile image mode=...    0.607202\n1   7  <PIL.JpegImagePlugin.JpegImageFile image mode=...    0.550944",
              "explanation": ""
            },
            {
              "code": "more_img_urls = [\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000080.jpg',\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000090.jpg',\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000106.jpg',\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000108.jpg',\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000139.jpg',\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000285.jpg',\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000632.jpg',\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000724.jpg',\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000776.jpg',\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000785.jpg',\n]\nimgs.insert({'id': 10 + i, 'img': url} for i, url in enumerate(more_img_urls))",
              "output": "Computing cells:  33%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                            | 10/30 [00:01<00:02,  8.90 cells/s]\nInserting rows into `img_tbl`: 10 rows [00:00, 1337.60 rows/s]\nComputing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 30/30 [00:01<00:00, 24.55 cells/s]\nInserted 10 rows with 0 errors.\n",
              "explanation": ""
            },
            {
              "code": "sim = imgs.img.similarity(sample_img)\nres = (\n    imgs.order_by(sim, asc=False)\n    .limit(2)\n    .select(imgs.id, imgs.img, sim)\n    .collect()\n)\nres",
              "output": "   id                                                img  similarity\n0   6  <PIL.JpegImagePlugin.JpegImageFile image mode=...    1.000000\n1  19  <PIL.JpegImagePlugin.JpegImageFile image mode=...    0.616822",
              "explanation": ""
            },
            {
              "code": "sim = imgs.img.similarity('train')  # String lookup\nres = (\n    imgs.order_by(sim, asc=False)\n    .limit(2)\n    .select(imgs.id, imgs.img, sim)\n    .collect()\n)\nres",
              "output": "   id                                                img  similarity\n0  13  <PIL.JpegImagePlugin.JpegImageFile image mode=...    0.273971\n1   9  <PIL.JpegImagePlugin.JpegImageFile image mode=...    0.239850",
              "explanation": ""
            }
          ]
        },
        {
          "title": "Creating multiple indexes on a single column",
          "explanation": [
            "When calling [`add_embedding_index()`](https://pixeltable.github.io/pixeltable/api/table/#pixeltable.Table.add_embedding_index), we now specify the index name (`idx_name`) directly. If it is not specified, Pixeltable will assign a name (such as `idx0`).",
            "To do a similarity query, we now call `similarity()` with the `idx` parameter:"
          ],
          "code_blocks": [
            {
              "code": "txts = pxt.create_table('indices_demo.text_tbl', {'text': pxt.String})\nsentences = [\n    \"Pablo Ruiz Picasso (25 October 1881 \u2013 8 April 1973) was a Spanish painter, sculptor, printmaker, ceramicist, and theatre designer who spent most of his adult life in France.\",\n    \"One of the most influential artists of the 20th century, he is known for co-founding the Cubist movement, the invention of constructed sculpture,[8][9] the co-invention of collage, and for the wide variety of styles that he helped develop and explore.\",\n    \"Among his most famous works are the proto-Cubist Les Demoiselles d'Avignon (1907) and the anti-war painting Guernica (1937), a dramatic portrayal of the bombing of Guernica by German and Italian air forces during the Spanish Civil War.\",\n    \"Picasso demonstrated extraordinary artistic talent in his early years, painting in a naturalistic manner through his childhood and adolescence.\",\n    \"During the first decade of the 20th century, his style changed as he experimented with different theories, techniques, and ideas.\",\n    \"After 1906, the Fauvist work of the older artist Henri Matisse motivated Picasso to explore more radical styles, beginning a fruitful rivalry between the two artists, who subsequently were often paired by critics as the leaders of modern art.\",\n    \"Picasso's output, especially in his early career, is often periodized.\",\n    \"While the names of many of his later periods are debated, the most commonly accepted periods in his work are the Blue Period (1901\u20131904), the Rose Period (1904\u20131906), the African-influenced Period (1907\u20131909), Analytic Cubism (1909\u20131912), and Synthetic Cubism (1912\u20131919), also referred to as the Crystal period.\",\n    \"Much of Picasso's work of the late 1910s and early 1920s is in a neoclassical style, and his work in the mid-1920s often has characteristics of Surrealism.\",\n    \"His later work often combines elements of his earlier styles.\",\n]\ntxts.insert({'text': s} for s in sentences)",
              "output": "Created table `text_tbl`.\nInserting rows into `text_tbl`: 10 rows [00:00, 3599.64 rows/s]\nInserted 10 rows with 0 errors.\n",
              "explanation": ""
            },
            {
              "code": "from pixeltable.functions.huggingface import sentence_transformer\n\ntxts.add_embedding_index(\n    'text',\n    idx_name='minilm_idx',\n    embedding=sentence_transformer.using(model_id='sentence-transformers/all-MiniLM-L12-v2')\n)\ntxts.add_embedding_index(\n    'text',\n    idx_name='e5_idx',\n    embedding=sentence_transformer.using(model_id='intfloat/e5-large-v2')\n)",
              "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:01<00:00,  6.86 cells/s]\nComputing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:01<00:00,  6.35 cells/s]\n",
              "explanation": ""
            },
            {
              "code": "sim = txts.text.similarity('cubism', idx='minilm_idx')\nres = txts.order_by(sim, asc=False).limit(2).select(txts.text, sim).collect()\nres",
              "output": "                                                text  similarity\n0  One of the most influential artists of the 20t...    0.443067\n1  While the names of many of his later periods a...    0.425664",
              "explanation": ""
            }
          ]
        },
        {
          "title": "Using a UDF for a custom embedding",
          "explanation": [
            "The above examples show how to use any model in the Hugging Face `CLIP` or `sentence_transformer` model families, and essentially the same pattern can be used for any other embedding with built-in Pixeltable support, such as OpenAI embeddings. But what if you want to adapt a new model family that doesn't have built-in support in Pixeltable? This can be done by writing a custom Pixeltable UDF.\n\nIn the following example, we'll write a simple UDF to use the [BERT](https://www.kaggle.com/models/tensorflow/bert/tensorFlow2/en-uncased-preprocess/3) model built on TensorFlow. First we install the necessary dependencies.",
            "Text embedding UDFs must always take a string as input, and return a 1-dimensional numpy array of fixed dimension (512 in the case of `small_bert`, the variant we'll be using). If we were writing an image embedding UDF, the `input` would have type `PIL.Image.Image` rather than `str`. The UDF is straightforward, loading the model and evaluating it against the input, with a minor data conversion on either side of the model invocation.",
            "Here's the output of our sample query run against `bert_idx`.",
            "Our example UDF is very simple, but it would perform poorly in a production setting. To make our UDF production-ready, we'd want to do two things:\n\n- Cache the model: the current version calls `hub.load()` on every UDF invocation. In a real application, we'd want to instantiate the model just once, then reuse it on subsequent UDF calls.\n- Batch our inputs: we'd use Pixeltable's batching capability to ensure we're making efficient use of the model. Batched UDFs are described in depth in the [User-Defined Functions](https://docs.pixeltable.com/docs/user-defined-functions-udfs) how-to guide.\n\nYou might have noticed that the updates to `bert_idx` seem sluggish; that's why!",
            "## Deleting an index\n\nTo delete an index, call [`Table.drop_embedding_index()`](https://pixeltable.github.io/pixeltable/api/table/#pixeltable.Table.drop_embedding_index):\n- specify the `idx_name` parameter if you have multiple indices\n- otherwise the `column_name` parameter is sufficient\n\nGiven that we have several embedding indices, we'll specify which index to drop:"
          ],
          "code_blocks": [
            {
              "code": "%pip install -qU tensorflow tensorflow-hub tensorflow-text",
              "output": null,
              "explanation": ""
            },
            {
              "code": "import tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_text  # Necessary to ensure BERT dependencies are loaded\nimport pixeltable as pxt\n\n@pxt.udf\ndef bert(input: str) -> pxt.Array[(512,), pxt.Float]:\n    \"\"\"Computes text embeddings using the small_bert model.\"\"\"\n    preprocessor = hub.load('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')\n    bert_model = hub.load('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/2')\n    tensor = tf.constant([input])  # Convert the string to a tensor\n    result = bert_model(preprocessor(tensor))['pooled_output']\n    return result.numpy()[0, :]",
              "output": null,
              "explanation": ""
            },
            {
              "code": "txts.add_embedding_index(\n    'text',\n    idx_name='bert_idx',\n    embedding=bert\n)",
              "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 10/10 [00:17<00:00,  1.72s/ cells]\n",
              "explanation": ""
            },
            {
              "code": "sim = txts.text.similarity('cubism', idx='bert_idx')\nres = txts.order_by(sim, asc=False).limit(2).select(txts.text, sim).collect()\nres",
              "output": "                                                text  similarity\n0  Picasso's output, especially in his early care...    0.698653\n1  During the first decade of the 20th century, h...    0.697284",
              "explanation": ""
            },
            {
              "code": "txts.drop_embedding_index(idx_name='e5_idx')",
              "output": null,
              "explanation": ""
            }
          ]
        }
      ],
      "api_usage": {},
      "key_concepts": [
        "incremental",
        "udf",
        "embedding",
        "computed column",
        "tool calling",
        "multimodal"
      ],
      "code_snippets": [
        {
          "code": "%pip install -qU pixeltable transformers sentence_transformers",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/feature-guides/embedding-indexes.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/feature-guides/embedding-indexes.ipynb)\n\n# Working with Embedding/Vector Indexes\n\n**If you are running this tutorial in Colab:**\nIn order to make the tutorial run a bit snappier, let's switch to a GPU-equipped instance for this Colab session. To do that, click on the `Runtime -> Change runtime type` menu item at the top, then select the `GPU` radio button and click on `Save`.\n\nMain takeaways:\n* Indexing in Pixeltable is declarative\n  - you create an index on a column and supply the embedding functions you want to use (for inserting data into the index as well as lookups)\n  - Pixeltable maintains the index in response to any kind of update of the indexed table (i.e., `insert()`/`update()`/`delete()`)\n* Perform index lookups with the `similarity()` pseudo-function, in combination with the `order_by()` and `limit()` clauses\nTo make this concrete, let's create a table of images with the [`create_table()`](https://pixeltable.github.io/pixeltable/api/pixeltable/#pixeltable.create_table) function.\nWe're also going to add some additional columns, to demonstrate combining similarity search with other predicates.",
          "output": null
        },
        {
          "code": "import pixeltable as pxt\n\n# Delete the `indices_demo` directory and its contents, if it exists\npxt.drop_dir('indices_demo', force=True)\n\n# Create the directory and table to use for the demo\npxt.create_dir('indices_demo')\nschema = {\n    'id': pxt.Int,\n    'img': pxt.Image,\n}\nimgs = pxt.create_table('indices_demo.img_tbl', schema)",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/feature-guides/embedding-indexes.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/feature-guides/embedding-indexes.ipynb)\n\n# Working with Embedding/Vector Indexes\n\n**If you are running this tutorial in Colab:**\nIn order to make the tutorial run a bit snappier, let's switch to a GPU-equipped instance for this Colab session. To do that, click on the `Runtime -> Change runtime type` menu item at the top, then select the `GPU` radio button and click on `Save`.\n\nMain takeaways:\n* Indexing in Pixeltable is declarative\n  - you create an index on a column and supply the embedding functions you want to use (for inserting data into the index as well as lookups)\n  - Pixeltable maintains the index in response to any kind of update of the indexed table (i.e., `insert()`/`update()`/`delete()`)\n* Perform index lookups with the `similarity()` pseudo-function, in combination with the `order_by()` and `limit()` clauses\nTo make this concrete, let's create a table of images with the [`create_table()`](https://pixeltable.github.io/pixeltable/api/pixeltable/#pixeltable.create_table) function.\nWe're also going to add some additional columns, to demonstrate combining similarity search with other predicates.",
          "output": "Connected to Pixeltable database at: postgresql+psycopg://postgres:@/pixeltable?host=/Users/asiegel/.pixeltable/pgdata\nCreated directory `indices_demo`.\nCreated table `img_tbl`.\n"
        },
        {
          "code": "img_urls = [\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000030.jpg',\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000034.jpg',\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000042.jpg',\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000049.jpg',\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000057.jpg',\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000061.jpg',\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000063.jpg',\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000064.jpg',\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000069.jpg',\n    'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000071.jpg',\n]\nimgs.insert({'id': i, 'img': url} for i, url in enumerate(img_urls))",
          "context": "To make this concrete, let's create a table of images with the [`create_table()`](https://pixeltable.github.io/pixeltable/api/pixeltable/#pixeltable.create_table) function.\nWe're also going to add some additional columns, to demonstrate combining similarity search with other predicates.\nWe start out by inserting 10 rows:",
          "output": "Computing cells:  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c        | 16/20 [00:01<00:00, 14.67 cells/s]\nInserting rows into `img_tbl`: 10 rows [00:00, 3589.17 rows/s]\nComputing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 20/20 [00:01<00:00, 18.16 cells/s]\nInserted 10 rows with 0 errors.\n"
        }
      ]
    },
    {
      "notebook": "feature-guides/udfs-in-pixeltable.ipynb",
      "title": "UDFs in Pixeltable",
      "description": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/feature-guides/udfs-in-pixeltable.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/feature-guides/udfs-in-pixeltable.ipynb) Pixeltable comes with a library of built-in functions and integrations, but sooner or later, you'll want to introduce some customized logic into your workflow. This is where Pixeltable's rich UDF (User-Defined Function) capability comes in. Pixeltable UDFs let you write code in Python, then directly insert your custom logic into Pixeltable expressions and computed columns. In this how-to guide, we'll show how to define UDFs, extend their capabilities, and use them in computed columns. To start, we'll install the necessary dependencies, create a Pixeltable directory and table to experiment with, and add some sample data.",
      "workflows": [],
      "api_usage": {},
      "key_concepts": [
        "incremental",
        "udf",
        "iterator",
        "computed column",
        "multimodal"
      ],
      "code_snippets": [
        {
          "code": "%pip install -qU pixeltable",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/feature-guides/udfs-in-pixeltable.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/feature-guides/udfs-in-pixeltable.ipynb)\n\n# UDFs in Pixeltable\n\nPixeltable comes with a library of built-in functions and integrations, but sooner or later, you'll want to introduce some customized logic into your workflow. This is where Pixeltable's rich UDF (User-Defined Function) capability comes in. Pixeltable UDFs let you write code in Python, then directly insert your custom logic into Pixeltable expressions and computed columns. In this how-to guide, we'll show how to define UDFs, extend their capabilities, and use them in computed columns.\n\nTo start, we'll install the necessary dependencies, create a Pixeltable directory and table to experiment with, and add some sample data.",
          "output": null
        },
        {
          "code": "import pixeltable as pxt\n\n# Create the directory and table\npxt.drop_dir('udf_demo', force=True)  # Ensure a clean slate for the demo\npxt.create_dir('udf_demo')\nt = pxt.create_table('udf_demo.strings', {'input': pxt.String})\n\n# Add some sample data\nt.insert([{'input': 'Hello, world!'}, {'input': 'You can do a lot with Pixeltable UDFs.'}])\nt.show()",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/feature-guides/udfs-in-pixeltable.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/feature-guides/udfs-in-pixeltable.ipynb)\n\n# UDFs in Pixeltable\n\nPixeltable comes with a library of built-in functions and integrations, but sooner or later, you'll want to introduce some customized logic into your workflow. This is where Pixeltable's rich UDF (User-Defined Function) capability comes in. Pixeltable UDFs let you write code in Python, then directly insert your custom logic into Pixeltable expressions and computed columns. In this how-to guide, we'll show how to define UDFs, extend their capabilities, and use them in computed columns.\n\nTo start, we'll install the necessary dependencies, create a Pixeltable directory and table to experiment with, and add some sample data.",
          "output": "Connected to Pixeltable database at: postgresql+psycopg://postgres:@/pixeltable?host=/Users/asiegel/.pixeltable/pgdata\nCreated directory `udf_demo`.\nCreated table `strings`.\nInserting rows into `strings`: 2 rows [00:00, 763.99 rows/s]\nInserted 2 rows with 0 errors.\n"
        },
        {
          "code": "import numpy as np\n\ndef longest_word(sentence: str, strip_punctuation: bool = False) -> str:\n    words = sentence.split()\n    if strip_punctuation:  # Remove non-alphanumeric characters from each word\n        words = [''.join(filter(str.isalnum, word)) for word in words]\n    i = np.argmax([len(word) for word in words])\n    return words[i]",
          "context": "## What is a UDF?\n\nA Pixeltable UDF is just a Python function that is marked with the `@pxt.udf` decorator.\n\n```python\n@pxt.udf\ndef add_one(n: int) -> int:\n    return n + 1\n```\n\nIt's as simple as that! Without the decorator, `add_one` would be an ordinary Python function that operates on integers. Adding `@pxt.udf` converts it into a Pixeltable function that operates on _columns_ of integers. The decorated function can then be used directly to define computed columns; Pixeltable will orchestrate its execution across all the input data.\nFor our first working example, let's do something slightly more interesting: write a function to extract the longest word from a sentence. (If there are ties for the longest word, we choose the first word among those ties.) In Python, that might look something like this:",
          "output": null
        },
        {
          "code": "longest_word(\"Let's check that it works.\", strip_punctuation=True)",
          "context": "## What is a UDF?\n\nA Pixeltable UDF is just a Python function that is marked with the `@pxt.udf` decorator.\n\n```python\n@pxt.udf\ndef add_one(n: int) -> int:\n    return n + 1\n```\n\nIt's as simple as that! Without the decorator, `add_one` would be an ordinary Python function that operates on integers. Adding `@pxt.udf` converts it into a Pixeltable function that operates on _columns_ of integers. The decorated function can then be used directly to define computed columns; Pixeltable will orchestrate its execution across all the input data.\nFor our first working example, let's do something slightly more interesting: write a function to extract the longest word from a sentence. (If there are ties for the longest word, we choose the first word among those ties.) In Python, that might look something like this:",
          "output": "'check'"
        },
        {
          "code": "@pxt.udf\ndef longest_word(sentence: str, strip_punctuation: bool = False) -> str:\n    words = sentence.split()\n    if strip_punctuation:  # Remove non-alphanumeric characters from each word\n        words = [''.join(filter(str.isalnum, word)) for word in words]\n    i = np.argmax([len(word) for word in words])\n    return words[i]",
          "context": "For our first working example, let's do something slightly more interesting: write a function to extract the longest word from a sentence. (If there are ties for the longest word, we choose the first word among those ties.) In Python, that might look something like this:\nThe `longest_word` Python function isn't a Pixeltable UDF (yet); it operates on individual strings, not columns of strings. Adding the decorator turns it into a UDF:",
          "output": null
        },
        {
          "code": "t.add_computed_column(longest_word=longest_word(t.input))\nt.show()",
          "context": "The `longest_word` Python function isn't a Pixeltable UDF (yet); it operates on individual strings, not columns of strings. Adding the decorator turns it into a UDF:\nNow we can use it to create a computed column. Pixeltable orchestrates the computation like it does with any other function, applying the UDF in turn to each existing row of the table, then updating incrementally each time a new row is added.",
          "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 138.28 cells/s]\nAdded 2 column values with 0 errors.\n"
        },
        {
          "code": "t.insert(input='Pixeltable updates tables incrementally.')\nt.show()",
          "context": "The `longest_word` Python function isn't a Pixeltable UDF (yet); it operates on individual strings, not columns of strings. Adding the decorator turns it into a UDF:\nNow we can use it to create a computed column. Pixeltable orchestrates the computation like it does with any other function, applying the UDF in turn to each existing row of the table, then updating incrementally each time a new row is added.",
          "output": "Computing cells:   0%|                                                    | 0/3 [00:00<?, ? cells/s]\nInserting rows into `strings`: 1 rows [00:00, 255.24 rows/s]\nComputing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 364.69 cells/s]\nInserted 1 row with 0 errors.\n"
        },
        {
          "code": "t.add_computed_column(\n    longest_word_2=longest_word(t.input, strip_punctuation=True)\n)\nt.show()",
          "context": "Now we can use it to create a computed column. Pixeltable orchestrates the computation like it does with any other function, applying the UDF in turn to each existing row of the table, then updating incrementally each time a new row is added.\nOops, those trailing punctuation marks are kind of annoying. Let's add another column, this time using the handy `strip_punctuation` parameter from our UDF. (We could alternatively drop the first column before adding the new one, but for purposes of this tutorial it's convenient to see how Pixeltable executes both variants side-by-side.) Note how _columns_ such as `t.input` and _constants_ such as `True` can be freely intermixed as arguments to the UDF.",
          "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 252.91 cells/s]\nAdded 3 column values with 0 errors.\n"
        },
        {
          "code": "@pxt.udf\ndef longest_word(sentence: str, strip_punctuation: bool = False) -> str:\n    words = sentence.split()\n    if strip_punctuation:\n        words = [\n            word if word[-1].isalnum() else word[:-1]\n            for word in words\n        ]\n    i = np.argmax([len(word) for word in words])\n    return words[i]",
          "context": "## Types in UDFs\n\nYou might have noticed that the `longest_word` UDF has _type hints_ in its signature.\n\n```python\ndef longest_word(sentence: str, strip_punctuation: bool = False) -> str: ...\n```\n\nThe `sentence` parameter, `strip_punctuation` parameter, and return value all have explicit types (`str`, `bool`, and `str` respectively). In general Python code, type hints are usually optional. But Pixeltable is a database system: _everything_ in Pixeltable must have a type. And since Pixeltable is also an orchestrator - meaning it sets up workflows and computed columns _before_ executing them - these types need to be known in advance. That's the reasoning behind a fundamental principle of Pixeltable UDFs:\n- Type hints are _required_.\n\nYou can turn almost any Python function into a Pixeltable UDF, provided that it has type hints, and provided that Pixeltable supports the types that it uses. The most familiar types that you'll use in UDFs are:\n- `int`\n- `float`\n- `str`\n- `list` (can optionally be parameterized, e.g., `list[str]`)\n- `dict` (can optionally be parameterized, e.g., `dict[str, int]`)\n- `PIL.Image.Image`\n\nIn addition to these standard Python types, Pixeltable also recognizes various kinds of arrays, audio and video media, and documents.\n## Local and Module UDFs\n\nThe `longest_word` UDF that we defined above is a _local_ UDF: it was defined directly in our notebook, rather than in a module that we imported. Many other UDFs, including all of Pixeltable's built-in functions, are defined in modules. We encountered a few of these in the Pixeltable Basics tutorial: the `huggingface.detr_for_object_detection` and `openai.vision` functions. (Although these are built-in functions, they behave the same way as UDFs, and in fact they're defined the same way under the covers.)\n\nThere is an important difference between the two. When you add a module UDF such as `openai.vision` to a table, Pixeltable stores a _reference_ to the corresponding Python function in the module. If you later restart your Python runtime and reload Pixeltable, then Pixeltable will re-import the module UDF when it loads the computed column. This means that any code changes made to the UDF will be picked up at that time, and the new version of the UDF will be used in any future execution.\n\nConversely, when you add a local UDF to a table, the _entire code_ for the UDF is serialized and stored in the table. This ensures that if you restart your notebook kernel (say), or even delete the notebook entirely, the UDF will continue to function. However, it also means that if you modify the UDF code, the updated logic will not be reflected in any existing Pixeltable columns.\n\nTo see how this works in practice, let's modify our `longest_word` UDF so that if `strip_punctuation` is `True`, then we remove only a single punctuation mark from the _end_ of each word.",
          "output": null
        },
        {
          "code": "t.insert(input=\"Let's check that it still works.\")\nt.show()",
          "context": "## Local and Module UDFs\n\nThe `longest_word` UDF that we defined above is a _local_ UDF: it was defined directly in our notebook, rather than in a module that we imported. Many other UDFs, including all of Pixeltable's built-in functions, are defined in modules. We encountered a few of these in the Pixeltable Basics tutorial: the `huggingface.detr_for_object_detection` and `openai.vision` functions. (Although these are built-in functions, they behave the same way as UDFs, and in fact they're defined the same way under the covers.)\n\nThere is an important difference between the two. When you add a module UDF such as `openai.vision` to a table, Pixeltable stores a _reference_ to the corresponding Python function in the module. If you later restart your Python runtime and reload Pixeltable, then Pixeltable will re-import the module UDF when it loads the computed column. This means that any code changes made to the UDF will be picked up at that time, and the new version of the UDF will be used in any future execution.\n\nConversely, when you add a local UDF to a table, the _entire code_ for the UDF is serialized and stored in the table. This ensures that if you restart your notebook kernel (say), or even delete the notebook entirely, the UDF will continue to function. However, it also means that if you modify the UDF code, the updated logic will not be reflected in any existing Pixeltable columns.\n\nTo see how this works in practice, let's modify our `longest_word` UDF so that if `strip_punctuation` is `True`, then we remove only a single punctuation mark from the _end_ of each word.\nNow we see that Pixeltable continues to use the _old_ definition, even as new rows are added to the table.",
          "output": "Computing cells:   0%|                                                    | 0/5 [00:00<?, ? cells/s]\nInserting rows into `strings`: 1 rows [00:00, 242.01 rows/s]\nComputing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:00<00:00, 623.99 cells/s]\nInserted 1 row with 0 errors.\n"
        },
        {
          "code": "t.add_computed_column(\n    longest_word_3=longest_word(t.input, strip_punctuation=True)\n)\nt.show()",
          "context": "Now we see that Pixeltable continues to use the _old_ definition, even as new rows are added to the table.\nBut if we add a new _column_ that references the `longest_word` UDF, Pixeltable will use the updated version.",
          "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 348.89 cells/s]\nAdded 4 column values with 0 errors.\n"
        },
        {
          "code": "from pixeltable.func import Batch\n\n@pxt.udf(batch_size=16)\ndef longest_word(sentences: Batch[str], strip_punctuation: bool = False) -> Batch[str]:\n    results = []\n    for sentence in sentences:\n        words = sentence.split()\n        if strip_punctuation:\n            words = [\n                word if word[-1].isalnum() else word[:-1]\n                for word in words\n            ]\n        i = np.argmax([len(word) for word in words])\n        results.append(words[i])\n    return results",
          "context": "The general rule is: changes to module UDFs will affect any future execution; changes to local UDFs will only affect _new columns_ that are defined using the new version of the UDF.\n## Batching\n\nPixeltable provides several ways to optimize UDFs for better performance. One of the most common is _batching_, which is particularly important for UDFs that involve GPU operations.\n\nOrdinary UDFs process one row at a time, meaning the UDF will be invoked exactly once per row processed. Conversely, a batched UDF processes several rows at a time; the specific number is user-configurable. As an example, let's modify our `longest_word` UDF to take a batched parameter. Here's what it looks like:",
          "output": null
        },
        {
          "code": "t.add_computed_column(\n    longest_word_3_batched=longest_word(t.input, strip_punctuation=True)\n)\nt.show()",
          "context": "## Batching\n\nPixeltable provides several ways to optimize UDFs for better performance. One of the most common is _batching_, which is particularly important for UDFs that involve GPU operations.\n\nOrdinary UDFs process one row at a time, meaning the UDF will be invoked exactly once per row processed. Conversely, a batched UDF processes several rows at a time; the specific number is user-configurable. As an example, let's modify our `longest_word` UDF to take a batched parameter. Here's what it looks like:\nThere are several changes:\n- The parameter `batch_size=16` has been added to the `@pxt.udf` decorator, specifying the batch size;\n- The `sentences` parameter has changed from `str` to `Batch[str]`;\n- The return type has also changed from `str` to `Batch[str]`; and\n- Instead of processing a single sentence, the UDF is processing a `Batch` of sentences and returning the result `Batch`.\n\nWhat exactly is a `Batch[str]`? Functionally, it's simply a `list[str]`, and you can use it exactly like a `list[str]` in any Python code. The only difference is in the type hint; a type hint of `Batch[str]` tells Pixeltable, \"My data consists of individual strings that I want you to process in batches\". Conversely, a type hint of `list[str]` would mean, \"My data consists of _lists_ of strings that I want you to process one at a time\".\n\nNotice that the `strip_punctuation` parameter is _not_ wrapped in a `Batch` type. This because `strip_punctuation` controls the behavior of the UDF, rather than being part of the input data. When we use the batched `longest_word` UDF, the `strip_punctuation` parameter will always be a constant, not a column.\n\nLet's put the new, batched UDF to work.",
          "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 353.90 cells/s]\nAdded 4 column values with 0 errors.\n"
        },
        {
          "code": "import pixeltable as pxt\n\nt = pxt.create_table('udf_demo.values', {'val': pxt.Int})\nt.insert({'val': n} for n in range(50))",
          "context": "## UDAs (Aggregate UDFs)\nOrdinary UDFs are always one-to-one on rows: each row of input generates one UDF output value. Functions that aggregate data, conversely, are many-to-one, and in Pixeltable they are represented by a related abstraction, the UDA (<i>U</i>ser-<i>D</i>efined <i>A</i>ggregate). Pixeltable has a number of built-in UDAs; if you've worked through the Fundamentals tutorial, you'll have already encountered a few of them, such as `sum` and `count`. In this section, we'll show how to define your own custom UDAs. For demonstration purposes, let's start by creating a table containing all the integers from 0 to 49.",
          "output": "Created table `values`.\nInserting rows into `values`: 50 rows [00:00, 9267.95 rows/s]\nInserted 50 rows with 0 errors.\n"
        },
        {
          "code": "import pixeltable.functions as pxtf\n\nt.select(pxtf.sum(t.val)).collect()",
          "context": "Ordinary UDFs are always one-to-one on rows: each row of input generates one UDF output value. Functions that aggregate data, conversely, are many-to-one, and in Pixeltable they are represented by a related abstraction, the UDA (<i>U</i>ser-<i>D</i>efined <i>A</i>ggregate). Pixeltable has a number of built-in UDAs; if you've worked through the Fundamentals tutorial, you'll have already encountered a few of them, such as `sum` and `count`. In this section, we'll show how to define your own custom UDAs. For demonstration purposes, let's start by creating a table containing all the integers from 0 to 49.\nIf we wanted to compute their sum using the built-in `sum` aggregate, we'd do it like this:",
          "output": "    sum\n0  1225"
        },
        {
          "code": "t.group_by(t.val // 10).order_by(t.val // 10).select(\n    t.val // 10, pxtf.sum(t.val)\n).collect()",
          "context": "If we wanted to compute their sum using the built-in `sum` aggregate, we'd do it like this:\nOr perhaps we want to group them by `n // 10` (corresponding to the tens digit of each integer) and sum each group:",
          "output": "   col_0  sum\n0      0   45\n1      1  145\n2      2  245\n3      3  345\n4      4  445"
        },
        {
          "code": "@pxt.uda\nclass sum_of_squares(pxt.Aggregator):\n    def __init__(self):\n        # No data yet; initialize `cur_sum` to 0\n        self.cur_sum = 0\n\n    def update(self, val: int) -> None:\n        # Update the value of `cur_sum` with the new datapoint\n        self.cur_sum += val * val\n\n    def value(self) -> int:\n        # Retrieve the current value of `cur_sum`\n        return self.cur_sum",
          "context": "Or perhaps we want to group them by `n // 10` (corresponding to the tens digit of each integer) and sum each group:\nNow let's define a new aggregate to compute the sum of squares of a set of numbers. To define an aggregate, we implement a subclass of the `pxt.Aggregator` Python class and decorate it with the `@pxt.uda` decorator, similar to what we did for UDFs. The subclass must implement three methods:\n\n- `__init__()` - initializes the aggregator; can be used to parameterize aggregator behavior\n- `update()` - updates the internal state of the aggregator with a new value\n- `value()` - retrieves the current value held by the aggregator\n\nIn our example, the class will have a single member `cur_sum`, which holds a running total of the squares of all the values we've seen.",
          "output": null
        },
        {
          "code": "t.select(sum_of_squares(t.val)).collect()",
          "context": "Or perhaps we want to group them by `n // 10` (corresponding to the tens digit of each integer) and sum each group:\nNow let's define a new aggregate to compute the sum of squares of a set of numbers. To define an aggregate, we implement a subclass of the `pxt.Aggregator` Python class and decorate it with the `@pxt.uda` decorator, similar to what we did for UDFs. The subclass must implement three methods:\n\n- `__init__()` - initializes the aggregator; can be used to parameterize aggregator behavior\n- `update()` - updates the internal state of the aggregator with a new value\n- `value()` - retrieves the current value held by the aggregator\n\nIn our example, the class will have a single member `cur_sum`, which holds a running total of the squares of all the values we've seen.",
          "output": "   sum_of_squares\n0           40425"
        },
        {
          "code": "t.group_by(t.val // 10).order_by(t.val // 10).select(\n    t.val // 10, sum_of_squares(t.val)\n).collect()",
          "context": "Or perhaps we want to group them by `n // 10` (corresponding to the tens digit of each integer) and sum each group:\nNow let's define a new aggregate to compute the sum of squares of a set of numbers. To define an aggregate, we implement a subclass of the `pxt.Aggregator` Python class and decorate it with the `@pxt.uda` decorator, similar to what we did for UDFs. The subclass must implement three methods:\n\n- `__init__()` - initializes the aggregator; can be used to parameterize aggregator behavior\n- `update()` - updates the internal state of the aggregator with a new value\n- `value()` - retrieves the current value held by the aggregator\n\nIn our example, the class will have a single member `cur_sum`, which holds a running total of the squares of all the values we've seen.",
          "output": "   col_0  sum_of_squares\n0      0             285\n1      1            2185\n2      2            6085\n3      3           11985\n4      4           19885"
        }
      ]
    },
    {
      "notebook": "feature-guides/working-with-external-files.ipynb",
      "title": "Working with External Files",
      "description": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/feature-guides/working-with-external-files.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/feature-guides/working-with-external-files.ipynb) In Pixeltable, all media data (videos, images, audio) resides in external files, and Pixeltable stores references to those. The files can be local or remote (e.g., in S3). For the latter, Pixeltable automatically caches the files locally on access. When interacting with media data via Pixeltable, either through queries or UDFs, the user sees the following Python types:",
      "workflows": [],
      "api_usage": {},
      "key_concepts": [
        "computed column",
        "multimodal",
        "udf"
      ],
      "code_snippets": [
        {
          "code": "%pip install -qU pixeltable boto3",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/feature-guides/working-with-external-files.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/feature-guides/working-with-external-files.ipynb)\n\n# Working with External Files\n\nIn Pixeltable, all media data (videos, images, audio) resides in external files, and Pixeltable stores references to those. The files can be local or remote (e.g., in S3). For the latter, Pixeltable automatically caches the files locally on access.\n\nWhen interacting with media data via Pixeltable, either through queries or UDFs, the user sees the following Python types:\n* `ImageType`: `PIL.Image.Image`\n* `VideoType`: `string` (local path)\n* `AudioType`: `string` (local path)\n\nLet's create a table and load some data to see what that looks like:",
          "output": null
        },
        {
          "code": "import tempfile\nimport random\nimport shutil\nimport pixeltable as pxt\n\n# First drop the `external_data` directory if it exists, to ensure\n# a clean environment for the demo\npxt.drop_dir('external_data', force=True)\npxt.create_dir('external_data')",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/feature-guides/working-with-external-files.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/feature-guides/working-with-external-files.ipynb)\n\n# Working with External Files\n\nIn Pixeltable, all media data (videos, images, audio) resides in external files, and Pixeltable stores references to those. The files can be local or remote (e.g., in S3). For the latter, Pixeltable automatically caches the files locally on access.\n\nWhen interacting with media data via Pixeltable, either through queries or UDFs, the user sees the following Python types:\n* `ImageType`: `PIL.Image.Image`\n* `VideoType`: `string` (local path)\n* `AudioType`: `string` (local path)\n\nLet's create a table and load some data to see what that looks like:",
          "output": "Connected to Pixeltable database at: postgresql+psycopg://postgres:@/pixeltable?host=/Users/asiegel/.pixeltable/pgdata\nCreated directory `external_data`.\n"
        },
        {
          "code": "v = pxt.create_table('external_data.videos', {'video': pxt.Video})\n\nprefix = 's3://multimedia-commons/'\npaths = [\n    'data/videos/mp4/ffe/ffb/ffeffbef41bbc269810b2a1a888de.mp4',\n    'data/videos/mp4/ffe/feb/ffefebb41485539f964760e6115fbc44.mp4',\n    'data/videos/mp4/ffe/f73/ffef7384d698b5f70d411c696247169.mp4'\n]\nv.insert({'video': prefix + p} for p in paths)",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/feature-guides/working-with-external-files.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/feature-guides/working-with-external-files.ipynb)\n\n# Working with External Files\n\nIn Pixeltable, all media data (videos, images, audio) resides in external files, and Pixeltable stores references to those. The files can be local or remote (e.g., in S3). For the latter, Pixeltable automatically caches the files locally on access.\n\nWhen interacting with media data via Pixeltable, either through queries or UDFs, the user sees the following Python types:\n* `ImageType`: `PIL.Image.Image`\n* `VideoType`: `string` (local path)\n* `AudioType`: `string` (local path)\n\nLet's create a table and load some data to see what that looks like:",
          "output": "Created table `videos`.\nComputing cells:   0%|                                                    | 0/6 [00:00<?, ? cells/s]\nInserting rows into `videos`: 3 rows [00:00, 1004.62 rows/s]\nComputing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:00<00:00, 79.14 cells/s]\nInserted 3 rows with 0 errors.\n"
        },
        {
          "code": "rows = list(v.select(v.video).collect())\nrows[0]",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/feature-guides/working-with-external-files.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/feature-guides/working-with-external-files.ipynb)\n\n# Working with External Files\n\nIn Pixeltable, all media data (videos, images, audio) resides in external files, and Pixeltable stores references to those. The files can be local or remote (e.g., in S3). For the latter, Pixeltable automatically caches the files locally on access.\n\nWhen interacting with media data via Pixeltable, either through queries or UDFs, the user sees the following Python types:\n* `ImageType`: `PIL.Image.Image`\n* `VideoType`: `string` (local path)\n* `AudioType`: `string` (local path)\n\nLet's create a table and load some data to see what that looks like:\nWe just inserted 3 rows with video files residing in S3. When we now query these, we are presented with their locally cached counterparts.\n\n(Note: we don't simply display the output of `collect()` here, because that is formatted as an HTML table with a media player and so would obscure the file path.)",
          "output": "{'video': '/Users/asiegel/.pixeltable/file_cache/682f022a704d4459adb2f29f7fe9577c_0_1fcfcb221263cff76a2853250fbbb2e90375dd495454c0007bc6ff4430c9a4a7.mp4'}"
        },
        {
          "code": "local_path = tempfile.mktemp(suffix='.mp4')\nshutil.copyfile(rows[0]['video'], local_path)\nlocal_path",
          "context": "We just inserted 3 rows with video files residing in S3. When we now query these, we are presented with their locally cached counterparts.\n\n(Note: we don't simply display the output of `collect()` here, because that is formatted as an HTML table with a media player and so would obscure the file path.)\nLet's make a local copy of the first file and insert that separately. First, the copy:",
          "output": "'/var/folders/hb/qd0dztsj43j_mdb6hbl1gzyc0000gn/T/tmp1jo4a7ca.mp4'"
        },
        {
          "code": "v.insert(video=local_path)",
          "context": "Let's make a local copy of the first file and insert that separately. First, the copy:\nNow the insert:",
          "output": "Computing cells:   0%|                                                    | 0/2 [00:00<?, ? cells/s]\nInserting rows into `videos`: 1 rows [00:00, 725.78 rows/s]\nComputing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 53.23 cells/s]\nInserted 1 row with 0 errors.\n"
        },
        {
          "code": "rows = list(v.select(v.video).collect())\nrows",
          "context": "Now the insert:\nWhen we query this again, we see that local paths are preserved:",
          "output": "[{'video': '/Users/asiegel/.pixeltable/file_cache/682f022a704d4459adb2f29f7fe9577c_0_1fcfcb221263cff76a2853250fbbb2e90375dd495454c0007bc6ff4430c9a4a7.mp4'},\n {'video': '/Users/asiegel/.pixeltable/file_cache/682f022a704d4459adb2f29f7fe9577c_0_fc11428b32768ae782193a57ebcbad706f45bbd9fa13354471e0bcd798fee3ea.mp4'},\n {'video': '/Users/asiegel/.pixeltable/file_cache/682f022a704d4459adb2f29f7fe9577c_0_b9fb0d9411bc9cd183a36866911baa7a8834f22f665bce47608566b38485c16a.mp4'},\n {'video': '/var/folders/hb/qd0dztsj43j_mdb6hbl1gzyc0000gn/T/tmp1jo4a7ca.mp4'}]"
        },
        {
          "code": "@pxt.udf\ndef f(v: pxt.Video) -> int:\n    print(f'{type(v)}: {v}')\n    return 1",
          "context": "When we query this again, we see that local paths are preserved:\nUDFs also see local paths:",
          "output": null
        },
        {
          "code": "v.select(f(v.video)).show()",
          "context": "When we query this again, we see that local paths are preserved:\nUDFs also see local paths:",
          "output": "<class 'str'>: /Users/asiegel/.pixeltable/file_cache/682f022a704d4459adb2f29f7fe9577c_0_1fcfcb221263cff76a2853250fbbb2e90375dd495454c0007bc6ff4430c9a4a7.mp4\n<class 'str'>: /Users/asiegel/.pixeltable/file_cache/682f022a704d4459adb2f29f7fe9577c_0_fc11428b32768ae782193a57ebcbad706f45bbd9fa13354471e0bcd798fee3ea.mp4\n<class 'str'>: /Users/asiegel/.pixeltable/file_cache/682f022a704d4459adb2f29f7fe9577c_0_b9fb0d9411bc9cd183a36866911baa7a8834f22f665bce47608566b38485c16a.mp4\n<class 'str'>: /var/folders/hb/qd0dztsj43j_mdb6hbl1gzyc0000gn/T/tmp1jo4a7ca.mp4\n"
        },
        {
          "code": "v.insert(video=prefix + 'bad_path.mp4')",
          "context": "UDFs also see local paths:\n## Dealing with errors\n\nWhen interacting with media data in Pixeltable, the user can assume that the underlying files exist, are local and are valid for their respective data type. In other words, the user doesn't need to consider error conditions.\n\nTo that end, Pixeltable validates media data on ingest. The default behavior is to reject invalid media files:",
          "output": "Computing cells:   0%|                                                    | 0/2 [00:01<?, ? cells/s]\n"
        },
        {
          "code": "# create invalid .mp4\nwith tempfile.NamedTemporaryFile(mode='wb', suffix='.mp4', delete=False) as temp_file:\n    temp_file.write(random.randbytes(1024))\n    corrupted_path = temp_file.name\n\nv.insert(video=corrupted_path)",
          "context": "## Dealing with errors\n\nWhen interacting with media data in Pixeltable, the user can assume that the underlying files exist, are local and are valid for their respective data type. In other words, the user doesn't need to consider error conditions.\n\nTo that end, Pixeltable validates media data on ingest. The default behavior is to reject invalid media files:\nThe same happens for corrupted files:",
          "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 1084.64 cells/s]\n"
        },
        {
          "code": "v.insert([{'video': prefix + 'bad_path.mp4'}, {'video': corrupted_path}], on_error='ignore')",
          "context": "The same happens for corrupted files:\nAlternatively, Pixeltable can also be instructed to record error conditions and proceed with the ingest, via the `on_error` flag (default: `'abort'`):",
          "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 20.98 cells/s]\nInserting rows into `videos`: 2 rows [00:00, 671.63 rows/s]\nComputing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 20.13 cells/s]\nInserted 2 rows with 4 errors across 2 columns (videos.video, videos.None).\n"
        },
        {
          "code": "v.select(v.video == None, v.video.errortype, v.video.errormsg).collect()",
          "context": "Alternatively, Pixeltable can also be instructed to record error conditions and proceed with the ingest, via the `on_error` flag (default: `'abort'`):\nEvery media column has properties `errortype` and `errormsg` (both containing `string` data) that indicate whether the column value is valid. Invalid values show up as `None` and have non-null `errortype`/`errormsg`:",
          "output": "   col_0 video_errortype                                     video_errormsg\n0  False            None                                               None\n1  False            None                                               None\n2  False            None                                               None\n3  False            None                                               None\n4   True           Error  Failed to download s3://multimedia-commons/bad...\n5   True           Error  Not a valid video: /var/folders/hb/qd0dztsj43j..."
        },
        {
          "code": "v.where(v.video.errortype != None).select(v.video.errormsg).collect()",
          "context": "Every media column has properties `errortype` and `errormsg` (both containing `string` data) that indicate whether the column value is valid. Invalid values show up as `None` and have non-null `errortype`/`errormsg`:\nErrors can now be inspected (and corrected) after the ingest:",
          "output": "                                      video_errormsg\n0  Failed to download s3://multimedia-commons/bad...\n1  Not a valid video: /var/folders/hb/qd0dztsj43j..."
        },
        {
          "code": "v.select(v.video.fileurl, v.video.localpath).collect()",
          "context": "Errors can now be inspected (and corrected) after the ingest:\n## Accessing the original file paths\n\nIn some cases, it will be necessary to access file paths (not, say, the `PIL.Image.Image`), and Pixeltable provides the column properties `fileurl` and `localpath` for that purpose:",
          "output": "                                       video_fileurl  \\\n0  s3://multimedia-commons/data/videos/mp4/ffe/ff...   \n1  s3://multimedia-commons/data/videos/mp4/ffe/fe...   \n2  s3://multimedia-commons/data/videos/mp4/ffe/f7...   \n3  file:///var/folders/hb/qd0dztsj43j_mdb6hbl1gzy...   \n4                                               None   \n5                                               None   \n\n                                     video_localpath  \n0  /Users/asiegel/.pixeltable/file_cache/682f022a...  \n1  /Users/asiegel/.pixeltable/file_cache/682f022a...  \n2  /Users/asiegel/.pixeltable/file_cache/682f022a...  \n3  /var/folders/hb/qd0dztsj43j_mdb6hbl1gzyc0000gn...  \n4                                               None  \n5                                               None  "
        }
      ]
    },
    {
      "notebook": "feature-guides/time-zones.ipynb",
      "title": "Time Zones in Pixeltable",
      "description": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/feature-guides/time-zones.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/feature-guides/time-zones.ipynb) Because typical use cases involve datasets that span multiple time zones, Pixeltable strives to be precise in how it handles time zone arithmetic for datetimes. Timestamps are always stored in the Pixeltable database in UTC, to ensure consistency across datasets and deployments. Time zone considerations therefore apply during insertion and retrieval of timestamp data.",
      "workflows": [],
      "api_usage": {},
      "key_concepts": [
        "multimodal"
      ],
      "code_snippets": [
        {
          "code": "%pip install -qU pixeltable",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/feature-guides/time-zones.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/feature-guides/time-zones.ipynb)\n# Time Zones in Pixeltable\n\nBecause typical use cases involve datasets that span multiple time zones, Pixeltable strives to be precise in how it handles time zone arithmetic for datetimes.\n\nTimestamps are always stored in the Pixeltable database in UTC, to ensure consistency across datasets and deployments. Time zone considerations therefore apply during insertion and retrieval of timestamp data.",
          "output": null
        },
        {
          "code": "import os\nos.environ['PIXELTABLE_TIME_ZONE'] = 'America/Los_Angeles'",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/feature-guides/time-zones.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/feature-guides/time-zones.ipynb)\n# Time Zones in Pixeltable\n\nBecause typical use cases involve datasets that span multiple time zones, Pixeltable strives to be precise in how it handles time zone arithmetic for datetimes.\n\nTimestamps are always stored in the Pixeltable database in UTC, to ensure consistency across datasets and deployments. Time zone considerations therefore apply during insertion and retrieval of timestamp data.\n### The Default Time Zone\n\nEvery Pixeltable deployment has a __default time zone__. The default time zone can be configured either by setting the `PIXELTABLE_TIME_ZONE` environment variable, or by adding a `time-zone` entry to the `[pixeltable]` section in `$PIXELTABLE_HOME/config.toml`. It must be a valid [IANA Time Zone](https://en.wikipedia.org/wiki/List_of_tz_database_time_zones).\n\n(See the [Pixeltable Configuration](https://pixeltable.github.io/pixeltable/config/) guide for more details on configuration options.)",
          "output": null
        },
        {
          "code": "import pixeltable as pxt\n\npxt.drop_dir('tz_demo', force=True)\npxt.create_dir('tz_demo')\nt = pxt.create_table('tz_demo.example', {'dt': pxt.Timestamp, 'note': pxt.String})",
          "context": "### Insertion and Retrieval\nWhen a `datetime` is inserted into the database, it will be converted to UTC and stored as an absolute timestamp. If the `datetime` has an explicit time zone, Pixeltable will use that time zone for the conversion; otherwise, Pixeltable will use the default time zone.\n\nWhen a `datetime` is retrieved, it will always be retrieved in the default time zone. To query in a different time zone, it is necessary to do an explicit conversion; we'll give an example of this in a moment. Let's first walk through a few examples that illustrate the default behavior.",
          "output": "Connected to Pixeltable database at: postgresql+psycopg://postgres:@/pixeltable?host=/Users/asiegel/.pixeltable/pgdata\nCreated directory `tz_demo`.\nCreated table `example`.\n"
        },
        {
          "code": "from datetime import datetime, timezone\nfrom zoneinfo import ZoneInfo\n\nnaive_dt    = datetime(2024, 8, 9, 23, 0, 0)\nexplicit_dt = datetime(2024, 8, 9, 23, 0, 0, tzinfo=ZoneInfo('America/Los_Angeles'))\nother_dt    = datetime(2024, 8, 9, 23, 0, 0, tzinfo=ZoneInfo('America/New_York'))\n\nt.insert([\n    {'dt': naive_dt,    'note': 'No time zone specified (uses default)'},\n    {'dt': explicit_dt, 'note': 'Time zone America/Los_Angeles was specified explicitly'},\n    {'dt': other_dt,    'note': 'Time zone America/New_York was specified explicitly'}\n])",
          "context": "### Insertion and Retrieval\nWhen a `datetime` is inserted into the database, it will be converted to UTC and stored as an absolute timestamp. If the `datetime` has an explicit time zone, Pixeltable will use that time zone for the conversion; otherwise, Pixeltable will use the default time zone.\n\nWhen a `datetime` is retrieved, it will always be retrieved in the default time zone. To query in a different time zone, it is necessary to do an explicit conversion; we'll give an example of this in a moment. Let's first walk through a few examples that illustrate the default behavior.",
          "output": "Inserting rows into `example`: 3 rows [00:00, 842.91 rows/s]\nInserted 3 rows with 0 errors.\n"
        },
        {
          "code": "t.collect()",
          "context": "When a `datetime` is inserted into the database, it will be converted to UTC and stored as an absolute timestamp. If the `datetime` has an explicit time zone, Pixeltable will use that time zone for the conversion; otherwise, Pixeltable will use the default time zone.\n\nWhen a `datetime` is retrieved, it will always be retrieved in the default time zone. To query in a different time zone, it is necessary to do an explicit conversion; we'll give an example of this in a moment. Let's first walk through a few examples that illustrate the default behavior.\nOn retrieval, all timestamps are normalized to the default time zone, regardless of how they were specified during insertion. ",
          "output": "                         dt                                               note\n0 2024-08-09 23:00:00-07:00              No time zone specified (uses default)\n1 2024-08-09 23:00:00-07:00  Time zone America/Los_Angeles was specified ex...\n2 2024-08-09 20:00:00-07:00  Time zone America/New_York was specified expli..."
        },
        {
          "code": "t.select(t.dt, dt_new_york=t.dt.astimezone('America/New_York'), note=t.note).collect()",
          "context": "On retrieval, all timestamps are normalized to the default time zone, regardless of how they were specified during insertion. \nTo represent timestamps in a different time zone, use the `astimezone` method.",
          "output": "                         dt               dt_new_york  \\\n0 2024-08-09 23:00:00-07:00 2024-08-10 02:00:00-04:00   \n1 2024-08-09 23:00:00-07:00 2024-08-10 02:00:00-04:00   \n2 2024-08-09 20:00:00-07:00 2024-08-09 23:00:00-04:00   \n\n                                                note  \n0              No time zone specified (uses default)  \n1  Time zone America/Los_Angeles was specified ex...  \n2  Time zone America/New_York was specified expli...  "
        },
        {
          "code": "t.select(\n    t.dt,\n    day_default=t.dt.day,\n    day_eastern=t.dt.astimezone('America/New_York').day\n).collect()",
          "context": "To represent timestamps in a different time zone, use the `astimezone` method.\n### Timestamp Methods and Properties\n\nThe Pixeltable API exposes all the standard `datetime` methods and properties from the Python library. Because retrieval uses the default time zone, they are all relative to the default time zone unless `astimezone` is used.",
          "output": "                         dt  day_default  day_eastern\n0 2024-08-09 23:00:00-07:00            9           10\n1 2024-08-09 23:00:00-07:00            9           10\n2 2024-08-09 20:00:00-07:00            9            9"
        }
      ]
    },
    {
      "notebook": "integrations/working-with-anthropic.ipynb",
      "title": "Working with Anthropic in Pixeltable",
      "description": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-anthropic.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-anthropic.ipynb) Pixeltable's Anthropic integration enables you to access Anthropic's Claude LLM via the Anthropic API. - An Anthropic account with an API key (https://docs.anthropic.com/en/api/getting-started)",
      "workflows": [],
      "api_usage": {},
      "key_concepts": [
        "computed column",
        "multimodal"
      ],
      "code_snippets": [
        {
          "code": "%pip install -qU pixeltable anthropic",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-anthropic.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-anthropic.ipynb)\n# Working with Anthropic in Pixeltable\n\nPixeltable's Anthropic integration enables you to access Anthropic's Claude LLM via the Anthropic API.\n\n### Prerequisites\n\n- An Anthropic account with an API key (https://docs.anthropic.com/en/api/getting-started)\n\n### Important Notes\n\n- Anthropic usage may incur costs based on your Anthropic plan.\n- Be mindful of sensitive data and consider security measures when integrating with external services.\n\nFirst you'll need to install required libraries and enter an Anthropic API key.",
          "output": null
        },
        {
          "code": "import os\nimport getpass\n\nif 'ANTHROPIC_API_KEY' not in os.environ:\n    os.environ['ANTHROPIC_API_KEY'] = getpass.getpass('Anthropic API Key:')",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-anthropic.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-anthropic.ipynb)\n# Working with Anthropic in Pixeltable\n\nPixeltable's Anthropic integration enables you to access Anthropic's Claude LLM via the Anthropic API.\n\n### Prerequisites\n\n- An Anthropic account with an API key (https://docs.anthropic.com/en/api/getting-started)\n\n### Important Notes\n\n- Anthropic usage may incur costs based on your Anthropic plan.\n- Be mindful of sensitive data and consider security measures when integrating with external services.\n\nFirst you'll need to install required libraries and enter an Anthropic API key.",
          "output": null
        },
        {
          "code": "import pixeltable as pxt\n\n# Remove the 'anthropic_demo' directory and its contents, if it exists\npxt.drop_dir('anthropic_demo', force=True)\npxt.create_dir('anthropic_demo')",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-anthropic.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-anthropic.ipynb)\n# Working with Anthropic in Pixeltable\n\nPixeltable's Anthropic integration enables you to access Anthropic's Claude LLM via the Anthropic API.\n\n### Prerequisites\n\n- An Anthropic account with an API key (https://docs.anthropic.com/en/api/getting-started)\n\n### Important Notes\n\n- Anthropic usage may incur costs based on your Anthropic plan.\n- Be mindful of sensitive data and consider security measures when integrating with external services.\n\nFirst you'll need to install required libraries and enter an Anthropic API key.\nNow let's create a Pixeltable directory to hold the tables for our demo.",
          "output": "Connected to Pixeltable database at: postgresql+psycopg://postgres:@/pixeltable?host=/Users/asiegel/.pixeltable/pgdata\nCreated directory `anthropic_demo`.\n"
        },
        {
          "code": "from pixeltable.functions import anthropic\n\n# Create a table in Pixeltable and add a computed column that calls Anthropic\n\nt = pxt.create_table('anthropic_demo.chat', {'input': pxt.String})\n\nmsgs = [{'role': 'user', 'content': t.input}]\nt.add_computed_column(output=anthropic.messages(\n    messages=msgs,\n    model='claude-3-haiku-20240307',\n    max_tokens=300,\n    model_kwargs={\n        # Optional dict with parameters for the Anthropic API\n        'system': 'Respond to the prompt with detailed historical information.',\n        'top_k': 40,\n        'top_p': 0.9,\n        'temperature': 0.7\n    }\n))",
          "context": "Now let's create a Pixeltable directory to hold the tables for our demo.\n## Messages\n\nCreate a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from Anthropic.",
          "output": "Created table `chat`.\nAdded 0 column values with 0 errors.\n"
        },
        {
          "code": "# Parse the response into a new column\nt.add_computed_column(response=t.output.content[0].text)",
          "context": "Now let's create a Pixeltable directory to hold the tables for our demo.\n## Messages\n\nCreate a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from Anthropic.",
          "output": "Added 0 column values with 0 errors.\n"
        },
        {
          "code": "# Start a conversation\nt.insert(input=\"What was the outcome of the 1904 US Presidential election?\")\nt.select(t.input, t.response).show()",
          "context": "Now let's create a Pixeltable directory to hold the tables for our demo.\n## Messages\n\nCreate a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from Anthropic.",
          "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:01<00:00,  1.54 cells/s]\nInserting rows into `chat`: 1 rows [00:00, 149.28 rows/s]\nComputing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:01<00:00,  1.53 cells/s]\nInserted 1 row with 0 errors.\n"
        }
      ]
    },
    {
      "notebook": "integrations/using-label-studio-with-pixeltable.ipynb",
      "title": "Using Label Studio for Annotations with Pixeltable",
      "description": "This tutorial demonstrates how to integrate Pixeltable with Label Studio, in order to provide seamless management of annotations data across the annotation workflow. We'll assume that you're at least somewhat familiar with Pixeltable and have read the [Pixeltable Basics](https://pixeltable.readme.io/docs/pixeltable-basics) tutorial. __This tutorial can only be run in a local Pixeltable installation, not in Colab or Kaggle__, since it relies on spinning up a locally running Label Studio instance. See the [Installation Guide](https://pixeltable.readme.io/docs/installation) for instructions on how to set up a local Pixeltable instance. To begin, let's ensure the requisite dependencies are installed.",
      "workflows": [],
      "api_usage": {},
      "key_concepts": [
        "incremental",
        "computed column",
        "multimodal",
        "iterator"
      ],
      "code_snippets": [
        {
          "code": "%pip install -qU pixeltable label-studio label-studio-sdk torch transformers",
          "context": "\n# Using Label Studio for Annotations with Pixeltable\n\nThis tutorial demonstrates how to integrate Pixeltable with Label Studio, in order to provide seamless management of annotations data across the annotation workflow. We'll assume that you're at least somewhat familiar with Pixeltable and have read the [Pixeltable Basics](https://pixeltable.readme.io/docs/pixeltable-basics) tutorial.\n\n__This tutorial can only be run in a local Pixeltable installation, not in Colab or Kaggle__, since it relies on spinning up a locally running Label Studio instance. See the [Installation Guide](https://pixeltable.readme.io/docs/installation) for instructions on how to set up a local Pixeltable instance.\n\nTo begin, let's ensure the requisite dependencies are installed.",
          "output": null
        },
        {
          "code": "import subprocess\nls_process = subprocess.Popen(['label-studio'], stderr=subprocess.PIPE)",
          "context": "\n# Using Label Studio for Annotations with Pixeltable\n\nThis tutorial demonstrates how to integrate Pixeltable with Label Studio, in order to provide seamless management of annotations data across the annotation workflow. We'll assume that you're at least somewhat familiar with Pixeltable and have read the [Pixeltable Basics](https://pixeltable.readme.io/docs/pixeltable-basics) tutorial.\n\n__This tutorial can only be run in a local Pixeltable installation, not in Colab or Kaggle__, since it relies on spinning up a locally running Label Studio instance. See the [Installation Guide](https://pixeltable.readme.io/docs/installation) for instructions on how to set up a local Pixeltable instance.\n\nTo begin, let's ensure the requisite dependencies are installed.\n## Set up Label Studio\n\nNow let's spin up a Label Studio server process. (If you're already running Label Studio, you can choose to skip this step, and instead enter your existing Label Studio URL and access token in the subsequent step.) Be patient, as it may take a minute or two to start.\n\nThis will open a new browser window containing the Label Studio interface. If you've never run Label Studio before, you'll need to create an account; a link to create one will appear in the Label Studio browser window. __Everything is running locally in this tutorial, so the account will exist only on your local system.__",
          "output": "Performing system checks...\n\nSystem check identified no issues (1 silenced).\nAugust 14, 2024 - 04:24:46\nDjango version 3.2.25, using settings 'label_studio.core.settings.label_studio'\nStarting development server at http://0.0.0.0:8080/\nQuit the server with CONTROL-C.\n"
        },
        {
          "code": "import getpass\nimport os\n\nif 'LABEL_STUDIO_URL' not in os.environ:\n    os.environ['LABEL_STUDIO_URL'] = 'http://localhost:8080/'\n\nif 'LABEL_STUDIO_API_KEY' not in os.environ:\n    os.environ['LABEL_STUDIO_API_KEY'] = getpass.getpass('Label Studio API key: ')",
          "context": "If for some reason the Label Studio browser window failed to open, you can always access it at: http://localhost:8080/\n\nOnce you've created an account in Label Studio, you'll need to locate your API key. In the Label Studio browser window, log in, and click on \"Account & Settings\" in the top right. Copy the Access Token from the interface.\n## Configure Pixeltable\n\nNext, we configure Pixeltable to communicate with Label Studio. Run the following command, pasting in the API key that you copied from the Label Studio interface.",
          "output": "Label Studio API key:  \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n"
        },
        {
          "code": "import pixeltable as pxt\n\nschema = {\n    'video': pxt.Video,\n    'date': pxt.Timestamp\n}\n\n# Before creating the table, we drop the `ls_demo` dir and all its contents,\n# in order to ensure a clean environment for the demo.\npxt.drop_dir('ls_demo', force=True)\npxt.create_dir('ls_demo')\nvideos_table = pxt.create_table('ls_demo.videos', schema)",
          "context": "## Configure Pixeltable\n\nNext, we configure Pixeltable to communicate with Label Studio. Run the following command, pasting in the API key that you copied from the Label Studio interface.\n## Create a Table to Store Videos\n\nNow we create the master table that will hold our videos to be annotated. This only needs to be done once, when we initially set up the workflow.",
          "output": "Connected to Pixeltable database at: postgresql://postgres:@/pixeltable?host=/Users/asiegel/.pixeltable/pgdata\nCreated directory `ls_demo`.\nCreated table `videos`.\n"
        },
        {
          "code": "from datetime import datetime\n\nurl_prefix = 'http://multimedia-commons.s3-website-us-west-2.amazonaws.com/data/videos/mp4/'\nfiles = [\n    '122/8ff/1228ff94bf742242ee7c88e4769ad5d5.mp4',\n    '2cf/a20/2cfa205eae979b31b1144abd9fa4e521.mp4',\n    'ffe/ff3/ffeff3c6bf57504e7a6cecaff6aefbc9.mp4',\n]\ntoday = datetime(2024, 4, 22)\nvideos_table.insert({'video': url_prefix + file, 'date': today} for file in files)",
          "context": "## Populate It with Data\nNow let's add some videos to the table to populate it. For this tutorial, we'll use some randomly selected videos from the Multimedia Commons archive. The table also contains a `date` field, for which we'll use a fixed date (but in a production setting, it would typically be the date on which the video was imported).",
          "output": "Inserting rows into `videos`: 3 rows [00:00, 993.05 rows/s]\nInserted 3 rows with 0 errors.\n"
        },
        {
          "code": "videos_table.head()",
          "context": "Now let's add some videos to the table to populate it. For this tutorial, we'll use some randomly selected videos from the Multimedia Commons archive. The table also contains a `date` field, for which we'll use a fixed date (but in a production setting, it would typically be the date on which the video was imported).\nLet's have a look at the table now.",
          "output": "                                               video       date\n0  /Users/asiegel/.pixeltable/file_cache/faebc341... 2024-04-22\n1  /Users/asiegel/.pixeltable/file_cache/faebc341... 2024-04-22\n2  /Users/asiegel/.pixeltable/file_cache/faebc341... 2024-04-22"
        },
        {
          "code": "# Create a view to filter on the specified date\n\nv = pxt.create_view(\n    'ls_demo.videos_2024_04_22',\n    videos_table.where(videos_table.date == today)\n)\n\n# Create a new Label Studio project and link it to the view. The\n# configuration uses Label Studio's standard XML format. This only\n# needs to be done once: after the view and project are linked,\n# the relationship is stored indefinitely in Pixeltable's metadata.\n\nlabel_config = '''\n    <View>\n      <Video name=\"video\" value=\"$video\"/>\n      <Choices name=\"video-category\" toName=\"video\" showInLine=\"true\">\n        <Choice value=\"city\"/>\n        <Choice value=\"food\"/>\n        <Choice value=\"sports\"/>\n      </Choices>\n    </View>\n    '''\n\npxt.io.create_label_studio_project(v, label_config)",
          "context": "Let's have a look at the table now.\n## Create a Label Studio project\n\nNext we'll create a new Label Studio project and link it to a new view on the Pixeltable table. You can link a Label Studio project to either a table or a view. For tables that are expecting a lot of input data, it's often easier to link to views. In this example, we'll create a view that filters the table down by date.",
          "output": "Inserting rows into `videos_2024_04_22`: 3 rows [00:00, 1864.69 rows/s]\nCreated view `videos_2024_04_22` with 3 rows, 0 exceptions.\nAdded 3 column values with 0 errors.\nComputing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 857.44 cells/s]\nLinked external store `ls_project_0` to table `videos_2024_04_22`.\nCreated 3 new task(s) in LabelStudioProject `videos_2024_04_22`.\n"
        },
        {
          "code": "v",
          "context": "## Create a Label Studio project\n\nNext we'll create a new Label Studio project and link it to a new view on the Pixeltable table. You can link a Label Studio project to either a table or a view. For tables that are expecting a lot of input data, it's often easier to link to views. In this example, we'll create a view that filters the table down by date.\nIf you look in the Label Studio UI now, you'll see that there's a new project with the name `videos_2022_04_22`, with three tasks, one for each of the videos in the view. If you want to create the project without populating it with tasks (yet), you can set `sync_immediately=False` in the call to `create_label_studio_project()`. You can always sync the table and project by calling `v.sync()`.\n\nNote also that we didn't have to specify an explicit mapping between Pixeltable columns and Label Studio data fields. This is because, by default, Pixeltable assumes the Pixeltable and Label Studio field names coincide. The data field in the Label Studio project has the name `$video`, which Pixeltable maps, by default, to the column in `ls_demo.videos_2024_02_22` that is also called `video`. If you want to override this behavior to specify an explicit mapping of columns to fields, you can do that with the `col_mapping` parameter of `create_label_studio_project()`.\n\nInspecting the view, we also see that Pixeltable created an additional column on the view, `annotations`, which will hold the output of our annotations workflow. The name of the output column can also be overridden by specifying a dict entry in `col_mapping` of the form `{'my_col_name': 'annotations'}`.",
          "output": "view 'videos_2024_04_22'\n\nColumn Name      Type Computed With\nannotations      json              \n      video     video              \n       date timestamp              "
        },
        {
          "code": "v = pxt.get_table('ls_demo.videos_2024_04_22')\nv.sync()",
          "context": "## Import the Annotations Back To Pixeltable\nNow let's try importing annotations from Label Studio back to our view.",
          "output": "Created 0 new task(s) in LabelStudioProject `videos_2024_04_22`.\nUpdated annotation(s) from 1 task(s) in LabelStudioProject `videos_2024_04_22`.\n"
        },
        {
          "code": "v.select(v.video, v.annotations).head()",
          "context": "Now let's try importing annotations from Label Studio back to our view.\nLet's see what effect that had. You'll see that any videos that you annotated now have their `annotations` field populated in the view.",
          "output": "                                               video  \\\n0  /Users/asiegel/.pixeltable/file_cache/faebc341...   \n1  /Users/asiegel/.pixeltable/file_cache/faebc341...   \n2  /Users/asiegel/.pixeltable/file_cache/faebc341...   \n\n                                         annotations  \n0  [{'id': 35, 'task': 141, 'result': [{'id': 'E_...  \n1                                               None  \n2                                               None  "
        },
        {
          "code": "v.add_computed_column(\n    video_category=v.annotations[0].result[0].value.choices[0]\n)\nv.select(v.video, v.annotations, v.video_category).head()",
          "context": "Let's see what effect that had. You'll see that any videos that you annotated now have their `annotations` field populated in the view.\n## Parse Annotations with a Computed Column\n\nPixeltable pulls in all sorts of metadata from Label Studio during a sync: everything that Label Studio reports back about the annotations, including things like the user account that created the annotations. Let's say that all we care about is the annotation value. We can add a computed column to our table to pull it out.",
          "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 394.63 cells/s]\nAdded 3 column values with 0 errors.\n"
        },
        {
          "code": "from pixeltable.functions.video import get_metadata\n\nv.add_computed_column(video_metadata=get_metadata(v.video))\nv.select(v.video, v.annotations, v.video_category, v.video_metadata).head()",
          "context": "## Parse Annotations with a Computed Column\n\nPixeltable pulls in all sorts of metadata from Label Studio during a sync: everything that Label Studio reports back about the annotations, including things like the user account that created the annotations. Let's say that all we care about is the annotation value. We can add a computed column to our table to pull it out.\nAnother useful operation is the `get_metadata` function, which returns information about the video itself, such as the resolution and codec (independent of Label Studio). Let's add another computed column to hold such metadata.",
          "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 138.97 cells/s]\nAdded 3 column values with 0 errors.\n"
        },
        {
          "code": "from datetime import datetime\nfrom pixeltable.iterators import FrameIterator\n\ntoday = datetime(2024, 4, 22)\nvideos_table = pxt.get_table('ls_demo.videos')\n\n# Create the view, using a `FrameIterator` to extract frames with a sample rate\n# of `fps=0.25`, or 1 frame per 4 seconds of video. Setting `fps=0` would use the\n# native framerate of the video, extracting every frame.\n\nframes = pxt.create_view(\n    'ls_demo.frames_2024_04_22',\n    videos_table.where(videos_table.date == today),\n    iterator=FrameIterator.create(video=videos_table.video, fps=0.25)\n)",
          "context": "## Preannotations with Pixeltable and Label Studio\nFrame extraction is another common operation in labeling workflows. In this example, we'll extract frames from our videos into a view, then use an object detection model to generate preannotations for each frame. The following code uses a Pixeltable `FrameIterator` to automatically extract frames into a new view, which we'll call `frames_2024_04_22`.",
          "output": "Inserting rows into `frames_2024_04_22`: 13 rows [00:00, 5434.66 rows/s]\nCreated view `frames_2024_04_22` with 13 rows, 0 exceptions.\n"
        },
        {
          "code": "# Show just the first 3 frames in the table, to avoid cluttering the notebook\n\nframes.select(frames.frame).head(3)",
          "context": "## Preannotations with Pixeltable and Label Studio\nFrame extraction is another common operation in labeling workflows. In this example, we'll extract frames from our videos into a view, then use an object detection model to generate preannotations for each frame. The following code uses a Pixeltable `FrameIterator` to automatically extract frames into a new view, which we'll call `frames_2024_04_22`.",
          "output": "                                               frame\n0  <PIL.Image.Image image mode=RGB size=640x480 a...\n1  <PIL.Image.Image image mode=RGB size=640x480 a...\n2  <PIL.Image.Image image mode=RGB size=640x480 a..."
        },
        {
          "code": "from pixeltable.functions.huggingface import detr_for_object_detection\n\n# Run the Resnet-50 object detection model against each frame to generate bounding boxes\nframes.add_computed_column(detections=detr_for_object_detection(\n    frames.frame,\n    model_id='facebook/detr-resnet-50',\n    threshold=0.95\n)\nframes.select(frames.frame, frames.detections).head(3)",
          "context": "Frame extraction is another common operation in labeling workflows. In this example, we'll extract frames from our videos into a view, then use an object detection model to generate preannotations for each frame. The following code uses a Pixeltable `FrameIterator` to automatically extract frames into a new view, which we'll call `frames_2024_04_22`.\nNow we'll use the Resnet-50 object detection model to generate preannotations. We do this by creating a new computed column.",
          "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13/13 [00:06<00:00,  2.10 cells/s]\nAdded 13 column values with 0 errors.\n"
        },
        {
          "code": "from pixeltable.functions.huggingface import detr_to_coco\n\nframes.add_computed_column(\n    preannotations=detr_to_coco(frames.frame, frames.detections)\n)\nframes.select(frames.frame, frames.detections, frames.preannotations).head(3)",
          "context": "Now we'll use the Resnet-50 object detection model to generate preannotations. We do this by creating a new computed column.\nWe'd like to send these detections to Label Studio as preannotations, but they're not quite ready. Label Studio expects preannotations in standard COCO format, but the Huggingface library outputs them in its own custom format. We can use Pixeltable's handy `detr_to_coco` function to do the conversion, using another computed column.",
          "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13/13 [00:00<00:00, 42.79 cells/s]\nAdded 13 column values with 0 errors.\n"
        },
        {
          "code": "frames_config = '''\n    <View>\n      <Image name=\"frame\" value=\"$frame\"/>\n      <RectangleLabels name=\"preannotations\" toName=\"frame\">\n        <Label value=\"car\" background=\"blue\"/>\n        <Label value=\"person\" background=\"red\"/>\n        <Label value=\"train\" background=\"green\"/>\n      </RectangleLabels>\n    </View>\n    '''\n\npxt.io.create_label_studio_project(frames, frames_config)",
          "context": "## Create a Label Studio Project for Frames\nWith our data workflow set up and the COCO preannotations prepared, all that's left is to create a corresponding Label Studio project. Note how Pixeltable automatically maps `RectangleLabels` preannotation fields to columns, just like it does with data fields. Here, Pixeltable interprets the `name=\"preannotations\"` attribute in `RectangleLabels` to mean, \"map these rectangle labels to the `preannotations` column in my linked table or view\".\n\nThe Label values `car`, `person`, and `train` are standard COCO object identifiers used by many off-the-shelf object detection models. You can find the complete list of them here, and include as many as you wish: https://raw.githubusercontent.com/pixeltable/pixeltable/release/docs/resources/coco-categories.csv",
          "output": "Added 13 column values with 0 errors.\nComputing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 13/13 [00:00<00:00, 42.09 cells/s]\nLinked external store `ls_project_0` to table `frames_2024_04_22`.\nCreated 13 new task(s) in LabelStudioProject `frames_2024_04_22`.\n"
        },
        {
          "code": "videos_table.insert(\n    video=url_prefix + '22a/948/22a9487a92956ac453a9c15e0fc4dd4.mp4',\n    date=today\n)",
          "context": "If you go into Label Studio and open up the new project, you can see the effect of adding the preannotations from Resnet-50 to our workflow.\n## Incremental Updates\n\nAs we saw in the [Pixeltable Basics](https://pixeltable.readme.io/docs/pixeltable-basics) tutorial, adding new data to Pixeltable results in incremental updates of everything downstream. We can see this by inserting a new video into our base videos table: all of the downstream views and computed columns are updated automatically, including the video metadata, frames, and preannotations.\n\nThe update may take some time, so please be patient (it involves a sequence of operations, including frame extraction and object detection).",
          "output": "Inserting rows into `videos`: 1 rows [00:00, 808.31 rows/s]\nInserting rows into `videos_2024_04_22`: 1 rows [00:00, 849.57 rows/s]\nInserting rows into `frames_2024_04_22`: 5 rows [00:00, 3225.89 rows/s]\nInserted 7 rows with 0 errors.\n"
        },
        {
          "code": "v.sync()\nframes.sync()",
          "context": "## Incremental Updates\n\nAs we saw in the [Pixeltable Basics](https://pixeltable.readme.io/docs/pixeltable-basics) tutorial, adding new data to Pixeltable results in incremental updates of everything downstream. We can see this by inserting a new video into our base videos table: all of the downstream views and computed columns are updated automatically, including the video metadata, frames, and preannotations.\n\nThe update may take some time, so please be patient (it involves a sequence of operations, including frame extraction and object detection).\nNote that the incremental updates do _not_ automatically sync the `Table` with the remote Label Studio projects. To issue a sync, we have to call the `sync()` methods separately. Note that tasks will be created only for the _newly added_ rows in the videos and frames views, not the existing ones.",
          "output": "Created 1 new task(s) in LabelStudioProject `videos_2024_04_22`.\nCreated 5 new task(s) in LabelStudioProject `frames_2024_04_22`.\n"
        },
        {
          "code": "v.external_stores  # Get a list of all external stores for `v`",
          "context": "Note that the incremental updates do _not_ automatically sync the `Table` with the remote Label Studio projects. To issue a sync, we have to call the `sync()` methods separately. Note that tasks will be created only for the _newly added_ rows in the videos and frames views, not the existing ones.\n## Deleting a Project\n\nTo remove a Label Studio project from a table or view, use `unlink_external_stores()`, as demonstrated by the following example. If you specify `delete_external_data=True`, then the Label Studio project will also be deleted, along with all existing data and annotations (be careful!) If `delete_external_data=False`, then the Label Studio project will be unlinked from Pixeltable, but the project and data will remain in Label Studio (so you'll need to delete the project manually if you later want to get rid of it).",
          "output": "['ls_project_0']"
        },
        {
          "code": "v.unlink_external_stores('ls_project_0', delete_external_data=True)",
          "context": "Note that the incremental updates do _not_ automatically sync the `Table` with the remote Label Studio projects. To issue a sync, we have to call the `sync()` methods separately. Note that tasks will be created only for the _newly added_ rows in the videos and frames views, not the existing ones.\n## Deleting a Project\n\nTo remove a Label Studio project from a table or view, use `unlink_external_stores()`, as demonstrated by the following example. If you specify `delete_external_data=True`, then the Label Studio project will also be deleted, along with all existing data and annotations (be careful!) If `delete_external_data=False`, then the Label Studio project will be unlinked from Pixeltable, but the project and data will remain in Label Studio (so you'll need to delete the project manually if you later want to get rid of it).",
          "output": "Deleted Label Studio project: videos_2024_04_22\nUnlinked external store from table `videos_2024_04_22`: ls_project_0\n"
        },
        {
          "code": "pxt.io.create_label_studio_project(\n    v,\n    label_config,\n    media_import_method='url',\n    s3_configuration={'bucket': 'pxt-test', 'aws_access_key_id': my_key, 'aws_secret_access_key': my_secret}\n)",
          "context": "## Deleting a Project\n\nTo remove a Label Studio project from a table or view, use `unlink_external_stores()`, as demonstrated by the following example. If you specify `delete_external_data=True`, then the Label Studio project will also be deleted, along with all existing data and annotations (be careful!) If `delete_external_data=False`, then the Label Studio project will be unlinked from Pixeltable, but the project and data will remain in Label Studio (so you'll need to delete the project manually if you later want to get rid of it).\n## Configuring `media_import_method`\n\nAll of the examples so far in this tutorial use HTTP file uploads to send media data to Label Studio. This is the simplest method and the easiest to configure, but it's undesirable for complex projects or projects with a lot of data. In fact, the Label Studio documentation includes this specific warning: \"Uploading data works fine for proof of concept projects, but it is not recommended for larger projects.\"\n\nIn Pixeltable, you can configure linked Label Studio projects to use URLs for media data (instead of file uploads) by specifying the `media_import_method='url'` argument in `create_label_studio_project`. This is recommended for all production applications, and is mandatory for projects whose input configuration is more complex than a single media file (in the Label Studio parlance, projects with more than one \"data key\").\n\nIf `media_import_method='url'`, then Pixeltable will simply pass the media data URLs directly to Label Studio. If the URLs are `http://` or `https://` URLs, then nothing more needs to be done.\n\nLabel Studio also supports `s3://` URLs with credentialed access. To use them, you'll need to configure access to your bucket in the project configuration. The simplest way to do this is by specifying an `s3_configuration` in `create_label_studio_project`. Here's an example, though it won't work directly in this demo notebook, since it relies on having an access key. (If your AWS credentials are stored in `~/.aws/credentials`, then you can omit the access key and secret, and Pixeltable will fill them in automatically.)",
          "output": null
        },
        {
          "code": "ls_process.kill()",
          "context": "Before you can set up credentialed S3 access, you'll need to configure your S3 bucket to work with Label Studio; the details on how to do this are described here:\n- [Label Studio Docs: Amazon S3](https://labelstud.io/guide/storage.html#Amazon-S3)\n\nFor the full documentation on `create_label_studio_project` usage, see:\n- [Pixeltable API Docs: create_label_studio_project](https://pixeltable.github.io/pixeltable/api/io/#pixeltable.io.create_label_studio_project)\n## Notebook Cleanup\n\nThat's the end of the tutorial! To conclude, let's terminate the running Label Studio process. (Of course, feel free to leave it running if you want to play around with it some more.)",
          "output": null
        }
      ]
    },
    {
      "notebook": "integrations/working-with-replicate.ipynb",
      "title": "Working with Replicate in Pixeltable",
      "description": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-replicate.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-replicate.ipynb) Pixeltable's Replicate integration enables you to access Replicate's models via the Replicate API. - A Replicate account with an API token.",
      "workflows": [],
      "api_usage": {},
      "key_concepts": [
        "computed column",
        "multimodal"
      ],
      "code_snippets": [
        {
          "code": "%pip install -qU pixeltable replicate",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-replicate.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-replicate.ipynb)\n# Working with Replicate in Pixeltable\n\nPixeltable's Replicate integration enables you to access Replicate's models via the Replicate API.\n\n### Prerequisites\n\n- A Replicate account with an API token.\n\n### Important Notes\n\n- Replicate usage may incur costs based on your Replicate plan.\n- Be mindful of sensitive data and consider security measures when integrating with external services.\n\nFirst you'll need to install required libraries and enter a Replicate API token.",
          "output": null
        },
        {
          "code": "import os\nimport getpass\n\nif 'REPLICATE_API_TOKEN' not in os.environ:\n    os.environ['REPLICATE_API_TOKEN'] = getpass.getpass('Replicate API Token:')",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-replicate.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-replicate.ipynb)\n# Working with Replicate in Pixeltable\n\nPixeltable's Replicate integration enables you to access Replicate's models via the Replicate API.\n\n### Prerequisites\n\n- A Replicate account with an API token.\n\n### Important Notes\n\n- Replicate usage may incur costs based on your Replicate plan.\n- Be mindful of sensitive data and consider security measures when integrating with external services.\n\nFirst you'll need to install required libraries and enter a Replicate API token.",
          "output": null
        },
        {
          "code": "import pixeltable as pxt\n\n# Remove the `replicate_demo` directory and its contents, if it exists\npxt.drop_dir('replicate_demo', force=True)\npxt.create_dir('replicate_demo')",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-replicate.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-replicate.ipynb)\n# Working with Replicate in Pixeltable\n\nPixeltable's Replicate integration enables you to access Replicate's models via the Replicate API.\n\n### Prerequisites\n\n- A Replicate account with an API token.\n\n### Important Notes\n\n- Replicate usage may incur costs based on your Replicate plan.\n- Be mindful of sensitive data and consider security measures when integrating with external services.\n\nFirst you'll need to install required libraries and enter a Replicate API token.\nNow let's create a Pixeltable directory to hold the tables for our demo.",
          "output": "Connected to Pixeltable database at: postgresql+psycopg://postgres:@/pixeltable?host=/Users/asiegel/.pixeltable/pgdata\nCreated directory `replicate_demo`.\n"
        },
        {
          "code": "from pixeltable.functions.replicate import run\n\n# Create a table in Pixeltable and pick a model hosted on Replicate with some parameters\n\nt = pxt.create_table('replicate_demo.chat', {'prompt': pxt.String})\n\ninput = {\n    'system_prompt': 'You are a helpful assistant.',\n    'prompt': t.prompt,\n    # These parameters are optional and can be used to tune model behavior:\n    'max_tokens': 300,\n    'top_p': 0.9,\n    'temperature': 0.8\n}\nt.add_computed_column(output=run(input, ref='meta/meta-llama-3-8b-instruct'))",
          "context": "Now let's create a Pixeltable directory to hold the tables for our demo.\n## Chat Completions\n\nCreate a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from Replicate.",
          "output": "Created table `chat`.\nAdded 0 column values with 0 errors.\n"
        },
        {
          "code": "# Parse the response into a new column\nt.add_computed_column(response=pxt.functions.string.join('', t.output))",
          "context": "Now let's create a Pixeltable directory to hold the tables for our demo.\n## Chat Completions\n\nCreate a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from Replicate.",
          "output": "Added 0 column values with 0 errors.\n"
        },
        {
          "code": "# Start a conversation\nt.insert(prompt='What foods are rich in selenium?')\nt.select(t.prompt, t.response).show()",
          "context": "Now let's create a Pixeltable directory to hold the tables for our demo.\n## Chat Completions\n\nCreate a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from Replicate.",
          "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:07<00:00,  1.89s/ cells]\nInserting rows into `chat`: 1 rows [00:00, 171.89 rows/s]\nComputing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:07<00:00,  1.89s/ cells]\nInserted 1 row with 0 errors.\n"
        },
        {
          "code": "t = pxt.create_table('replicate_demo.images', {'prompt': pxt.String})\n\ninput = {\n    'prompt': t.prompt,\n    'go_fast': True,\n    'megapixels': '1'\n}\nt.add_computed_column(output=run(input, ref='black-forest-labs/flux-schnell'))",
          "context": "## Chat Completions\n\nCreate a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from Replicate.\n## Image Generation\n\nHere's an example that shows how to use Replicate's image generation models with Pixeltable. We'll use the FLUX Schnell model.",
          "output": "Created table `images`.\nAdded 0 column values with 0 errors.\n"
        },
        {
          "code": "t.insert(prompt='Draw a pencil sketch of a friendly dinosaur playing tennis in a cornfield.')",
          "context": "## Chat Completions\n\nCreate a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from Replicate.\n## Image Generation\n\nHere's an example that shows how to use Replicate's image generation models with Pixeltable. We'll use the FLUX Schnell model.",
          "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00,  2.17 cells/s]\nInserting rows into `images`: 1 rows [00:00, 198.61 rows/s]\nComputing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00,  2.14 cells/s]\nInserted 1 row with 0 errors.\n"
        },
        {
          "code": "t.select(t.prompt, t.output).collect()",
          "context": "## Chat Completions\n\nCreate a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from Replicate.\n## Image Generation\n\nHere's an example that shows how to use Replicate's image generation models with Pixeltable. We'll use the FLUX Schnell model.",
          "output": "                                              prompt  \\\n0  Draw a pencil sketch of a friendly dinosaur pl...   \n\n                                              output  \n0  [https://replicate.delivery/yhqm/DH2e9QRelTr0K...  "
        },
        {
          "code": "t.add_computed_column(image=t.output[0].astype(pxt.Image))\nt.select(t.image).collect()",
          "context": "## Image Generation\n\nHere's an example that shows how to use Replicate's image generation models with Pixeltable. We'll use the FLUX Schnell model.\nWe see that Replicate returns our image as an array containing a single URL. To turn it into an actual image, we cast the string to type `pxt.Image` in a new computed column:",
          "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 1/1 [00:00<00:00, 93.29 cells/s]\nAdded 1 column value with 0 errors.\n"
        }
      ]
    },
    {
      "notebook": "integrations/working-with-hugging-face.ipynb",
      "title": "Working with Hugging Face",
      "description": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-hugging-face.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-hugging-face.ipynb) Pixeltable unifies data and computation into a table interface. In this tutorial, we'll go into more depth on the Hugging Face integration between datasets and how Hugging Face models can be incorporated into Pixeltable workflows to run models locally. Now let's load the Hugging Face dataset, as described in the [Hugging Face documentation](https://huggingface.co/docs/datasets/en/package_reference/loading_methods).",
      "workflows": [],
      "api_usage": {},
      "key_concepts": [
        "incremental",
        "udf",
        "embedding",
        "iterator",
        "multimodal"
      ],
      "code_snippets": [
        {
          "code": "%pip install -qU pixeltable datasets torch transformers tiktoken spacy",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-hugging-face.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-hugging-face.ipynb)\n\n# Working with Hugging Face\nPixeltable unifies data and computation into a table interface. In this tutorial, we'll go into more depth on the Hugging Face integration between datasets and how Hugging Face models can be incorporated into Pixeltable workflows to run models locally.",
          "output": null
        },
        {
          "code": "import datasets\n\npadoru = (\n    datasets.load_dataset(\"not-lain/padoru\", split='train')\n    .select_columns(['Image', 'ImageSize', 'Name', 'ImageSource'])\n)",
          "context": "Pixeltable unifies data and computation into a table interface. In this tutorial, we'll go into more depth on the Hugging Face integration between datasets and how Hugging Face models can be incorporated into Pixeltable workflows to run models locally.\nNow let's load the Hugging Face dataset, as described in the [Hugging Face documentation](https://huggingface.co/docs/datasets/en/package_reference/loading_methods).",
          "output": null
        },
        {
          "code": "padoru",
          "context": "Now let's load the Hugging Face dataset, as described in the [Hugging Face documentation](https://huggingface.co/docs/datasets/en/package_reference/loading_methods).\nIt preserves the Hugging Face information about whether the data is part of the *test*, *train* or *validation* split.",
          "output": "Dataset({\n    features: ['Image', 'ImageSize', 'Name', 'ImageSource'],\n    num_rows: 382\n})"
        },
        {
          "code": "import pixeltable as pxt\n\npxt.drop_dir('hf_demo', force=True)\npxt.create_dir('hf_demo')\nt = pxt.io.import_huggingface_dataset('hf_demo.padoru', padoru)",
          "context": "## Create a Pixeltable Table from a Hugging Face Dataset\nNow we create a table and Pixeltable will map column types as needed. Check out other ways to bring data into Pixeltable with [pixeltable.io](https://pixeltable.github.io/pixeltable/api/io/) such as csv, parquet, pandas, json and others.",
          "output": "Connected to Pixeltable database at: postgresql+psycopg://postgres:@/pixeltable?host=/Users/asiegel/.pixeltable/pgdata\nCreated directory `hf_demo`.\nCreated table `padoru_tmp_8951741`.\nComputing cells:  13%|\u2588\u2588\u2588\u2588\u2588\u258f                                   | 64/504 [00:01<00:07, 58.91 cells/s]\nComputing cells:  25%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                             | 128/504 [00:01<00:05, 73.21 cells/s]\nComputing cells:  38%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                        | 192/504 [00:02<00:05, 61.52 cells/s]\nComputing cells:  51%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                   | 256/504 [00:04<00:04, 57.96 cells/s]\nComputing cells:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d              | 320/504 [00:04<00:02, 64.93 cells/s]\nComputing cells:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d         | 384/504 [00:06<00:02, 55.42 cells/s]\nComputing cells:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c    | 448/504 [00:07<00:00, 67.03 cells/s]\nComputing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 504/504 [00:08<00:00, 51.01 cells/s]\nInserting rows into `padoru_tmp_8951741`: 126 rows [00:07, 17.34 rows/s]\u001b[A\nComputing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 504/504 [00:08<00:00, 60.68 cells/s]\nInserted 126 rows with 0 errors.\nComputing cells:  13%|\u2588\u2588\u2588\u2588\u2588\u258f                                   | 64/504 [00:00<00:06, 69.02 cells/s]\nComputing cells:  25%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                             | 128/504 [00:01<00:05, 69.13 cells/s]\nComputing cells:  38%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                        | 192/504 [00:02<00:04, 69.73 cells/s]\nComputing cells:  51%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                   | 256/504 [00:03<00:03, 69.73 cells/s]\nComputing cells:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d              | 320/504 [00:04<00:02, 68.81 cells/s]\nComputing cells:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d         | 384/504 [00:05<00:01, 69.80 cells/s]\nComputing cells:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c    | 448/504 [00:06<00:00, 70.41 cells/s]\nComputing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 504/504 [00:07<00:00, 69.62 cells/s]\nInserting rows into `padoru_tmp_8951741`: 126 rows [00:06, 19.92 rows/s]\u001b[A\nComputing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 504/504 [00:07<00:00, 69.45 cells/s]\nInserted 126 rows with 0 errors.\nComputing cells:  13%|\u2588\u2588\u2588\u2588\u2588\u258f                                   | 64/504 [00:00<00:06, 72.73 cells/s]\nComputing cells:  25%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                             | 128/504 [00:01<00:05, 71.98 cells/s]\nComputing cells:  38%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258f                        | 192/504 [00:02<00:04, 71.21 cells/s]\nComputing cells:  51%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258e                   | 256/504 [00:03<00:03, 71.61 cells/s]\nComputing cells:  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d              | 320/504 [00:04<00:02, 70.11 cells/s]\nComputing cells:  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258d         | 384/504 [00:05<00:01, 69.79 cells/s]\nComputing cells:  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u258c    | 448/504 [00:06<00:00, 68.98 cells/s]\nComputing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 504/504 [00:07<00:00, 68.68 cells/s]\nInserting rows into `padoru_tmp_8951741`: 126 rows [00:06, 19.96 rows/s]\u001b[A\nComputing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 504/504 [00:07<00:00, 70.04 cells/s]\nInserted 126 rows with 0 errors.\nComputing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16/16 [00:00<00:00, 67.34 cells/s]\nInserting rows into `padoru_tmp_8951741`: 4 rows [00:00, 3502.55 rows/s]\nComputing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 16/16 [00:00<00:00, 66.64 cells/s]\nInserted 4 rows with 0 errors.\n"
        },
        {
          "code": "t.head(3)",
          "context": "## Create a Pixeltable Table from a Hugging Face Dataset\nNow we create a table and Pixeltable will map column types as needed. Check out other ways to bring data into Pixeltable with [pixeltable.io](https://pixeltable.github.io/pixeltable/api/io/) such as csv, parquet, pandas, json and others.",
          "output": "                                               Image  ImageSize  \\\n0  <PIL.WebPImagePlugin.WebPImageFile image mode=...     240993   \n1  <PIL.WebPImagePlugin.WebPImageFile image mode=...     993097   \n2  <PIL.WebPImagePlugin.WebPImageFile image mode=...     255549   \n\n            Name                                     ImageSource  \n0        AI-Chan  https://knowyourmeme.com/photos/1439336-padoru  \n1       Platelet  https://knowyourmeme.com/photos/1438687-padoru  \n2  Nezuko Kamado  https://knowyourmeme.com/photos/1568913-padoru  "
        },
        {
          "code": "from pixeltable.functions.huggingface import clip\nimport PIL.Image\n\n# create embedding index on the 'Image' column\nt.add_embedding_index(\n    'Image',\n    embedding=clip.using(model_id='openai/clip-vit-base-patch32')\n)",
          "context": "## Leveraging Hugging Face Models with Pixeltable's Embedding Functionality\n\nPixeltable contains a built-in adapter for certain model families, so all we have to do is call the [Pixeltable function for Hugging Face](https://pixeltable.github.io/pixeltable/api/functions/huggingface/). A nice thing about the Huggingface models is that they run locally, so you don't need an account with a service provider in order to use them.\nPixeltable can also create and populate an index with `table.add_embedding_index()` for string and image embeddings. That definition is persisted as part of the table's metadata, which allows Pixeltable to maintain the index in response to updates to the table.\n\nIn this example we are using `CLIP`. You can use any embedding function you like, via Pixeltable's UDF mechanism (which is described in detail our [guide to user-defined functions](https://docs.pixeltable.com/docs/user-defined-functions-udfs)).",
          "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 382/382 [00:16<00:00, 22.63 cells/s]\n"
        },
        {
          "code": "sample_img = t.select(t.Image).head(1)[0]['Image']\n\nsim = t.Image.similarity(sample_img)\n\n# use 'similarity()' in the order_by() clause and apply a limit in order to utilize the index\nt.order_by(sim, asc=False).limit(3).select(t.Image, sim=sim).collect()",
          "context": "## Leveraging Hugging Face Models with Pixeltable's Embedding Functionality\n\nPixeltable contains a built-in adapter for certain model families, so all we have to do is call the [Pixeltable function for Hugging Face](https://pixeltable.github.io/pixeltable/api/functions/huggingface/). A nice thing about the Huggingface models is that they run locally, so you don't need an account with a service provider in order to use them.\nPixeltable can also create and populate an index with `table.add_embedding_index()` for string and image embeddings. That definition is persisted as part of the table's metadata, which allows Pixeltable to maintain the index in response to updates to the table.\n\nIn this example we are using `CLIP`. You can use any embedding function you like, via Pixeltable's UDF mechanism (which is described in detail our [guide to user-defined functions](https://docs.pixeltable.com/docs/user-defined-functions-udfs)).",
          "output": "                                               Image       sim\n0  <PIL.WebPImagePlugin.WebPImageFile image mode=...  1.000000\n1  <PIL.WebPImagePlugin.WebPImageFile image mode=...  0.962924\n2  <PIL.WebPImagePlugin.WebPImageFile image mode=...  0.960727"
        }
      ]
    },
    {
      "notebook": "integrations/working-with-fireworks.ipynb",
      "title": "Working with Fireworks AI in Pixeltable",
      "description": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-fireworks.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-fireworks.ipynb) Pixeltable's Fireworks integration enables you to access LLMs hosted on the Fireworks platform. - A Fireworks account with an API key (https://fireworks.ai/api-keys)",
      "workflows": [],
      "api_usage": {},
      "key_concepts": [
        "computed column",
        "multimodal"
      ],
      "code_snippets": [
        {
          "code": "%pip install -qU pixeltable fireworks-ai",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-fireworks.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-fireworks.ipynb)\n# Working with Fireworks AI in Pixeltable\n\nPixeltable's Fireworks integration enables you to access LLMs hosted on the Fireworks platform.\n\n### Prerequisites\n\n- A Fireworks account with an API key (https://fireworks.ai/api-keys)\n\n### Important Notes\n\n- Fireworks usage may incur costs based on your Fireworks plan.\n- Be mindful of sensitive data and consider security measures when integrating with external services.\n\nFirst you'll need to install required libraries and enter a Fireworks API key.",
          "output": null
        },
        {
          "code": "import os\nimport getpass\n\nif 'FIREWORKS_API_KEY' not in os.environ:\n    os.environ['FIREWORKS_API_KEY'] = getpass.getpass('Fireworks API Key:')",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-fireworks.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-fireworks.ipynb)\n# Working with Fireworks AI in Pixeltable\n\nPixeltable's Fireworks integration enables you to access LLMs hosted on the Fireworks platform.\n\n### Prerequisites\n\n- A Fireworks account with an API key (https://fireworks.ai/api-keys)\n\n### Important Notes\n\n- Fireworks usage may incur costs based on your Fireworks plan.\n- Be mindful of sensitive data and consider security measures when integrating with external services.\n\nFirst you'll need to install required libraries and enter a Fireworks API key.",
          "output": null
        },
        {
          "code": "import pixeltable as pxt\n\n# Remove the 'fireworks_demo' directory and its contents, if it exists\npxt.drop_dir('fireworks_demo', force=True)\npxt.create_dir('fireworks_demo')",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-fireworks.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-fireworks.ipynb)\n# Working with Fireworks AI in Pixeltable\n\nPixeltable's Fireworks integration enables you to access LLMs hosted on the Fireworks platform.\n\n### Prerequisites\n\n- A Fireworks account with an API key (https://fireworks.ai/api-keys)\n\n### Important Notes\n\n- Fireworks usage may incur costs based on your Fireworks plan.\n- Be mindful of sensitive data and consider security measures when integrating with external services.\n\nFirst you'll need to install required libraries and enter a Fireworks API key.\nNow let's create a Pixeltable directory to hold the tables for our demo.",
          "output": "Connected to Pixeltable database at: postgresql+psycopg://postgres:@/pixeltable?host=/Users/asiegel/.pixeltable/pgdata\nCreated directory 'fireworks_demo'.\n"
        },
        {
          "code": "from pixeltable.functions.fireworks import chat_completions\n\n# Create a table in Pixeltable and pick a model hosted on Fireworks with some parameters\n\nt = pxt.create_table('fireworks_demo.chat', {'input': pxt.String})\n\nmessages = [{'role': 'user', 'content': t.input}]\nt.add_computed_column(output=chat_completions(\n    messages=messages,\n    model='accounts/fireworks/models/mixtral-8x22b-instruct',\n    model_kwargs={\n        # Optional dict with parameters for the Fireworks API\n        'max_tokens': 300,\n        'top_k': 40,\n        'top_p': 0.9,\n        'temperature': 0.7\n    }\n))",
          "context": "Now let's create a Pixeltable directory to hold the tables for our demo.\n## Completions\n\nCreate a Table: In Pixeltable, create a table with columns to  represent your input data and the columns where you want to store the results from Fireworks.",
          "output": "Created table `chat`.\nAdded 0 column values with 0 errors.\n"
        },
        {
          "code": "# Parse the bot_response into a new column\nt.add_computed_column(response=t.output.choices[0].message.content)",
          "context": "Now let's create a Pixeltable directory to hold the tables for our demo.\n## Completions\n\nCreate a Table: In Pixeltable, create a table with columns to  represent your input data and the columns where you want to store the results from Fireworks.",
          "output": "Added 0 column values with 0 errors.\n"
        },
        {
          "code": "# Start a conversation\nt.insert(input=\"Can you tell me who was President of the US in 1961?\")\nt.select(t.input, t.response).show()",
          "context": "Now let's create a Pixeltable directory to hold the tables for our demo.\n## Completions\n\nCreate a Table: In Pixeltable, create a table with columns to  represent your input data and the columns where you want to store the results from Fireworks.",
          "output": "Inserting rows into `chat`: 1 rows [00:00, 85.10 rows/s]\nInserted 1 row with 0 errors.\n"
        }
      ]
    },
    {
      "notebook": "integrations/working-with-fiftyone.ipynb",
      "title": "Working with Voxel51 in Pixeltable",
      "description": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-fiftyone.ipynb) [![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-fiftyone.ipynb) Pixeltable can export data directly from tables and views to the popular [Voxel51](https://voxel51.com/) frontend, providing a way to visualize and explore image and video datasets. In this tutorial, we'll learn how to: - Export data from Pixeltable to Voxel51 - Apply labels from image classification and object detection models to exported data",
      "workflows": [
        {
          "title": "Example 1: An Image Dataset",
          "explanation": [
            "Now we export our new table to a Voxel51 dataset and load it into a new Voxel51 session within our demo notebook. Once it's been loaded, the images can be interactively navigated as with any other Voxel51 dataset.",
            "## Adding Labels",
            "We'll now show how Voxel51 labels can be attached to the exported dataset. Currently, Pixeltable supports only classification and detection labels; other Voxel51 label types may be added in the future.\n\nFirst, let's generate some labels by applying two models from the Huggingface `transformers` library: A ViT model for image classification and a DETR model for object detection.",
            "Both models output JSON containing the model results. Let's peek at the contents of our table now:",
            "Now we need to transform our model data into the format the Voxel51 API expects (see the Pixeltable documentation for [pxt.io.export_images_as_fo_dataset](https://pixeltable.github.io/pixeltable/api/io/#pixeltable.io.export_images_as_fo_dataset) for details). We'll use Pixeltable UDFs to do the appropriate conversions.",
            "We can test that our UDFs are working as expected with a `select()` statement.",
            "Now we pass the modified structures to `export_images_as_fo_dataset`.",
            "## Adding Multiple Label Sets\n\nYou can include multiple label sets of the same type in the same dataset by passing a `list` or `dict` of expressions to the `classifications` and/or `detections` parameters. If a `list` is specified, default names will be assigned to the label sets; if a `dict` is specified, the label sets will be named according to its keys.\n\nAs an example, let's try recomputing our detections using the more powerful DETR model ResNet-101, and then load them into the same Voxel51 dataset as the earlier detections in order to compare them side-by-side.",
            "Exploring the resulting images, we can see that the results are not much different between the two models, at least on our small sample dataset."
          ],
          "code_blocks": [
            {
              "code": "import fiftyone as fo\nimport pixeltable as pxt\n\n# Create a Pixeltable directory for the demo. We first drop the directory if it\n# exists, in order to ensure a clean environment.\n\npxt.drop_dir('fo_demo', force=True)\npxt.create_dir('fo_demo')",
              "output": "Connected to Pixeltable database at: postgresql+psycopg://postgres:@/pixeltable?host=/Users/asiegel/.pixeltable/pgdata\nCreated directory `fo_demo`.\n",
              "explanation": ""
            },
            {
              "code": "# Create a Pixeltable table for our dataset and insert some sample images.\n\nurl_prefix = 'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images'\n\nurls = [\n    'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images/000000000019.jpg',\n    'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images/000000000025.jpg',\n    'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images/000000000030.jpg',\n    'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images/000000000034.jpg',\n]\n\nt = pxt.create_table('fo_demo.images', {'image': pxt.Image})\nt.insert({'image': url} for url in urls)\nt.head()",
              "output": "Created table images.\nInserting rows into images: 4 rows [00:00, 2775.85 rows/s]\nInserted 4 rows with 0 errors.\n",
              "explanation": ""
            },
            {
              "code": "fo_dataset = pxt.io.export_images_as_fo_dataset(t, t.image)\nsession = fo.launch_app(fo_dataset)",
              "output": "",
              "explanation": ""
            },
            {
              "code": "from pixeltable.functions.huggingface import vit_for_image_classification, detr_for_object_detection\n\nt.add_computed_column(classifications=vit_for_image_classification(\n    t.image, model_id='google/vit-base-patch16-224'\n))\nt.add_computed_column(detections=detr_for_object_detection(\n    t.image, model_id='facebook/detr-resnet-50'\n))",
              "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00,  5.86 cells/s]\nAdded 4 column values with 0 errors.\nComputing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:01<00:00,  2.15 cells/s]\nAdded 4 column values with 0 errors.\n",
              "explanation": ""
            },
            {
              "code": "t.head()",
              "output": "                                               image  \\\n0  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n1  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n2  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n3  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n\n                                     classifications  \\\n0  {'labels': [345, 690, 912, 346, 730], 'scores'...   \n1  {'labels': [340, 353, 386, 9, 352], 'scores': ...   \n2  {'labels': [883, 738, 708, 725, 716], 'scores'...   \n3  {'labels': [340, 353, 386, 352, 9], 'scores': ...   \n\n                                          detections  \n0  {'boxes': [[335.8553771972656, 43.413597106933...  \n1  {'boxes': [[51.96046829223633, 356.18701171875...  \n2  {'boxes': [[238.07403564453125, 155.5013275146...  \n3  {'boxes': [[-0.230712890625, 19.50192070007324...  ",
              "explanation": ""
            },
            {
              "code": "@pxt.udf\ndef vit_to_fo(vit_labels: list) -> list:\n    return [\n        {'label': label, 'confidence': score}\n        for label, score in zip(vit_labels['label_text'], vit_labels['scores'])\n    ]\n\n@pxt.udf\ndef detr_to_fo(img: pxt.Image, detr_labels: dict) -> list:\n    result = []\n    for label, box, score in zip(detr_labels['label_text'], detr_labels['boxes'], detr_labels['scores']):\n        # DETR gives us bounding boxes in (x1,y1,x2,y2) absolute (pixel) coordinates.\n        # Voxel51 expects (x,y,w,h) relative (fractional) coordinates.\n        # So we need to do a conversion.\n        fo_box = [\n            box[0] / img.width,\n            box[1] / img.height,\n            (box[2] - box[0]) / img.width,\n            (box[3] - box[1]) / img.height,\n        ]\n        result.append({'label': label, 'bounding_box': fo_box, 'confidence': score})\n    return result",
              "output": null,
              "explanation": ""
            },
            {
              "code": "t.select(\n    t.image,\n    t.classifications,\n    vit_to_fo(t.classifications),\n    t.detections,\n    detr_to_fo(t.image, t.detections)\n).head()",
              "output": "                                               image  \\\n0  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n1  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n2  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n3  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n\n                                     classifications  \\\n0  {'labels': [345, 690, 912, 346, 730], 'scores'...   \n1  {'labels': [340, 353, 386, 9, 352], 'scores': ...   \n2  {'labels': [883, 738, 708, 725, 716], 'scores'...   \n3  {'labels': [340, 353, 386, 352, 9], 'scores': ...   \n\n                                           vit_to_fo  \\\n0  [{'label': 'ox', 'confidence': 0.7670010328292...   \n1  [{'label': 'zebra', 'confidence': 0.3253521025...   \n2  [{'label': 'vase', 'confidence': 0.63610780239...   \n3  [{'label': 'zebra', 'confidence': 0.9953896999...   \n\n                                          detections  \\\n0  {'boxes': [[335.8553771972656, 43.413597106933...   \n1  {'boxes': [[51.96046829223633, 356.18701171875...   \n2  {'boxes': [[238.07403564453125, 155.5013275146...   \n3  {'boxes': [[-0.230712890625, 19.50192070007324...   \n\n                                          detr_to_fo  \n0  [{'label': 'person', 'bounding_box': [0.524774...  \n1  [{'label': 'giraffe', 'bounding_box': [0.08118...  \n2  [{'label': 'vase', 'bounding_box': [0.37199068...  \n3  [{'label': 'zebra', 'bounding_box': [-0.000360...  ",
              "explanation": ""
            },
            {
              "code": "fo_dataset = pxt.io.export_images_as_fo_dataset(\n    t,\n    t.image,\n    classifications=vit_to_fo(t.classifications),\n    detections=detr_to_fo(t.image, t.detections)\n)\nsession = fo.launch_app(fo_dataset)",
              "output": "",
              "explanation": ""
            },
            {
              "code": "t.add_computed_column(detections_101=detr_for_object_detection(\n    t.image, model_id='facebook/detr-resnet-101'\n))",
              "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:01<00:00,  2.90 cells/s]\nAdded 4 column values with 0 errors.\n",
              "explanation": ""
            },
            {
              "code": "fo_dataset = pxt.io.export_images_as_fo_dataset(\n    t,\n    t.image,\n    classifications=vit_to_fo(t.classifications),\n    detections={\n        'detections_50': detr_to_fo(t.image, t.detections),\n        'detections_101': detr_to_fo(t.image, t.detections_101)\n    }\n)\nsession = fo.launch_app(fo_dataset)",
              "output": "",
              "explanation": ""
            }
          ]
        }
      ],
      "api_usage": {},
      "key_concepts": [
        "computed column",
        "multimodal",
        "udf"
      ],
      "code_snippets": [
        {
          "code": "%pip install -qU pixeltable fiftyone torch transformers",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-fiftyone.ipynb)\n[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-fiftyone.ipynb)\n\n# Working with Voxel51 in Pixeltable\nPixeltable can export data directly from tables and views to the popular [Voxel51](https://voxel51.com/) frontend, providing a way to visualize and explore image and video datasets. In this tutorial, we'll learn how to:\n- Export data from Pixeltable to Voxel51\n- Apply labels from image classification and object detection models to exported data\n\nWe begin by installing the necessary libraries for this tutorial.",
          "output": null
        }
      ]
    },
    {
      "notebook": "integrations/working-with-gemini.ipynb",
      "title": "Working with Gemini in Pixeltable",
      "description": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-gemini.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-gemini.ipynb) Pixeltable's Gemini integration enables you to access the Gemini LLM via the Google Gemini API. - A Google AI Studio account with an API key (https://aistudio.google.com/app/apikey)",
      "workflows": [],
      "api_usage": {},
      "key_concepts": [
        "computed column",
        "multimodal"
      ],
      "code_snippets": [
        {
          "code": "%pip install -qU pixeltable google-genai",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-gemini.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-gemini.ipynb)\n# Working with Gemini in Pixeltable\n\nPixeltable's Gemini integration enables you to access the Gemini LLM via the Google Gemini API.\n\n### Prerequisites\n\n- A Google AI Studio account with an API key (https://aistudio.google.com/app/apikey)\n\n### Important Notes\n\n- Google AI Studio usage may incur costs based on your plan.\n- Be mindful of sensitive data and consider security measures when integrating with external services.\n\nFirst you'll need to install required libraries and enter a Gemini API key obtained via Google AI Studio.",
          "output": null
        },
        {
          "code": "import os\nimport getpass\n\nif 'GEMINI_API_KEY' not in os.environ:\n    os.environ['GEMINI_API_KEY'] = getpass.getpass('Google AI Studio API Key:')",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-gemini.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-gemini.ipynb)\n# Working with Gemini in Pixeltable\n\nPixeltable's Gemini integration enables you to access the Gemini LLM via the Google Gemini API.\n\n### Prerequisites\n\n- A Google AI Studio account with an API key (https://aistudio.google.com/app/apikey)\n\n### Important Notes\n\n- Google AI Studio usage may incur costs based on your plan.\n- Be mindful of sensitive data and consider security measures when integrating with external services.\n\nFirst you'll need to install required libraries and enter a Gemini API key obtained via Google AI Studio.",
          "output": null
        },
        {
          "code": "import pixeltable as pxt\n\n# Remove the 'gemini_demo' directory and its contents, if it exists\npxt.drop_dir('gemini_demo', force=True)\npxt.create_dir('gemini_demo')",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-gemini.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-gemini.ipynb)\n# Working with Gemini in Pixeltable\n\nPixeltable's Gemini integration enables you to access the Gemini LLM via the Google Gemini API.\n\n### Prerequisites\n\n- A Google AI Studio account with an API key (https://aistudio.google.com/app/apikey)\n\n### Important Notes\n\n- Google AI Studio usage may incur costs based on your plan.\n- Be mindful of sensitive data and consider security measures when integrating with external services.\n\nFirst you'll need to install required libraries and enter a Gemini API key obtained via Google AI Studio.\nNow let's create a Pixeltable directory to hold the tables for our demo.",
          "output": "Connected to Pixeltable database at: postgresql+psycopg://postgres:@/pixeltable?host=/Users/asiegel/.pixeltable/pgdata\nCreated directory 'gemini_demo'.\n"
        },
        {
          "code": "from google.genai.types import GenerateContentConfigDict\nfrom pixeltable.functions import gemini\n\n# Create a table in Pixeltable and pick a model hosted on Google AI Studio with some parameters\n\nt = pxt.create_table('gemini_demo.text', {'input': pxt.String})\n\nconfig = GenerateContentConfigDict(\n    stop_sequences=['\\n'],\n    max_output_tokens=300,\n    temperature=1.0,\n    top_p=0.95,\n    top_k=40,\n)\nt.add_computed_column(output=gemini.generate_content(\n    t.input,\n    model='gemini-2.0-flash',\n    config=config\n))",
          "context": "Now let's create a Pixeltable directory to hold the tables for our demo.\n## Generate Content\n\nCreate a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from Gemini.",
          "output": "Created table `text`.\nAdded 0 column values with 0 errors.\n"
        },
        {
          "code": "# Ask Gemini to generate some content based on the input\nt.insert([\n    {'input': 'Write a story about a magic backpack.'}, \n    {'input': 'Tell me a science joke.'}\n])",
          "context": "Now let's create a Pixeltable directory to hold the tables for our demo.\n## Generate Content\n\nCreate a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from Gemini.",
          "output": "Inserting rows into `text`: 2 rows [00:00, 176.84 rows/s]\nInserted 2 rows with 0 errors.\n"
        },
        {
          "code": "# Parse the response into a new column\nt.add_computed_column(response=t.output['candidates'][0]['content']['parts'][0]['text'])\nt.select(t.input, t.response).head()",
          "context": "Now let's create a Pixeltable directory to hold the tables for our demo.\n## Generate Content\n\nCreate a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from Gemini.",
          "output": "Added 2 column values with 0 errors.\n"
        },
        {
          "code": "from google.genai.types import GenerateImagesConfigDict\n\nimages_t = pxt.create_table('gemini_demo.images', {'prompt': pxt.String})\n\nconfig = GenerateImagesConfigDict(aspect_ratio='16:9')\nimages_t.add_computed_column(generated_image=gemini.generate_images(\n    images_t.prompt,\n    model='imagen-3.0-generate-002',\n    config=config\n))",
          "context": "## Generate Content\n\nCreate a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from Gemini.\n## Generate Images with Imagen",
          "output": "Created table `images`.\nAdded 0 column values with 0 errors.\n"
        },
        {
          "code": "images_t.insert([{'prompt': 'A friendly dinosaur playing tennis in a cornfield'}])",
          "context": "## Generate Content\n\nCreate a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from Gemini.\n## Generate Images with Imagen",
          "output": "Inserting rows into `images`: 1 rows [00:00, 382.10 rows/s]\nInserted 1 row with 0 errors.\n"
        },
        {
          "code": "images_t.head()",
          "context": "## Generate Content\n\nCreate a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from Gemini.\n## Generate Images with Imagen",
          "output": "                                              prompt  \\\n0  A friendly dinosaur playing tennis in a cornfield   \n\n                                     generated_image  \n0  <PIL.JpegImagePlugin.JpegImageFile image mode=...  "
        },
        {
          "code": "videos_t = pxt.create_table('gemini_demo.videos', {'prompt': pxt.String})\n\nvideos_t.add_computed_column(generated_video=gemini.generate_videos(\n    videos_t.prompt,\n    model='veo-2.0-generate-001',\n))  ",
          "context": "## Generate Images with Imagen\n## Generate Video with Veo",
          "output": "Created table `videos`.\nAdded 0 column values with 0 errors.\n"
        },
        {
          "code": "videos_t.insert([{'prompt': 'A giant pixel floating over the open ocean in a sea of data'}])",
          "context": "## Generate Images with Imagen\n## Generate Video with Veo",
          "output": "Inserting rows into `videos`: 1 rows [00:00, 65.14 rows/s]\nInserted 1 row with 0 errors.\n"
        },
        {
          "code": "videos_t.head()",
          "context": "## Generate Images with Imagen\n## Generate Video with Veo",
          "output": "                                              prompt  \\\n0  A giant pixel floating over the open ocean in ...   \n\n                                     generated_video  \n0  /Users/asiegel/.pixeltable/media/b941f34bd2e14...  "
        },
        {
          "code": "images_t.add_computed_column(generated_video=gemini.generate_videos(\n    image=images_t.generated_image,\n    model='veo-2.0-generate-001',\n))",
          "context": "## Generate Video from an existing Image\nWe'll add an additional computed column to our existing `images_t` to animate the generated images.",
          "output": "Added 1 column value with 0 errors.\n"
        },
        {
          "code": "images_t.head()",
          "context": "## Generate Video from an existing Image\nWe'll add an additional computed column to our existing `images_t` to animate the generated images.",
          "output": "                                              prompt  \\\n0  A friendly dinosaur playing tennis in a cornfield   \n\n                                     generated_image  \\\n0  <PIL.JpegImagePlugin.JpegImageFile image mode=...   \n\n                                     generated_video  \n0  /Users/asiegel/.pixeltable/media/f2cd9b2e9b2e4...  "
        }
      ]
    },
    {
      "notebook": "integrations/working-with-mistralai.ipynb",
      "title": "Working with Mistral AI in Pixeltable",
      "description": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-mistralai.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-mistralai.ipynb) Pixeltable's Mistral AI integration enables you to access Mistral's LLM and other models via the Mistral AI API. - A Mistral AI account with an API key (https://console.mistral.ai/api-keys/)",
      "workflows": [],
      "api_usage": {},
      "key_concepts": [
        "computed column",
        "multimodal"
      ],
      "code_snippets": [
        {
          "code": "%pip install -qU pixeltable mistralai",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-mistralai.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-mistralai.ipynb)\n# Working with Mistral AI in Pixeltable\n\nPixeltable's Mistral AI integration enables you to access Mistral's LLM and other models via the Mistral AI API.\n\n### Prerequisites\n\n- A Mistral AI account with an API key (https://console.mistral.ai/api-keys/)\n\n### Important Notes\n\n- Mistral AI usage may incur costs based on your Mistral AI plan.\n- Be mindful of sensitive data and consider security measures when integrating with external services.\n\nFirst you'll need to install required libraries and enter a Mistral AI API key.",
          "output": null
        },
        {
          "code": "import os\nimport getpass\n\nif 'MISTRAL_API_KEY' not in os.environ:\n    os.environ['MISTRAL_API_KEY'] = getpass.getpass('Mistral AI API Key:')",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-mistralai.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-mistralai.ipynb)\n# Working with Mistral AI in Pixeltable\n\nPixeltable's Mistral AI integration enables you to access Mistral's LLM and other models via the Mistral AI API.\n\n### Prerequisites\n\n- A Mistral AI account with an API key (https://console.mistral.ai/api-keys/)\n\n### Important Notes\n\n- Mistral AI usage may incur costs based on your Mistral AI plan.\n- Be mindful of sensitive data and consider security measures when integrating with external services.\n\nFirst you'll need to install required libraries and enter a Mistral AI API key.",
          "output": null
        },
        {
          "code": "import pixeltable as pxt\n\n# Remove the 'mistralai_demo' directory and its contents, if it exists\npxt.drop_dir('mistralai_demo', force=True)\npxt.create_dir('mistralai_demo')",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-mistralai.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-mistralai.ipynb)\n# Working with Mistral AI in Pixeltable\n\nPixeltable's Mistral AI integration enables you to access Mistral's LLM and other models via the Mistral AI API.\n\n### Prerequisites\n\n- A Mistral AI account with an API key (https://console.mistral.ai/api-keys/)\n\n### Important Notes\n\n- Mistral AI usage may incur costs based on your Mistral AI plan.\n- Be mindful of sensitive data and consider security measures when integrating with external services.\n\nFirst you'll need to install required libraries and enter a Mistral AI API key.\nNow let's create a Pixeltable directory to hold the tables for our demo.",
          "output": "Connected to Pixeltable database at: postgresql+psycopg://postgres:@/pixeltable?host=/Users/asiegel/.pixeltable/pgdata\nCreated directory `mistralai_demo`.\n"
        },
        {
          "code": "from pixeltable.functions.mistralai import chat_completions\n\n# Create a table in Pixeltable and add a computed column that calls Mistral AI\n\nt = pxt.create_table('mistralai_demo.chat', {'input': pxt.String})\n\nmessages = [{'role': 'user', 'content': t.input}]\nt.add_computed_column(output=chat_completions(\n    messages=messages,\n    model='mistral-small-latest',\n    model_kwargs={\n        # Optional dict with parameters for the Mistral API\n        'max_tokens': 300,\n        'top_p': 0.9,\n        'temperature': 0.7\n    }\n))",
          "context": "Now let's create a Pixeltable directory to hold the tables for our demo.\n## Messages\n\nCreate a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from Mistral.",
          "output": "Created table `chat`.\nAdded 0 column values with 0 errors.\n"
        },
        {
          "code": "# Parse the response into a new column\nt.add_computed_column(response=t.output.choices[0].message.content)",
          "context": "Now let's create a Pixeltable directory to hold the tables for our demo.\n## Messages\n\nCreate a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from Mistral.",
          "output": "Added 0 column values with 0 errors.\n"
        },
        {
          "code": "# Start a conversation\nt.insert(input=\"What three species of fish have the highest mercury content?\")\nt.select(t.input, t.response).show()",
          "context": "Now let's create a Pixeltable directory to hold the tables for our demo.\n## Messages\n\nCreate a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from Mistral.",
          "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:02<00:00,  1.26 cells/s]\nInserting rows into `chat`: 1 rows [00:00, 134.12 rows/s]\nComputing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:02<00:00,  1.26 cells/s]\nInserted 1 row with 0 errors.\n"
        }
      ]
    },
    {
      "notebook": "integrations/working-with-together.ipynb",
      "title": "Working with Together AI in Pixeltable",
      "description": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-together.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-together.ipynb) - A Together AI account with an API key (https://api.together.ai/settings/api-keys) - Together.ai usage may incur costs based on your Together.ai plan.",
      "workflows": [],
      "api_usage": {},
      "key_concepts": [
        "computed column",
        "multimodal",
        "embedding"
      ],
      "code_snippets": [
        {
          "code": "%pip install -qU pixeltable together",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-together.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-together.ipynb)\n# Working with Together AI in Pixeltable\n\n### Prerequisites\n- A Together AI account with an API key (https://api.together.ai/settings/api-keys)\n\n### Important Notes\n\n- Together.ai usage may incur costs based on your Together.ai plan.\n- Be mindful of sensitive data and consider security measures when integrating with external services.\n\nFirst you'll need to install required libraries and enter your Together API key.",
          "output": null
        },
        {
          "code": "import os\nimport getpass\n\nif 'TOGETHER_API_KEY' not in os.environ:\n    os.environ['TOGETHER_API_KEY'] = getpass.getpass('Together API Key: ')",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-together.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-together.ipynb)\n# Working with Together AI in Pixeltable\n\n### Prerequisites\n- A Together AI account with an API key (https://api.together.ai/settings/api-keys)\n\n### Important Notes\n\n- Together.ai usage may incur costs based on your Together.ai plan.\n- Be mindful of sensitive data and consider security measures when integrating with external services.\n\nFirst you'll need to install required libraries and enter your Together API key.",
          "output": null
        },
        {
          "code": "import pixeltable as pxt\n\n# Remove the 'together_demo' directory and its contents, if it exists\npxt.drop_dir('together_demo', force=True)\npxt.create_dir('together_demo')",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-together.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-together.ipynb)\n# Working with Together AI in Pixeltable\n\n### Prerequisites\n- A Together AI account with an API key (https://api.together.ai/settings/api-keys)\n\n### Important Notes\n\n- Together.ai usage may incur costs based on your Together.ai plan.\n- Be mindful of sensitive data and consider security measures when integrating with external services.\n\nFirst you'll need to install required libraries and enter your Together API key.\nNow let's create a Pixeltable directory to hold the tables for our demo.",
          "output": "Connected to Pixeltable database at: postgresql+psycopg://postgres:@/pixeltable?host=/Users/asiegel/.pixeltable/pgdata\nCreated directory 'together_demo'.\n"
        },
        {
          "code": "from pixeltable.functions import together\n\nchat_t = pxt.create_table('together_demo.chat', {'input': pxt.String})\n\nmessages = [{'role': 'user', 'content': chat_t.input}]\n\nchat_t.add_computed_column(output=together.chat_completions(\n    messages=messages,\n    model='meta-llama/Meta-Llama-3.1-8B-Instruct-Turbo',\n    model_kwargs={\n        # Optional dict with parameters for the Together API\n        'max_tokens': 300,\n        'stop': ['\\n'],\n        'temperature': 0.7,\n        'top_p': 0.9,\n    }\n))\nchat_t.add_computed_column(response=chat_t.output.choices[0].message.content)",
          "context": "Now let's create a Pixeltable directory to hold the tables for our demo.\n## Chat Completions\n\nCreate a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from OpenAI.",
          "output": "Created table `chat`.\nAdded 0 column values with 0 errors.\nAdded 0 column values with 0 errors.\n"
        },
        {
          "code": "# Start a conversation\nchat_t.insert([\n    {'input': 'How many species of felids have been classified?'},\n    {'input': 'Can you make me a coffee?'}\n])\nchat_t.select(chat_t.input, chat_t.response).head()",
          "context": "Now let's create a Pixeltable directory to hold the tables for our demo.\n## Chat Completions\n\nCreate a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from OpenAI.",
          "output": "Inserting rows into `chat`: 2 rows [00:00, 221.12 rows/s]\nInserted 2 rows with 0 errors.\n"
        },
        {
          "code": "emb_t = pxt.create_table('together_demo.embeddings', {'input': pxt.String})\nemb_t.add_computed_column(embedding=together.embeddings(\n    input=emb_t.input,\n    model='BAAI/bge-base-en-v1.5'\n))",
          "context": "## Chat Completions\n\nCreate a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from OpenAI.\n## Embeddings",
          "output": "Created table `embeddings`.\nAdded 0 column values with 0 errors.\n"
        },
        {
          "code": "emb_t.insert(input='Together AI provides a variety of embeddings models.')",
          "context": "## Chat Completions\n\nCreate a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from OpenAI.\n## Embeddings",
          "output": "Inserting rows into `embeddings`: 1 rows [00:00, 135.03 rows/s]\nInserted 1 row with 0 errors.\n"
        },
        {
          "code": "emb_t.head()",
          "context": "## Chat Completions\n\nCreate a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from OpenAI.\n## Embeddings",
          "output": "                                               input  \\\n0  Together AI provides a variety of embeddings m...   \n\n                                           embedding  \n0  [0.016232446, -0.2097417, 0.20096539, 0.153079...  "
        },
        {
          "code": "image_t = pxt.create_table('together_demo.images', {'input': pxt.String})\nimage_t.add_computed_column(img=together.image_generations(\n    image_t.input,\n    model='black-forest-labs/FLUX.1-schnell',\n    model_kwargs={'steps': 5}\n))",
          "context": "## Embeddings\n## Image Generations",
          "output": "Created table `images`.\nAdded 0 column values with 0 errors.\n"
        },
        {
          "code": "image_t.insert([\n    {'input': 'A friendly dinosaur playing tennis in a cornfield'}\n])",
          "context": "## Embeddings\n## Image Generations",
          "output": "Inserting rows into `images`: 1 rows [00:00, 204.46 rows/s]\nInserted 1 row with 0 errors.\n"
        },
        {
          "code": "image_t",
          "context": "## Embeddings\n## Image Generations",
          "output": "Table 'together_demo.images'\n\n Column Name    Type                                      Computed With\n       input  String                                                   \n         img   Image  image_generations(input, model='black-forest-l..."
        },
        {
          "code": "image_t.head()",
          "context": "## Embeddings\n## Image Generations",
          "output": "                                               input  \\\n0  A friendly dinosaur playing tennis in a cornfield   \n\n                                                 img  \n0  <PIL.JpegImagePlugin.JpegImageFile image mode=...  "
        }
      ]
    },
    {
      "notebook": "integrations/working-with-deepseek.ipynb",
      "title": "Working with Deepseek in Pixeltable",
      "description": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-deepseek.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-deepseek.ipynb) Pixeltable's Deepseek integration enables you to access Deepseek's LLM via the Deepseek API. - A Deepseek account with an API key (https://api-docs.deepseek.com/)",
      "workflows": [],
      "api_usage": {},
      "key_concepts": [
        "computed column",
        "multimodal"
      ],
      "code_snippets": [
        {
          "code": "%pip install -qU pixeltable openai",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-deepseek.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-deepseek.ipynb)\n# Working with Deepseek in Pixeltable\n\nPixeltable's Deepseek integration enables you to access Deepseek's LLM via the Deepseek API.\n\n### Prerequisites\n\n- A Deepseek account with an API key (https://api-docs.deepseek.com/)\n\n### Important Notes\n\n- Deepseek usage may incur costs based on your Deepseek plan.\n- Be mindful of sensitive data and consider security measures when integrating with external services.\n\nFirst you'll need to install the required libraries and enter a Deepseek API key. Deepseek uses the OpenAI SDK as its Python API, so we need to install it in addition to Pixeltable.",
          "output": null
        },
        {
          "code": "import os\nimport getpass\n\nif 'DEEPSEEK_API_KEY' not in os.environ:\n    os.environ['DEEPSEEK_API_KEY'] = getpass.getpass('Deepseek API Key:')",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-deepseek.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-deepseek.ipynb)\n# Working with Deepseek in Pixeltable\n\nPixeltable's Deepseek integration enables you to access Deepseek's LLM via the Deepseek API.\n\n### Prerequisites\n\n- A Deepseek account with an API key (https://api-docs.deepseek.com/)\n\n### Important Notes\n\n- Deepseek usage may incur costs based on your Deepseek plan.\n- Be mindful of sensitive data and consider security measures when integrating with external services.\n\nFirst you'll need to install the required libraries and enter a Deepseek API key. Deepseek uses the OpenAI SDK as its Python API, so we need to install it in addition to Pixeltable.",
          "output": null
        },
        {
          "code": "import pixeltable as pxt\n\n# Remove the 'deepseek_demo' directory and its contents, if it exists\npxt.drop_dir('deepseek_demo', force=True)\npxt.create_dir('deepseek_demo')",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-deepseek.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-deepseek.ipynb)\n# Working with Deepseek in Pixeltable\n\nPixeltable's Deepseek integration enables you to access Deepseek's LLM via the Deepseek API.\n\n### Prerequisites\n\n- A Deepseek account with an API key (https://api-docs.deepseek.com/)\n\n### Important Notes\n\n- Deepseek usage may incur costs based on your Deepseek plan.\n- Be mindful of sensitive data and consider security measures when integrating with external services.\n\nFirst you'll need to install the required libraries and enter a Deepseek API key. Deepseek uses the OpenAI SDK as its Python API, so we need to install it in addition to Pixeltable.\nNow let's create a Pixeltable directory to hold the tables for our demo.",
          "output": "Connected to Pixeltable database at: postgresql+psycopg://postgres:@/pixeltable?host=/Users/asiegel/.pixeltable/pgdata\nCreated directory 'deepseek_demo'.\n"
        },
        {
          "code": "from pixeltable.functions import deepseek\n\n# Create a table in Pixeltable and add a computed column that calls Deepseek\n\nt = pxt.create_table('deepseek_demo.chat', {'input': pxt.String})\n\nmsgs = [{'role': 'user', 'content': t.input}]\nt.add_computed_column(output=deepseek.chat_completions(\n    messages=msgs,\n    model='deepseek-chat',\n))",
          "context": "Now let's create a Pixeltable directory to hold the tables for our demo.\n## Messages\n\nCreate a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from Deepseek.",
          "output": "Created table `chat`.\nAdded 0 column values with 0 errors.\n"
        },
        {
          "code": "# Parse the response into a new column\nt.add_computed_column(response=t.output.choices[0].message.content)",
          "context": "Now let's create a Pixeltable directory to hold the tables for our demo.\n## Messages\n\nCreate a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from Deepseek.",
          "output": "Added 0 column values with 0 errors.\n"
        },
        {
          "code": "# Start a conversation\nt.insert(input=\"What was the outcome of the 1904 US Presidential election?\")\nt.select(t.input, t.response).show()",
          "context": "Now let's create a Pixeltable directory to hold the tables for our demo.\n## Messages\n\nCreate a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from Deepseek.",
          "output": "Inserting rows into `chat`: 1 rows [00:00, 105.11 rows/s]\nInserted 1 row with 0 errors.\n"
        }
      ]
    },
    {
      "notebook": "integrations/working-with-groq.ipynb",
      "title": "Working with Groq in Pixeltable",
      "description": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-groq.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-groq.ipynb) Pixeltable's Groq integration enables you to access Groq models via the Groq API. - A Groq account with an API key (https://console.groq.com/docs/quickstart)",
      "workflows": [],
      "api_usage": {},
      "key_concepts": [
        "computed column",
        "multimodal"
      ],
      "code_snippets": [
        {
          "code": "%pip install -qU pixeltable groq",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-groq.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-groq.ipynb)\n# Working with Groq in Pixeltable\n\nPixeltable's Groq integration enables you to access Groq models via the Groq API.\n\n### Prerequisites\n- A Groq account with an API key (https://console.groq.com/docs/quickstart)\n\n### Important Notes\n\n- Groq usage may incur costs based on your Groq plan.\n- Be mindful of sensitive data and consider security measures when integrating with external services.\nFirst you'll need to install required libraries and enter your OpenAI API key.",
          "output": null
        },
        {
          "code": "import os\nimport getpass\nif 'GROQ_API_KEY' not in os.environ:\n    os.environ['GROQ_API_KEY'] = getpass.getpass('Enter your Groq API key:')",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-groq.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-groq.ipynb)\n# Working with Groq in Pixeltable\n\nPixeltable's Groq integration enables you to access Groq models via the Groq API.\n\n### Prerequisites\n- A Groq account with an API key (https://console.groq.com/docs/quickstart)\n\n### Important Notes\n\n- Groq usage may incur costs based on your Groq plan.\n- Be mindful of sensitive data and consider security measures when integrating with external services.\nFirst you'll need to install required libraries and enter your OpenAI API key.",
          "output": null
        },
        {
          "code": "import pixeltable as pxt\n\n# Remove the 'groq_demo' directory and its contents, if it exists\npxt.drop_dir('groq_demo', force=True)\npxt.create_dir('groq_demo')",
          "context": "First you'll need to install required libraries and enter your OpenAI API key.\nNow let's create a Pixeltable directory to hold the tables for our demo.",
          "output": "Connected to Pixeltable database at: postgresql+psycopg://postgres:@/pixeltable?host=/Users/asiegel/.pixeltable/pgdata\nCreated directory 'groq_demo'.\n"
        },
        {
          "code": "from pixeltable.functions import groq\n\n# Create a table in Pixeltable and add a computed column that calls OpenAI\n\nt = pxt.create_table('groq_demo.chat', {'input': pxt.String})\n\nmessages = [{'role': 'user', 'content': t.input}]\nt.add_computed_column(output=groq.chat_completions(\n    messages=messages,\n    model='llama3-8b-8192',\n    model_kwargs={\n        # Optional dict with parameters for the Groq API\n        'max_tokens': 300,\n        'top_p': 0.9,\n        'temperature': 0.7\n    }\n))",
          "context": "Now let's create a Pixeltable directory to hold the tables for our demo.\n## Chat Completions\n\nCreate a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from Groq.",
          "output": "Created table `chat`.\nAdded 0 column values with 0 errors.\n"
        },
        {
          "code": "# Parse the response into a new column\nt.add_computed_column(response=t.output.choices[0].message.content)",
          "context": "Now let's create a Pixeltable directory to hold the tables for our demo.\n## Chat Completions\n\nCreate a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from Groq.",
          "output": "Added 0 column values with 0 errors.\n"
        },
        {
          "code": "# Start a conversation\nt.insert(input=\"How many islands are in the Aleutian island chain?\")\nt.select(t.input, t.response).head()",
          "context": "Now let's create a Pixeltable directory to hold the tables for our demo.\n## Chat Completions\n\nCreate a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from Groq.",
          "output": "Inserting rows into `chat`: 1 rows [00:00, 76.95 rows/s]\nInserted 1 row with 0 errors.\n"
        }
      ]
    },
    {
      "notebook": "integrations/working-with-ollama.ipynb",
      "title": "Working with Ollama in Pixeltable",
      "description": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-ollama.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-ollama.ipynb) Ollama is a popular platform for local serving of LLMs. In this tutorial, we'll show how to integrate Ollama models into a Pixeltable workflow. You'll need to have an Ollama server instance to query. There are several ways to do this.",
      "workflows": [],
      "api_usage": {},
      "key_concepts": [
        "incremental",
        "computed column",
        "multimodal"
      ],
      "code_snippets": [
        {
          "code": "# To install Ollama on colab, uncomment and run the following\n# three lines (this will also work on a local Linux machine\n# if you don't already have Ollama installed).\n\n# !curl -fsSL https://ollama.com/install.sh | sh\n# import subprocess\n# ollama_process = subprocess.Popen(['ollama', 'serve'], stderr=subprocess.PIPE)",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-ollama.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-ollama.ipynb)\n# Working with Ollama in Pixeltable\n\nOllama is a popular platform for local serving of LLMs. In this tutorial, we'll show how to integrate Ollama models into a Pixeltable workflow.\n\n## Install Ollama\n\nYou'll need to have an Ollama server instance to query. There are several ways to do this.\n\n### Running on a Local Machine\n\nIf you're running this notebook on your own machine, running Windows, Mac OS, or Linux, you can install Ollama at: https://ollama.com/download \n\n### Running on Google Colab\n\n- OR, if you're running on Colab, you can install Ollama by uncommenting and running the following code.",
          "output": null
        },
        {
          "code": "# To run the notebook against an instance of Ollama running on a\n# remote server, uncomment the following line and specify the URL.\n\n# os.environs['OLLAMA_HOST'] = 'https://127.0.0.1:11434'",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-ollama.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-ollama.ipynb)\n# Working with Ollama in Pixeltable\n\nOllama is a popular platform for local serving of LLMs. In this tutorial, we'll show how to integrate Ollama models into a Pixeltable workflow.\n\n## Install Ollama\n\nYou'll need to have an Ollama server instance to query. There are several ways to do this.\n\n### Running on a Local Machine\n\nIf you're running this notebook on your own machine, running Windows, Mac OS, or Linux, you can install Ollama at: https://ollama.com/download \n\n### Running on Google Colab\n\n- OR, if you're running on Colab, you can install Ollama by uncommenting and running the following code.\n### Running on a remote Ollama server\n\n- OR, if you have access to an Ollama server running remotely, you can uncomment and run the following line, replacing the default URL with the URL of your remote Ollama instance.",
          "output": null
        },
        {
          "code": "%pip install -qU ollama",
          "context": "### Running on a remote Ollama server\n\n- OR, if you have access to an Ollama server running remotely, you can uncomment and run the following line, replacing the default URL with the URL of your remote Ollama instance.\nOnce you've completed the installation, run the following commands to verify that it's been successfully installed. This may result in an LLM being downloaded, so it may take some time.",
          "output": null
        },
        {
          "code": "import ollama\n\nollama.pull('qwen2.5:0.5b')\nollama.generate('qwen2.5:0.5b', 'What is the capital of Missouri?')['response']",
          "context": "### Running on a remote Ollama server\n\n- OR, if you have access to an Ollama server running remotely, you can uncomment and run the following line, replacing the default URL with the URL of your remote Ollama instance.\nOnce you've completed the installation, run the following commands to verify that it's been successfully installed. This may result in an LLM being downloaded, so it may take some time.",
          "output": "\"The capital city of Missouri is Jefferson City. It's located in the central part of the state and serves as the administrative center for the Midwestern U.S. territory of Missouri. The state is known for its rich history, particularly regarding the Missouri River, which runs through its central parts and provides access to major cities along the border with Illinois.\""
        },
        {
          "code": "%pip install -qU pixeltable",
          "context": "## Install Pixeltable\nNow, let's install Pixeltable and create a table for the demo.",
          "output": null
        },
        {
          "code": "import pixeltable as pxt\nfrom pixeltable.functions.ollama import chat\n\npxt.drop_dir('ollama_demo', force=True)\npxt.create_dir('ollama_demo')\nt = pxt.create_table('ollama_demo.chat', {'input': pxt.String})\n\nmessages = [{'role': 'user', 'content': t.input}]\n\nt.add_computed_column(output=chat(\n    messages=messages,\n    model='qwen2.5:0.5b',\n    # These parameters are optional and can be used to tune model behavior:\n    options={'max_tokens': 300, 'top_p': 0.9, 'temperature': 0.5},\n))\n\n# Extract the response content into a separate column\n\nt.add_computed_column(response=t.output.message.content)",
          "context": "## Install Pixeltable\nNow, let's install Pixeltable and create a table for the demo.",
          "output": "Connected to Pixeltable database at: postgresql+psycopg://postgres:@/pixeltable?host=/Users/asiegel/.pixeltable/pgdata\nCreated directory `ollama_demo`.\nCreated table `chat`.\nAdded 0 column values with 0 errors.\nAdded 0 column values with 0 errors.\n"
        },
        {
          "code": "# Start a conversation\nt.insert(input='What are the most popular services for LLM inference?')\nt.select(t.input, t.response).show()",
          "context": "Now, let's install Pixeltable and create a table for the demo.\nWe can insert our input prompts into the table now. As always, Pixeltable automatically updates the computed columns by calling the relevant Ollama endpoint.",
          "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:02<00:00,  1.18 cells/s]\nInserting rows into `chat`: 1 rows [00:00, 75.39 rows/s]\nComputing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:02<00:00,  1.17 cells/s]\nInserted 1 row with 0 errors.\n"
        }
      ]
    },
    {
      "notebook": "integrations/working-with-llama-cpp.ipynb",
      "title": "Working with llama.cpp in Pixeltable",
      "description": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-llama-cpp.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-llama-cpp.ipynb) This tutorial demonstrates how to use Pixeltable's built-in `llama.cpp` integration to run local LLMs efficiently. **If you are running this tutorial in Colab:**",
      "workflows": [],
      "api_usage": {},
      "key_concepts": [
        "computed column",
        "multimodal",
        "iterator"
      ],
      "code_snippets": [
        {
          "code": "%pip install -qU pixeltable llama-cpp-python huggingface-hub",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-llama-cpp.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-llama-cpp.ipynb)\n# Working with llama.cpp in Pixeltable\n\nThis tutorial demonstrates how to use Pixeltable's built-in `llama.cpp` integration to run local LLMs efficiently.\n\n**If you are running this tutorial in Colab:**\nIn order to make the tutorial run a bit snappier, let's switch to a GPU-equipped instance for this Colab session. To do that, click on the `Runtime -> Change runtime type` menu item at the top, then select the `GPU` radio button and click on `Save`.\n\n### Important Notes\n\n- Models are automatically downloaded from Hugging Face and cached locally\n- Different quantization levels are available for performance/quality tradeoffs\n- Consider memory usage when choosing models and quantizations\n## Set Up Environment\n\nFirst, let's install Pixeltable with llama.cpp support:",
          "output": null
        },
        {
          "code": "import pixeltable as pxt\nfrom pixeltable.functions import llama_cpp\n\npxt.drop_dir('llama_demo', force=True)\npxt.create_dir('llama_demo')\n\nt = pxt.create_table('llama_demo.chat', {'input': pxt.String})",
          "context": "## Set Up Environment\n\nFirst, let's install Pixeltable with llama.cpp support:\n## Create a Table for Chat Completions\n\nNow let's create a table that will contain our inputs and responses.",
          "output": "Connected to Pixeltable database at: postgresql+psycopg://postgres:@/pixeltable?host=/Users/asiegel/.pixeltable/pgdata\nCreated directory `llama_demo`.\nCreated table `chat`.\n"
        },
        {
          "code": "# Add a computed column that uses llama.cpp for chat completion\n# against the input.\n\nmessages = [\n    {'role': 'system', 'content': 'You are a helpful assistant.'},\n    {'role': 'user', 'content': t.input}\n]\n\nt.add_computed_column(result=llama_cpp.create_chat_completion(\n    messages,\n    repo_id='Qwen/Qwen2.5-0.5B-Instruct-GGUF',\n    repo_filename='*q5_k_m.gguf'\n))\n\n# Extract the output content from the JSON structure returned\n# by llama_cpp.\n\nt.add_computed_column(output=t.result.choices[0].message.content)",
          "context": "## Create a Table for Chat Completions\n\nNow let's create a table that will contain our inputs and responses.\nNext, we add a computed column that calls the Pixeltable `create_chat_completion` UDF, which adapts the corresponding llama.cpp API call. In our examples, we'll use pretrained models from the Hugging Face repository. llama.cpp makes it easy to do this by specifying a repo_id (from the URL of the model) and filename from the model repo; the model will then be downloaded and cached automatically.\n\n(If this is your first time using Pixeltable, the <a href=\"https://docs.pixeltable.com/docs/tables-and-data-operations\">Pixeltable Fundamentals</a> tutorial contains more details about table creation, computed columns, and UDFs.)\n\nFor this demo we'll use `Qwen2.5-0.5B`, a very small (0.5-billion parameter) model that still produces decent results. We'll use `Q5_K_M` (5-bit) quantization, which gives an excellent balance of quality and efficiency.",
          "output": "Added 0 column values with 0 errors.\nAdded 0 column values with 0 errors.\n"
        },
        {
          "code": "# Test with a simple question\nt.insert([\n    {'input': 'What is the capital of France?'},\n    {'input': 'What are some edible species of fish?'},\n    {'input': 'Who are the most prominent classical composers?'}\n])",
          "context": "Next, we add a computed column that calls the Pixeltable `create_chat_completion` UDF, which adapts the corresponding llama.cpp API call. In our examples, we'll use pretrained models from the Hugging Face repository. llama.cpp makes it easy to do this by specifying a repo_id (from the URL of the model) and filename from the model repo; the model will then be downloaded and cached automatically.\n\n(If this is your first time using Pixeltable, the <a href=\"https://docs.pixeltable.com/docs/tables-and-data-operations\">Pixeltable Fundamentals</a> tutorial contains more details about table creation, computed columns, and UDFs.)\n\nFor this demo we'll use `Qwen2.5-0.5B`, a very small (0.5-billion parameter) model that still produces decent results. We'll use `Q5_K_M` (5-bit) quantization, which gives an excellent balance of quality and efficiency.\n## Test Chat Completion\n\nLet's try a simple query:",
          "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9/9 [00:03<00:00,  2.25 cells/s]\nInserting rows into `chat`: 3 rows [00:00, 1112.74 rows/s]\nComputing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 9/9 [00:03<00:00,  2.25 cells/s]\nInserted 3 rows with 0 errors.\n"
        },
        {
          "code": "t.select(t.input, t.output).collect()",
          "context": "Next, we add a computed column that calls the Pixeltable `create_chat_completion` UDF, which adapts the corresponding llama.cpp API call. In our examples, we'll use pretrained models from the Hugging Face repository. llama.cpp makes it easy to do this by specifying a repo_id (from the URL of the model) and filename from the model repo; the model will then be downloaded and cached automatically.\n\n(If this is your first time using Pixeltable, the <a href=\"https://docs.pixeltable.com/docs/tables-and-data-operations\">Pixeltable Fundamentals</a> tutorial contains more details about table creation, computed columns, and UDFs.)\n\nFor this demo we'll use `Qwen2.5-0.5B`, a very small (0.5-billion parameter) model that still produces decent results. We'll use `Q5_K_M` (5-bit) quantization, which gives an excellent balance of quality and efficiency.\n## Test Chat Completion\n\nLet's try a simple query:",
          "output": "                                             input  \\\n0                   What is the capital of France?   \n1            What are some edible species of fish?   \n2  Who are the most prominent classical composers?   \n\n                                              output  \n0                    Paris is the capital of France.  \n1  Here are some edible species of fish that you ...  \n2  The most prominent classical composers in the ...  "
        },
        {
          "code": "t.add_computed_column(result_l3=llama_cpp.create_chat_completion(\n    messages,\n    repo_id='bartowski/Llama-3.2-1B-Instruct-GGUF',\n    repo_filename='*Q5_K_M.gguf'\n))\n\nt.add_computed_column(output_l3=t.result_l3.choices[0].message.content)\n\nt.select(t.input, t.output, t.output_l3).collect()",
          "context": "## Test Chat Completion\n\nLet's try a simple query:\n## Comparing Models\n\nLocal model frameworks like `llama.cpp` make it easy to compare the output of different models. Let's try comparing the output from `Qwen` against a somewhat larger model, `Llama-3.2-1B`. As always, when we add a new computed column to our table, it's automatically evaluated against the existing table rows.",
          "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:08<00:00,  2.74s/ cells]\nAdded 3 column values with 0 errors.\nComputing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 349.89 cells/s]\nAdded 3 column values with 0 errors.\n"
        },
        {
          "code": "messages_teacher = [\n    {'role': 'system',\n     'content': 'You are a patient school teacher. '\n                'Explain concepts simply and clearly.'},\n    {'role': 'user', 'content': t.input}\n]\n\nt.add_computed_column(result_teacher=llama_cpp.create_chat_completion(\n    messages_teacher,\n    repo_id='bartowski/Llama-3.2-1B-Instruct-GGUF',\n    repo_filename='*Q5_K_M.gguf'\n))\n\nt.add_computed_column(output_teacher=t.result_teacher.choices[0].message.content)\n\nt.select(t.input, t.output_teacher).collect()",
          "context": "## Comparing Models\n\nLocal model frameworks like `llama.cpp` make it easy to compare the output of different models. Let's try comparing the output from `Qwen` against a somewhat larger model, `Llama-3.2-1B`. As always, when we add a new computed column to our table, it's automatically evaluated against the existing table rows.\nJust for fun, let's try running against a different system prompt with a different persona.",
          "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:06<00:00,  2.30s/ cells]\nAdded 3 column values with 0 errors.\nComputing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:00<00:00, 605.33 cells/s]\nAdded 3 column values with 0 errors.\n"
        }
      ]
    },
    {
      "notebook": "integrations/working-with-openai.ipynb",
      "title": "Working with OpenAI in Pixeltable",
      "description": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-openai.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-openai.ipynb) Pixeltable's OpenAI integration enables you to access OpenAI models via the OpenAI API. - An OpenAI account with an API key (https://openai.com/index/openai-api/)",
      "workflows": [],
      "api_usage": {},
      "key_concepts": [
        "computed column",
        "multimodal",
        "embedding"
      ],
      "code_snippets": [
        {
          "code": "%pip install -qU pixeltable openai",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-openai.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-openai.ipynb)\n# Working with OpenAI in Pixeltable\n\nPixeltable's OpenAI integration enables you to access OpenAI models via the OpenAI API.\n\n### Prerequisites\n- An OpenAI account with an API key (https://openai.com/index/openai-api/)\n\n### Important Notes\n\n- OpenAI usage may incur costs based on your OpenAI plan.\n- Be mindful of sensitive data and consider security measures when integrating with external services.\nFirst you'll need to install required libraries and enter your OpenAI API key.",
          "output": null
        },
        {
          "code": "import os\nimport getpass\nif 'OPENAI_API_KEY' not in os.environ:\n    os.environ['OPENAI_API_KEY'] = getpass.getpass('Enter your OpenAI API key:')",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-openai.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-openai.ipynb)\n# Working with OpenAI in Pixeltable\n\nPixeltable's OpenAI integration enables you to access OpenAI models via the OpenAI API.\n\n### Prerequisites\n- An OpenAI account with an API key (https://openai.com/index/openai-api/)\n\n### Important Notes\n\n- OpenAI usage may incur costs based on your OpenAI plan.\n- Be mindful of sensitive data and consider security measures when integrating with external services.\nFirst you'll need to install required libraries and enter your OpenAI API key.",
          "output": null
        },
        {
          "code": "import pixeltable as pxt\n\n# Remove the 'openai_demo' directory and its contents, if it exists\npxt.drop_dir('openai_demo', force=True)\npxt.create_dir('openai_demo')",
          "context": "First you'll need to install required libraries and enter your OpenAI API key.\nNow let's create a Pixeltable directory to hold the tables for our demo.",
          "output": "Connected to Pixeltable database at: postgresql+psycopg://postgres:@/pixeltable?host=/Users/asiegel/.pixeltable/pgdata\nCreated directory 'openai_demo'.\n"
        },
        {
          "code": "from pixeltable.functions import openai\n\n# Create a table in Pixeltable and add a computed column that calls OpenAI\n\nt = pxt.create_table('openai_demo.chat', {'input': pxt.String})\n\nmessages = [{'role': 'user', 'content': t.input}]\nt.add_computed_column(output=openai.chat_completions(\n    messages=messages,\n    model='gpt-4o-mini',\n    model_kwargs={\n        # Optional dict with parameters for the OpenAI API\n        'max_tokens': 300,\n        'top_p': 0.9,\n        'temperature': 0.7\n    }\n))",
          "context": "Now let's create a Pixeltable directory to hold the tables for our demo.\n## Chat Completions\n\nCreate a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from OpenAI.",
          "output": "Created table `chat`.\nAdded 0 column values with 0 errors.\n"
        },
        {
          "code": "# Parse the response into a new column\nt.add_computed_column(response=t.output.choices[0].message.content)",
          "context": "Now let's create a Pixeltable directory to hold the tables for our demo.\n## Chat Completions\n\nCreate a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from OpenAI.",
          "output": "Added 0 column values with 0 errors.\n"
        },
        {
          "code": "# Start a conversation\nt.insert(input=\"How many islands are in the Aleutian island chain?\")\nt.select(t.input, t.response).head()",
          "context": "Now let's create a Pixeltable directory to hold the tables for our demo.\n## Chat Completions\n\nCreate a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from OpenAI.",
          "output": "Inserting rows into `chat`: 1 rows [00:00, 106.84 rows/s]\nInserted 1 row with 0 errors.\n"
        },
        {
          "code": "emb_t = pxt.create_table('openai_demo.embeddings', {'input': pxt.String})\nemb_t.add_computed_column(embedding=openai.embeddings(\n    input=emb_t.input,\n    model='text-embedding-3-small'\n))",
          "context": "## Chat Completions\n\nCreate a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from OpenAI.\n## Embeddings",
          "output": "Created table `embeddings`.\nAdded 0 column values with 0 errors.\n"
        },
        {
          "code": "emb_t.insert(input='OpenAI provides a variety of embeddings models.')",
          "context": "## Chat Completions\n\nCreate a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from OpenAI.\n## Embeddings",
          "output": "Inserting rows into `embeddings`: 1 rows [00:00, 164.18 rows/s]\nInserted 1 row with 0 errors.\n"
        },
        {
          "code": "emb_t.head()",
          "context": "## Chat Completions\n\nCreate a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from OpenAI.\n## Embeddings",
          "output": "                                             input  \\\n0  OpenAI provides a variety of embeddings models.   \n\n                                           embedding  \n0  [-0.022908814, -0.044679735, 0.06892383, -0.01...  "
        },
        {
          "code": "image_t = pxt.create_table('openai_demo.images', {'input': pxt.String})\nimage_t.add_computed_column(img=openai.image_generations(\n    image_t.input,\n    model='dall-e-2',\n))",
          "context": "## Embeddings\n## Image Generations",
          "output": "Created table `images`.\nAdded 0 column values with 0 errors.\n"
        },
        {
          "code": "image_t.insert([\n    {'input': 'A giant Pixel floating in the open ocean in a sea of data'}\n])",
          "context": "## Embeddings\n## Image Generations",
          "output": "Inserting rows into `images`: 1 rows [00:00, 444.88 rows/s]\nInserted 1 row with 0 errors.\n"
        },
        {
          "code": "image_t",
          "context": "## Embeddings\n## Image Generations",
          "output": "Table 'openai_demo.images'\n\n Column Name                 Type                               Computed With\n       input               String                                            \n         img  Image[(1024, 1024)]  image_generations(input, model='dall-e-2')"
        },
        {
          "code": "image_t.head()",
          "context": "## Embeddings\n## Image Generations",
          "output": "                                               input  \\\n0  A giant Pixel floating in the open ocean in a ...   \n\n                                                 img  \n0  <PIL.JpegImagePlugin.JpegImageFile image mode=...  "
        },
        {
          "code": "audio_t = pxt.create_table('openai_demo.audio', {'input': pxt.Audio})\naudio_t.add_computed_column(result=openai.transcriptions(\n    audio_t.input,\n    model='whisper-1',\n    model_kwargs={\n        'language': 'en',\n        'prompt': 'Transcribe the contents of this recording.'\n    },\n))",
          "context": "## Image Generations\n## Audio Transcription",
          "output": "Created table `audio`.\nAdded 0 column values with 0 errors.\n"
        },
        {
          "code": "url = (\n    'https://github.com/pixeltable/pixeltable/raw/release/tests/data/audio/'\n    'jfk_1961_0109_cityuponahill-excerpt.flac'\n)\naudio_t.insert([{'input': url}])",
          "context": "## Image Generations\n## Audio Transcription",
          "output": "Inserting rows into `audio`: 1 rows [00:00, 160.69 rows/s]\nInserted 1 row with 0 errors.\n"
        },
        {
          "code": "audio_t.head()",
          "context": "## Image Generations\n## Audio Transcription",
          "output": "                                               input  \\\n0  /Users/asiegel/.pixeltable/file_cache/a3235626...   \n\n                                              result  \n0  {'text': 'Allow me to illustrate. During the l...  "
        },
        {
          "code": "audio_t.head()[0]['result']['text']",
          "context": "## Image Generations\n## Audio Transcription",
          "output": "'Allow me to illustrate. During the last 60 days, I have been at the task of constructing an administration. It has been a long and deliberate process. Some have counseled greater speed. Others have counseled more expedient tests. But I have been guided by the standard John Winthrop set before his shipmates on the flagship Arabella 331 years ago, as they too faced the task of building a new government on a perilous frontier. We must always consider, he said, that we shall be as a city upon a hill. The eyes of all peoples are upon us. Today, the eyes of all people are truly upon us. And our governments, in every branch, at every level,'"
        }
      ]
    },
    {
      "notebook": "integrations/working-with-bedrock.ipynb",
      "title": "Working with Bedrock in Pixeltable",
      "description": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-bedrock.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-bedrock.ipynb) Pixeltable's Bedrock integration enables you to access AWS Bedrock via the Bedrock API. - Activate Bedrock in your AWS account.",
      "workflows": [],
      "api_usage": {},
      "key_concepts": [
        "tool calling",
        "computed column",
        "multimodal",
        "udf"
      ],
      "code_snippets": [
        {
          "code": "%pip install -qU pixeltable boto3 duckduckgo-search",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-bedrock.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-bedrock.ipynb)\n# Working with Bedrock in Pixeltable\n\nPixeltable's Bedrock integration enables you to access AWS Bedrock via the Bedrock API.\n\n### Prerequisites\n\n- Activate Bedrock in your AWS account.\n- Request access to your desired models (e.g. Claude Sonnet 3.7, Amazon Nova Pro)\n- Optional - you may need to configure AWS CLI locally to authenticate with your AWS account.\n\n### Important Notes\n\n- Bedrock usage may incur costs based on your Bedrock plan.\n- Be mindful of sensitive data and consider security measures when integrating with external services.\n\nFirst you'll need to install required libraries and enter an Bedrock API key.",
          "output": null
        },
        {
          "code": "import os\nimport getpass\n\nif 'AWS_ACCESS_KEY' not in os.environ:\n    os.environ['AWS_ACCESS_KEY'] = getpass.getpass('Enter your AWS Access Key:')\n\nif 'AWS_SECRET_ACCESS_KEY' not in os.environ:\n    os.environ['AWS_SECRET_ACCESS_KEY'] = getpass.getpass('Enter your AWS Secret Access Key:')",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-bedrock.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-bedrock.ipynb)\n# Working with Bedrock in Pixeltable\n\nPixeltable's Bedrock integration enables you to access AWS Bedrock via the Bedrock API.\n\n### Prerequisites\n\n- Activate Bedrock in your AWS account.\n- Request access to your desired models (e.g. Claude Sonnet 3.7, Amazon Nova Pro)\n- Optional - you may need to configure AWS CLI locally to authenticate with your AWS account.\n\n### Important Notes\n\n- Bedrock usage may incur costs based on your Bedrock plan.\n- Be mindful of sensitive data and consider security measures when integrating with external services.\n\nFirst you'll need to install required libraries and enter an Bedrock API key.",
          "output": null
        },
        {
          "code": "import pixeltable as pxt\n\n# Remove the `bedrock_demo` directory and its contents, if it exists\npxt.drop_dir('bedrock_demo', force=True)\npxt.create_dir('bedrock_demo')",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-bedrock.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-bedrock.ipynb)\n# Working with Bedrock in Pixeltable\n\nPixeltable's Bedrock integration enables you to access AWS Bedrock via the Bedrock API.\n\n### Prerequisites\n\n- Activate Bedrock in your AWS account.\n- Request access to your desired models (e.g. Claude Sonnet 3.7, Amazon Nova Pro)\n- Optional - you may need to configure AWS CLI locally to authenticate with your AWS account.\n\n### Important Notes\n\n- Bedrock usage may incur costs based on your Bedrock plan.\n- Be mindful of sensitive data and consider security measures when integrating with external services.\n\nFirst you'll need to install required libraries and enter an Bedrock API key.\nNow let's create a Pixeltable directory to hold the tables for our demo.",
          "output": "Created directory 'bedrock_demo'.\n"
        },
        {
          "code": "from pixeltable.functions import bedrock\n\n# Create a table in Pixeltable and pick a model hosted on Bedrock with some parameters\n\nt = pxt.create_table('bedrock_demo.chat', {'input': pxt.String})\n\nmsgs = [{'role': 'user', 'content': t.input}]\nt.add_computed_column(output=bedrock.converse(\n        model_id=\"amazon.nova-pro-v1:0\",\n        messages=[\n            {\n                'role': 'user',\n                'content': [\n                    {\n                        'text': t.input,\n                    }\n                ]\n            }\n        ],\n    ))",
          "context": "Now let's create a Pixeltable directory to hold the tables for our demo.\n## Basic Messages\n\nCreate a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from Bedrock.",
          "output": "Created table `chat`.\nAdded 0 column values with 0 errors.\n"
        },
        {
          "code": "# Parse the response into a new column\nt.add_computed_column(response=t.output.output.message.content[0].text)",
          "context": "Now let's create a Pixeltable directory to hold the tables for our demo.\n## Basic Messages\n\nCreate a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from Bedrock.",
          "output": "Added 0 column values with 0 errors.\n"
        },
        {
          "code": "# Start a conversation\nt.insert(input=\"What was the outcome of the 1904 US Presidential election?\")\nt.select(t.input, t.response).show()",
          "context": "Now let's create a Pixeltable directory to hold the tables for our demo.\n## Basic Messages\n\nCreate a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from Bedrock.",
          "output": "Inserting rows into `chat`: 1 rows [00:00, 147.54 rows/s]\nInserted 1 row with 0 errors.\n"
        },
        {
          "code": "import pixeltable as pxt\nimport pixeltable.functions as pxtf\nfrom pixeltable.functions.bedrock import converse, invoke_tools\nfrom duckduckgo_search import DDGS\n\n# Initialize app structure\npxt.drop_dir(\"agents\", force=True)\npxt.create_dir(\"agents\")",
          "context": "## Basic Messages\n\nCreate a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from Bedrock.\n## Advanced: Tool-based Agent with Bedrock\n\nNow let's create a more advanced example using Bedrock with tools for news search and weather information.",
          "output": "Created directory 'agents'.\n"
        },
        {
          "code": "# Define tools\n@pxt.udf\ndef search_news(keywords: str, max_results: int) -> str:\n    \"\"\"Search news using DuckDuckGo and return results.\"\"\"\n    try:\n        with DDGS() as ddgs:\n            results = ddgs.news(\n                keywords=keywords,\n                region=\"wt-wt\",\n                safesearch=\"off\",\n                timelimit=\"m\",\n                max_results=max_results,\n            )\n            formatted_results = []\n            for i, r in enumerate(results, 1):\n                formatted_results.append(\n                    f\"{i}. Title: {r['title']}\\n\"\n                    f\"   Source: {r['source']}\\n\"\n                    f\"   Published: {r['date']}\\n\"\n                    f\"   Snippet: {r['body']}\\n\"\n                )\n            return \"\\n\".join(formatted_results)\n    except Exception as e:\n        return f\"Search failed: {str(e)}\"\n\n@pxt.udf\ndef get_weather(location: str) -> str:\n    \"\"\"Mock weather function - replace with actual API call.\"\"\"\n    return f\"Current weather in {location}: 72\u00b0F, Partly Cloudy\"\n\n# Register all tools\ntools = pxt.tools(search_news, get_weather)",
          "context": "## Basic Messages\n\nCreate a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from Bedrock.\n## Advanced: Tool-based Agent with Bedrock\n\nNow let's create a more advanced example using Bedrock with tools for news search and weather information.",
          "output": null
        },
        {
          "code": "# Create base table\ntool_agent = pxt.create_table(\n    \"agents.tools\", \n    {\"prompt\": pxt.String}, \n    if_exists=\"ignore\"\n)\n\n# Add tool selection and execution workflow\ntool_agent.add_computed_column(\n    initial_response=converse(\n        model_id=\"amazon.nova-pro-v1:0\",\n        messages=[\n            {\n                'role': 'user',\n                'content': [\n                    {\n                        'text': tool_agent.prompt,\n                    }\n                ]\n            }\n        ],\n        tool_config=tools,\n    )\n)\n\n# Add tool execution\ntool_agent.add_computed_column(\n    tool_output=invoke_tools(tools, tool_agent.initial_response)\n)",
          "context": "## Basic Messages\n\nCreate a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from Bedrock.\n## Advanced: Tool-based Agent with Bedrock\n\nNow let's create a more advanced example using Bedrock with tools for news search and weather information.",
          "output": "Created table `tools`.\nAdded 0 column values with 0 errors.\nAdded 0 column values with 0 errors.\n"
        },
        {
          "code": "# Add response formatting\ntool_agent.add_computed_column(\n    tool_response_prompt=pxtf.string.format(\n        \"Orginal Prompt\\n{0}: Tool Output\\n{1}\", \n        tool_agent.prompt, \n        tool_agent.tool_output\n    ),\n    if_exists=\"ignore\",\n)\n\n# Add final response generation\ntool_agent.add_computed_column(\n    final_response=converse(\n        model_id=\"amazon.nova-pro-v1:0\",\n        messages=[\n            {\n                'role': 'user',\n                'content': [\n                    {\n                        'text': tool_agent.tool_response_prompt,\n                    }\n                ]\n            }\n        ]\n    )\n)\n\ntool_agent.add_computed_column(\n    answer=tool_agent.final_response.output.message.content[0].text\n)",
          "context": "## Basic Messages\n\nCreate a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from Bedrock.\n## Advanced: Tool-based Agent with Bedrock\n\nNow let's create a more advanced example using Bedrock with tools for news search and weather information.",
          "output": "Added 0 column values with 0 errors.\nAdded 0 column values with 0 errors.\nAdded 0 column values with 0 errors.\n"
        },
        {
          "code": "# Example queries using different tools\nqueries = [\n    \"What's the latest news about SpaceX?\",\n    \"What's the weather in San Francisco?\",\n]\n\n# Use the agent\nfor query in queries:\n    tool_agent.insert(prompt=query)\n    result = tool_agent.select(\n        tool_agent.prompt,\n        tool_agent.tool_output,\n        tool_agent.answer\n    ).tail(1)\n    print(f\"\\nQuery: {query}\")\n    print(f\"Answer: {result['answer'][0]}\")\n\n# Display the full table\ntool_agent.select(tool_agent.prompt, tool_agent.answer).show()",
          "context": "## Basic Messages\n\nCreate a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from Bedrock.\n## Advanced: Tool-based Agent with Bedrock\n\nNow let's create a more advanced example using Bedrock with tools for news search and weather information.",
          "output": "Inserting rows into `tools`: 1 rows [00:00, 168.89 rows/s]\nInserted 1 row with 0 errors.\n\nQuery: What's the latest news about SpaceX?\nAnswer: Here's the latest news about SpaceX:\n\n1. **Starbase City Proposal**:\n   - **Title**: The home of Elon Musk's SpaceX could become an official Texas city called Starbase\n   - **Source**: The Associated Press\n   - **Published**: 2025-04-30\n   - **Snippet**: An election is scheduled for Saturday to determine whether a small area of coastal South Texas, which is home to SpaceX, will become an official city named Starbase.\n\n2. **250th Starlink Satellite Mission**:\n   - **Title**: SpaceX launches 250th Starlink satellite mission, lands rocket at sea (video, photos)\n   - **Source**: Space.com\n   - **Published**: 2025-04-28\n   - **Snippet**: SpaceX achieved a milestone by launching its 250th Starlink mission. A Falcon 9 rocket carrying 23 Starlink satellites, including 13 with direct-to-cell capability, lifted off from Cape Canaveral Space Force Station.\n\n3. **Back-to-Back Starlink Launches**:\n   - **Title**: SpaceX Falcon 9 rocket launches 1st of 2 planned Starlink launches in 2 days, lands booster at sea (video)\n   - **Source**: Space.com\n   - **Published**: 2025-04-28\n   - **Snippet**: SpaceX conducted its 49th Falcon 9 mission of 2025 and has another Starlink launch scheduled for later that night from Cape Canaveral Space Force Station.\n\n4. **Amazon\u2019s Satellite Internet Competition**:\n   - **Title**: Amazon Just Launched Its First Internet Satellites to Compete Against SpaceX's Starlink\n   - **Source**: The Associated Press\n   - **Published**: 2025-04-29\n   - **Snippet**: Amazon launched its first set of internet satellites using an Atlas V rocket, aiming to compete with SpaceX's Starlink.\n\n5. **Recent Starlink Satellite Launch**:\n   - **Title**: SpaceX launches batch of Starlink satellites into low-Earth orbit\n   - **Source**: UPI\n   - **Published**: 2025-04-28\n   - **Snippet**: SpaceX launched a Falcon 9 rocket carrying nearly two dozen Starlink satellites into low-Earth orbit from Florida's east coast.\nInserting rows into `tools`: 1 rows [00:00, 363.87 rows/s]\nInserted 1 row with 0 errors.\n\nQuery: What's the weather in San Francisco?\nAnswer: Based on the tool output provided, here's the current weather in San Francisco:\n\n- **Temperature:** 72\u00b0F\n- **Condition:** Partly Cloudy\n\nIf you need more detailed information or a forecast for the upcoming days, feel free to ask!\n"
        }
      ]
    },
    {
      "notebook": "use-cases/rag-demo.ipynb",
      "title": "Document Indexing and RAG",
      "description": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/use-cases/rag-demo.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/use-cases/rag-demo.ipynb) In this tutorial, we'll demonstrate how RAG operations can be implemented in Pixeltable. In particular, we'll develop a RAG application that summarizes a collection of PDF documents and uses ChatGPT to answer questions about them. In a traditional RAG workflow, such operations might be implemented as a Python script that runs on a periodic schedule or in response to certain events. In Pixeltable, they are implemented as persistent tables that are updated automatically and incrementally as new data becomes available. **If you are running this tutorial in Colab:**",
      "workflows": [],
      "api_usage": {},
      "key_concepts": [
        "udf",
        "incremental",
        "embedding",
        "iterator",
        "computed column",
        "multimodal"
      ],
      "code_snippets": [
        {
          "code": "import os\nimport getpass\nif 'OPENAI_API_KEY' not in os.environ:\n    os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/use-cases/rag-demo.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/use-cases/rag-demo.ipynb)\n\n# Document Indexing and RAG\nIn this tutorial, we'll demonstrate how RAG operations can be implemented in Pixeltable. In particular, we'll develop a RAG application that summarizes a collection of PDF documents and uses ChatGPT to answer questions about them.\n\nIn a traditional RAG workflow, such operations might be implemented as a Python script that runs on a periodic schedule or in response to certain events. In Pixeltable, they are implemented as persistent tables that are updated automatically and incrementally as new data becomes available.\n\n**If you are running this tutorial in Colab:**\nIn order to make the tutorial run a bit snappier, let's switch to a GPU-equipped instance for this Colab session. To do that, click on the `Runtime -> Change runtime type` menu item at the top, then select the `GPU` radio button and click on `Save`.\n\nWe first set up our OpenAI API key:",
          "output": null
        },
        {
          "code": "%pip install -q pixeltable sentence-transformers tiktoken openai openpyxl",
          "context": "In this tutorial, we'll demonstrate how RAG operations can be implemented in Pixeltable. In particular, we'll develop a RAG application that summarizes a collection of PDF documents and uses ChatGPT to answer questions about them.\n\nIn a traditional RAG workflow, such operations might be implemented as a Python script that runs on a periodic schedule or in response to certain events. In Pixeltable, they are implemented as persistent tables that are updated automatically and incrementally as new data becomes available.\n\n**If you are running this tutorial in Colab:**\nIn order to make the tutorial run a bit snappier, let's switch to a GPU-equipped instance for this Colab session. To do that, click on the `Runtime -> Change runtime type` menu item at the top, then select the `GPU` radio button and click on `Save`.\n\nWe first set up our OpenAI API key:\nWe then install the packages we need for this tutorial and then set up our environment.",
          "output": "Note: you may need to restart the kernel to use updated packages.\n"
        },
        {
          "code": "import numpy as np\nimport pixeltable as pxt\n\n# Ensure a clean slate for the demo\npxt.drop_dir('rag_demo', force=True)\npxt.create_dir('rag_demo')",
          "context": "In this tutorial, we'll demonstrate how RAG operations can be implemented in Pixeltable. In particular, we'll develop a RAG application that summarizes a collection of PDF documents and uses ChatGPT to answer questions about them.\n\nIn a traditional RAG workflow, such operations might be implemented as a Python script that runs on a periodic schedule or in response to certain events. In Pixeltable, they are implemented as persistent tables that are updated automatically and incrementally as new data becomes available.\n\n**If you are running this tutorial in Colab:**\nIn order to make the tutorial run a bit snappier, let's switch to a GPU-equipped instance for this Colab session. To do that, click on the `Runtime -> Change runtime type` menu item at the top, then select the `GPU` radio button and click on `Save`.\n\nWe first set up our OpenAI API key:\nWe then install the packages we need for this tutorial and then set up our environment.",
          "output": "Connected to Pixeltable database at: postgresql+psycopg://postgres:@/pixeltable?host=/home/marcel/.pixeltable/pgdata\nCreated directory 'rag_demo'.\n"
        },
        {
          "code": "base = 'https://github.com/pixeltable/pixeltable/raw/main/docs/resources/rag-demo/'\nqa_url = base + 'Q-A-Rag.xlsx'\nqueries_t = pxt.io.import_excel('rag_demo.queries', qa_url)",
          "context": "We then install the packages we need for this tutorial and then set up our environment.\nNext we'll create a table containing the sample questions we want to answer. The questions are stored in an Excel spreadsheet, along with a set of \"ground truth\" answers to help evaluate our model pipeline. We can use Pixeltable's handy `import_excel()` utility to load them. Note that we can pass the URL of the spreadsheet directly to the import utility.",
          "output": "Created table 'queries'.\nInserting rows into `queries`: 8 rows [00:00, 1853.94 rows/s]\nInserted 8 rows with 0 errors.\n"
        },
        {
          "code": "queries_t.head()",
          "context": "We then install the packages we need for this tutorial and then set up our environment.\nNext we'll create a table containing the sample questions we want to answer. The questions are stored in an Excel spreadsheet, along with a set of \"ground truth\" answers to help evaluate our model pipeline. We can use Pixeltable's handy `import_excel()` utility to load them. Note that we can pass the URL of the spreadsheet directly to the import utility.",
          "output": "   S__No_                                           Question  \\\n0       1          What is roughly the current mortage rate?   \n1       2  What is the current dividend yield for Alphabe...   \n2       3     What is the market capitalization of Alphabet?   \n3       4  What are the latest financial metrics for Acce...   \n4       5  What is the overall latest rating for Amazon.c...   \n5       6  What is the operating cash flow of Amazon in Q...   \n6       7    What is the expected EPS for Nvidia in Q1 2026?   \n7       8           What are the main reasons to buy Nvidia?   \n\n                                      correct_answer  \n0                                               0.07  \n1                                             0.0046  \n2                                    $2182.8 Billion  \n3  missed consensus forecasts and strong total bo...  \n4                                               SELL  \n5                                     18,989 Million  \n6                                           0.73 EPS  \n7  Datacenter, GPUs Demands, Self-driving, and ca...  "
        },
        {
          "code": "documents_t = pxt.create_table(\n    'rag_demo.documents',\n    {'document': pxt.Document}\n)\n\ndocuments_t",
          "context": "Next we'll create a table containing the sample questions we want to answer. The questions are stored in an Excel spreadsheet, along with a set of \"ground truth\" answers to help evaluate our model pipeline. We can use Pixeltable's handy `import_excel()` utility to load them. Note that we can pass the URL of the spreadsheet directly to the import utility.\n## Outline\n\nThere are two major parts to our RAG application:\n\n1. Document Indexing: Load the documents, split them into chunks, and index them using a vector embedding.\n2. Querying: For each question on our list, do a top-k lookup for the most relevant chunks, use them to construct a ChatGPT prompt, and send the enriched prompt to an LLM.\n\nWe'll implement both parts in Pixeltable.\n\n## Document Indexing \n\nAll data in Pixeltable, including documents, resides in tables.\n\nTables are persistent containers that can serve as the store of record for your data. Since we are starting from scratch, we will start with an empty table `rag_demo.documents` with a single column, `document`.",
          "output": "Created table 'documents'.\n"
        },
        {
          "code": "document_urls = [\n    base + 'Argus-Market-Digest-June-2024.pdf',\n    base + 'Argus-Market-Watch-June-2024.pdf',\n    base + 'Company-Research-Alphabet.pdf',\n    base + 'Jefferson-Amazon.pdf',\n    base + 'Mclean-Equity-Alphabet.pdf',\n    base + 'Zacks-Nvidia-Report.pdf',\n]",
          "context": "## Outline\n\nThere are two major parts to our RAG application:\n\n1. Document Indexing: Load the documents, split them into chunks, and index them using a vector embedding.\n2. Querying: For each question on our list, do a top-k lookup for the most relevant chunks, use them to construct a ChatGPT prompt, and send the enriched prompt to an LLM.\n\nWe'll implement both parts in Pixeltable.\n\n## Document Indexing \n\nAll data in Pixeltable, including documents, resides in tables.\n\nTables are persistent containers that can serve as the store of record for your data. Since we are starting from scratch, we will start with an empty table `rag_demo.documents` with a single column, `document`.\nNext, we'll insert our first few source documents into the new table. We'll leave the rest for later, in order to show how to update the indexed document base incrementally.",
          "output": null
        },
        {
          "code": "documents_t.insert({'document': url} for url in document_urls[:3])\ndocuments_t.show()",
          "context": "## Outline\n\nThere are two major parts to our RAG application:\n\n1. Document Indexing: Load the documents, split them into chunks, and index them using a vector embedding.\n2. Querying: For each question on our list, do a top-k lookup for the most relevant chunks, use them to construct a ChatGPT prompt, and send the enriched prompt to an LLM.\n\nWe'll implement both parts in Pixeltable.\n\n## Document Indexing \n\nAll data in Pixeltable, including documents, resides in tables.\n\nTables are persistent containers that can serve as the store of record for your data. Since we are starting from scratch, we will start with an empty table `rag_demo.documents` with a single column, `document`.\nNext, we'll insert our first few source documents into the new table. We'll leave the rest for later, in order to show how to update the indexed document base incrementally.",
          "output": "Inserting rows into `documents`: 3 rows [00:00, 1925.76 rows/s]\nInserted 3 rows with 0 errors.\n"
        },
        {
          "code": "from pixeltable.iterators import DocumentSplitter\n\nchunks_t = pxt.create_view(\n    'rag_demo.chunks',\n    documents_t,\n    iterator=DocumentSplitter.create(\n        document=documents_t.document,\n        separators='token_limit',\n        limit=300\n    )\n)",
          "context": "Next, we'll insert our first few source documents into the new table. We'll leave the rest for later, in order to show how to update the indexed document base incrementally.\nIn RAG applications, we often decompose documents into smaller units, or chunks, rather than treating each document as a single entity. In this example, we'll use Pixeltable's built-in `DocumentSplitter`, but in general the chunking methodology is highly customizable. `DocumentSplitter` has a variety of options for controlling the chunking behavior, and it's also possible to replace it entirely with a user-defined iterator (or an adapter for a third-party document splitter).\n\nIn Pixeltable, operations such as chunking can be automated by creating **views** of the base `documents` table. A view is a virtual derived table: rather than adding data directly to the view, we define it via a computation over the base table. In this example, the view is defined by iteration over the chunks of a `DocumentSplitter`.",
          "output": "Inserting rows into `chunks`: 41 rows [00:00, 21767.91 rows/s]\n"
        },
        {
          "code": "chunks_t",
          "context": "In RAG applications, we often decompose documents into smaller units, or chunks, rather than treating each document as a single entity. In this example, we'll use Pixeltable's built-in `DocumentSplitter`, but in general the chunking methodology is highly customizable. `DocumentSplitter` has a variety of options for controlling the chunking behavior, and it's also possible to replace it entirely with a user-defined iterator (or an adapter for a third-party document splitter).\n\nIn Pixeltable, operations such as chunking can be automated by creating **views** of the base `documents` table. A view is a virtual derived table: rather than adding data directly to the view, we define it via a computation over the base table. In this example, the view is defined by iteration over the chunks of a `DocumentSplitter`.\nOur `chunks` view now has 3 columns:",
          "output": "view 'rag_demo.chunks' (of 'rag_demo.documents')\n\n Column Name              Type Computed With\n         pos     Required[Int]              \n        text  Required[String]              \n    document          Document              "
        },
        {
          "code": "chunks_t.where(chunks_t.pos < 2).show()",
          "context": "- `text` is the chunk text produced by the `DocumentSplitter`\n- `pos` is a system-generated integer column, starting at 0, that provides a sequence number for each row\n- `document`, which is simply the `document` column from the base table `documents`. We won't need it here, but having access to the base table's columns (in effect a parent-child join) can be quite useful.\nNotice that as soon as we created it, `chunks` was automatically populated with data from the existing documents in our base table. We can select the first 2 chunks from each document using common dataframe operations, in order to get a feel for what was extracted:",
          "output": "   pos                                               text  \\\n0    0  MARKET DIGEST\\n- 1 -\\n FRIDAY, JUNE 21, 2024\\n...   \n1    1   500 lower by 0.79% and 0.25%,\\nrespectively. ...   \n2    0  Friday, June 21, 2024\\nIntermediate Term:\\nMar...   \n3    1  \\n37058.23\\n5473.17\\n4831.39\\n17721.59\\n15160....   \n4    0  Company Research Highlights\\n\u00ae\\nReport created...   \n5    1  +11.78%\\nEnterprise Value\\n$2103.1 B\\n6/20/202...   \n\n                                            document  \n0  /home/marcel/.pixeltable/file_cache/c7638b6b71...  \n1  /home/marcel/.pixeltable/file_cache/c7638b6b71...  \n2  /home/marcel/.pixeltable/file_cache/c7638b6b71...  \n3  /home/marcel/.pixeltable/file_cache/c7638b6b71...  \n4  /home/marcel/.pixeltable/file_cache/c7638b6b71...  \n5  /home/marcel/.pixeltable/file_cache/c7638b6b71...  "
        },
        {
          "code": "from pixeltable.functions.huggingface import sentence_transformer\n\nchunks_t.add_embedding_index(\n    'text',\n    embedding=sentence_transformer.using(model_id='intfloat/e5-large-v2')\n)",
          "context": "Notice that as soon as we created it, `chunks` was automatically populated with data from the existing documents in our base table. We can select the first 2 chunks from each document using common dataframe operations, in order to get a feel for what was extracted:\nNow let's compute vector embeddings for the document chunks and store them in a vector index. Pixeltable has built-in support for vector indexing using a variety of embedding model families, and it's easy for users to add new ones via UDFs. In this demo, we're going to use the E5 model from the Huggingface `sentence_transformers` library, which runs locally. \n\nThe following command creates a vector index on the `text` column in the `chunks` table, using the E5 embedding model. (For details on index creation, see the [Embedding and Vector Indices](https://docs.pixeltable.com/docs/embedding-vector-indexes) guide.) Note that defining the index is sufficient in order to load it with the existing data (and also to update it when the underlying data changes, as we'll see later).",
          "output": null
        },
        {
          "code": "query_text = \"What is the expected EPS for Nvidia in Q1 2026?\"\nsim = chunks_t.text.similarity(query_text)\nnvidia_eps_query = (\n    chunks_t\n    .order_by(sim, asc=False)\n    .select(similarity=sim, text=chunks_t.text)\n    .limit(5)\n)\nnvidia_eps_query.collect()",
          "context": "This completes the first part of our application, creating an indexed document base. Next, we'll use it to run some queries.\n## Querying\n\nIn order to express a top-k lookup against our index, we use Pixeltable's `similarity` operator in combination with the standard `order_by` and `limit` operations. Before building this into our application, let's run a sample query to make sure it works.",
          "output": "   similarity                                               text\n0    0.800725   on 6/20/2024:\\n$176.30\\nCommunication Service...\n1    0.799386  /B/E/S from Refinitiv\\nEarnings in US Dollars\\...\n2    0.796000  +11.78%\\nEnterprise Value\\n$2103.1 B\\n6/20/202...\n3    0.794835  Friday, June 21, 2024\\nIntermediate Term:\\nMar...\n4    0.794355  2024:\\n$176.30\\nCommunication Services Sector\\..."
        },
        {
          "code": "# A @query is essentially a reusable, parameterized query that is attached to a table (or view),\n# which is a modular way of getting data from that table.\n\n@pxt.query\ndef top_k(query_text: str):\n    sim = chunks_t.text.similarity(query_text)\n    return (\n        chunks_t.order_by(sim, asc=False)\n            .select(chunks_t.text, sim=sim)\n            .limit(5)\n    )\n\n# Now add a computed column to `queries_t`, calling the query\n# `top_k` that we just defined.\nqueries_t.add_computed_column(\n    question_context=top_k(queries_t.Question)\n)",
          "context": "## Querying\n\nIn order to express a top-k lookup against our index, we use Pixeltable's `similarity` operator in combination with the standard `order_by` and `limit` operations. Before building this into our application, let's run a sample query to make sure it works.\nWe perform this context retrieval for each row of our `queries` table by adding it as a computed column. In this case, the operation is a top-k similarity lookup against the data in the `chunks` table. To implement this operation, we'll use Pixeltable's `@query` decorator to enhance the capabilities of the `chunks` table.",
          "output": "Added 8 column values with 0 errors.\n"
        },
        {
          "code": "queries_t",
          "context": "We perform this context retrieval for each row of our `queries` table by adding it as a computed column. In this case, the operation is a top-k similarity lookup against the data in the `chunks` table. To implement this operation, we'll use Pixeltable's `@query` decorator to enhance the capabilities of the `chunks` table.\nOur `queries` table now looks like this:",
          "output": "table 'rag_demo.queries'\n\n       Column Name    Type    Computed With\n            S__No_     Int                 \n          Question  String                 \n    correct_answer  String                 \n  question_context    Json  top_k(Question)"
        },
        {
          "code": "queries_t.select(queries_t.question_context).head(1)",
          "context": "Our `queries` table now looks like this:\nThe new column `question_context` now contains the result of executing the query for each row, formatted as a list of dictionaries:",
          "output": "                                    question_context\n0  [{'sim': 0.7950694561004639, 'text': ' that si..."
        },
        {
          "code": "# Define a UDF to create an LLM prompt given a top-k list of\n# context chunks and a question.\n@pxt.udf\ndef create_prompt(top_k_list: list[dict], question: str) -> str:\n    concat_top_k = '\\n\\n'.join(\n        elt['text'] for elt in reversed(top_k_list)\n    )\n    return f'''\n    PASSAGES:\n\n    {concat_top_k}\n\n    QUESTION:\n\n    {question}'''",
          "context": "The new column `question_context` now contains the result of executing the query for each row, formatted as a list of dictionaries:\n### Asking the LLM\n\nNow it's time for the final step in our application: feeding the document chunks and questions to an LLM for resolution. In this demo, we'll use OpenAI for this, but any other inference cloud or local model could be used instead.\n\nWe start by defining a UDF that takes a top-k list of context chunks and a question and turns them into a ChatGPT prompt.",
          "output": null
        },
        {
          "code": "queries_t.add_computed_column(\n    prompt=create_prompt(queries_t.question_context, queries_t.Question)\n)",
          "context": "### Asking the LLM\n\nNow it's time for the final step in our application: feeding the document chunks and questions to an LLM for resolution. In this demo, we'll use OpenAI for this, but any other inference cloud or local model could be used instead.\n\nWe start by defining a UDF that takes a top-k list of context chunks and a question and turns them into a ChatGPT prompt.\nWe then add that again as a computed column to `queries`:",
          "output": "Added 8 column values with 0 errors.\n"
        },
        {
          "code": "queries_t",
          "context": "We then add that again as a computed column to `queries`:\nWe now have a new string column containing the prompt:",
          "output": "table 'rag_demo.queries'\n\n       Column Name    Type                              Computed With\n            S__No_     Int                                           \n          Question  String                                           \n    correct_answer  String                                           \n  question_context    Json                            top_k(Question)\n            prompt  String  create_prompt(question_context, Question)"
        },
        {
          "code": "queries_t.select(queries_t.prompt).head(1)",
          "context": "We then add that again as a computed column to `queries`:\nWe now have a new string column containing the prompt:",
          "output": "                                              prompt\n0  \\n    PASSAGES:\\n\\n    .20; the new guidance i..."
        },
        {
          "code": "from pixeltable.functions import openai\n\n# Assemble the prompt and instructions into OpenAI's message format\nmessages = [\n    {\n        'role': 'system',\n        'content': 'Please read the following passages and answer the question based on their contents.'\n    },\n    {\n        'role': 'user',\n        'content': queries_t.prompt\n    }\n]\n\n# Add a computed column that calls OpenAI\nqueries_t.add_computed_column(\n    response=openai.chat_completions(model='gpt-4o-mini', messages=messages)\n)",
          "context": "We now have a new string column containing the prompt:\nWe now add another computed column to call OpenAI. For the `chat_completions()` call, we need to construct two messages, containing the instructions to the model and the prompt. For the latter, we can simply reference the `prompt` column we just added.",
          "output": "Added 8 column values with 0 errors.\n"
        },
        {
          "code": "queries_t.add_computed_column(\n    answer=queries_t.response.choices[0].message.content\n)",
          "context": "We now add another computed column to call OpenAI. For the `chat_completions()` call, we need to construct two messages, containing the instructions to the model and the prompt. For the latter, we can simply reference the `prompt` column we just added.\nOur `queries` table now contains a JSON-structured column `response`, which holds the entire API response structure. At the moment, we're only interested in the response content, which we can extract easily into another computed column:",
          "output": "Added 8 column values with 0 errors.\n"
        },
        {
          "code": "queries_t",
          "context": "Our `queries` table now contains a JSON-structured column `response`, which holds the entire API response structure. At the moment, we're only interested in the response content, which we can extract easily into another computed column:\nWe now have the following `queries` schema:",
          "output": "table 'rag_demo.queries'\n\n       Column Name            Type                                      Computed With\n            S__No_             Int                                                   \n          Question          String                                                   \n    correct_answer          String                                                   \n  question_context            Json                                    top_k(Question)\n            prompt          String          create_prompt(question_context, Question)\n          response  Required[Json]  chat_completions(model='gpt-4o-mini', messages...\n            answer            Json                response.choices[0].message.content"
        },
        {
          "code": "queries_t.select(queries_t.Question, queries_t.correct_answer, queries_t.answer).show()",
          "context": "We now have the following `queries` schema:\nLet's take a look at what we got back:",
          "output": "                                            Question  \\\n0          What is roughly the current mortage rate?   \n1  What is the overall latest rating for Amazon.c...   \n2     What is the market capitalization of Alphabet?   \n3  What is the current dividend yield for Alphabe...   \n4    What is the expected EPS for Nvidia in Q1 2026?   \n5  What is the operating cash flow of Amazon in Q...   \n6  What are the latest financial metrics for Acce...   \n7           What are the main reasons to buy Nvidia?   \n\n                                      correct_answer  \\\n0                                               0.07   \n1                                               SELL   \n2                                    $2182.8 Billion   \n3                                             0.0046   \n4                                           0.73 EPS   \n5                                     18,989 Million   \n6  missed consensus forecasts and strong total bo...   \n7  Datacenter, GPUs Demands, Self-driving, and ca...   \n\n                                              answer  \n0              The current mortgage rate is near 7%.  \n1  The provided passages contain detailed informa...  \n2  The market capitalization of Alphabet Inc. is ...  \n3  The passages do not provide any information re...  \n4  The provided passages do not contain any infor...  \n5  The provided passages do not contain informati...  \n6  The latest financial metrics for Accenture PLC...  \n7  The passages provided do not explicitly discus...  "
        },
        {
          "code": "documents_t.insert({'document': p} for p in document_urls[3:])",
          "context": "## Incremental Updates\nPixeltable's views and computed columns update automatically in response to new data. We can see this when we add the remaining documents to our `documents` table. Watch how the `chunks` view is updated to stay in sync with `documents`:",
          "output": "Inserting rows into `documents`: 3 rows [00:00, 1949.63 rows/s]\nInserting rows into `chunks`: 68 rows [00:00, 601.57 rows/s]\nInserted 71 rows with 0 errors.\n"
        },
        {
          "code": "documents_t.show()",
          "context": "## Incremental Updates\nPixeltable's views and computed columns update automatically in response to new data. We can see this when we add the remaining documents to our `documents` table. Watch how the `chunks` view is updated to stay in sync with `documents`:",
          "output": "                                            document\n0  /home/marcel/.pixeltable/file_cache/c7638b6b71...\n1  /home/marcel/.pixeltable/file_cache/c7638b6b71...\n2  /home/marcel/.pixeltable/file_cache/c7638b6b71...\n3  /home/marcel/.pixeltable/file_cache/c7638b6b71...\n4  /home/marcel/.pixeltable/file_cache/c7638b6b71...\n5  /home/marcel/.pixeltable/file_cache/c7638b6b71..."
        },
        {
          "code": "nvidia_eps_query.collect()",
          "context": "(Note: although Pixeltable updates `documents` and `chunks`, it **does not** automatically update the `queries` table. This is by design: we don't want all rows in `queries` to get automatically re-executed every time a single new document is added to the document base. However, newly-added rows will be run over the new, incrementally-updated index.)\nTo confirm that the `chunks` index got updated, we'll re-run the chunks retrieval query for the question\n\n```What is the expected EPS for Nvidia in Q1 2026?```\n\nPreviously, our most similar chunk had a similarity score of ~0.81. Let's see what we get now:",
          "output": "   similarity                                               text\n0    0.862984  4\\n7,192 A\\n13,507 A\\n18,120 A\\n22,103 A\\n60,9...\n1    0.854627   and Microsoft's Xbox One will also be going w...\n2    0.847631  ations for Windows to deliver maximum performa...\n3    0.841044  2024, the total long-term debt was $8.46 billi...\n4    0.840021  9.78%.\\nNVIDIA Drive Thor solution was adopted..."
        },
        {
          "code": "queries_t.recompute_columns('response')",
          "context": "To confirm that the `chunks` index got updated, we'll re-run the chunks retrieval query for the question\n\n```What is the expected EPS for Nvidia in Q1 2026?```\n\nPreviously, our most similar chunk had a similarity score of ~0.81. Let's see what we get now:\nOur most similar chunk now has a score of ~0.86 and pulls in more relevant chunks from the newly-inserted documents.\n\nLet's recompute the `response` column of the `queries` table, which will automatically recompute the `answer` column as well.",
          "output": "Inserting rows into `queries`: 8 rows [00:00, 2128.01 rows/s]\n"
        },
        {
          "code": "queries_t.select(\n    queries_t.Question,\n    queries_t.correct_answer,\n    queries_t.answer\n).show()",
          "context": "Our most similar chunk now has a score of ~0.86 and pulls in more relevant chunks from the newly-inserted documents.\n\nLet's recompute the `response` column of the `queries` table, which will automatically recompute the `answer` column as well.\nAs a final step, let's confirm that all the queries now have answers:",
          "output": "                                            Question  \\\n0          What is roughly the current mortage rate?   \n1     What is the market capitalization of Alphabet?   \n2  What is the current dividend yield for Alphabe...   \n3  What is the overall latest rating for Amazon.c...   \n4  What is the operating cash flow of Amazon in Q...   \n5    What is the expected EPS for Nvidia in Q1 2026?   \n6           What are the main reasons to buy Nvidia?   \n7  What are the latest financial metrics for Acce...   \n\n                                      correct_answer  \\\n0                                               0.07   \n1                                    $2182.8 Billion   \n2                                             0.0046   \n3                                               SELL   \n4                                     18,989 Million   \n5                                           0.73 EPS   \n6  Datacenter, GPUs Demands, Self-driving, and ca...   \n7  missed consensus forecasts and strong total bo...   \n\n                                              answer  \n0              The current mortgage rate is near 7%.  \n1  The market capitalization of Alphabet Class A ...  \n2  The provided passages do not mention a current...  \n3  The provided passages contain detailed informa...  \n4  The provided passages do not contain any infor...  \n5  The passages provided do not contain any infor...  \n6  The passages provided do not explicitly mentio...  \n7  The latest financial metrics for Accenture PLC...  "
        },
        {
          "code": "",
          "context": "Our most similar chunk now has a score of ~0.86 and pulls in more relevant chunks from the newly-inserted documents.\n\nLet's recompute the `response` column of the `queries` table, which will automatically recompute the `answer` column as well.\nAs a final step, let's confirm that all the queries now have answers:",
          "output": null
        }
      ]
    },
    {
      "notebook": "use-cases/object-detection-in-videos.ipynb",
      "title": "Object Detection in Videos",
      "description": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/use-cases/object-detection-in-videos.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/use-cases/object-detection-in-videos.ipynb) In this tutorial, we'll demonstrate how to use Pixeltable to do frame-by-frame object detection, made simple through Pixeltable's video-related functionality: * automatic frame extraction",
      "workflows": [
        {
          "title": "Creating a tutorial directory and table",
          "explanation": [
            "All data in Pixeltable is stored in tables, which in turn reside in directories. We'll begin by creating a `detection_demo` directory and a table to hold our videos, with a single column of type `pxt.Video`.",
            "In order to interact with the frames, we take advantage of Pixeltable's component view concept: we create a \"view\" of our video table that contains one row for each frame of each video in the table. Pixeltable provides the built-in `FrameIterator` class for this.",
            "You'll see that neither the `videos` table nor the `frames` view has any actual data yet, because we haven't yet added any videos to the table. However, the `frames` view is now configured to automatically track the `videos` table as new data shows up.\n\nThe new view is automatically configured with six columns:\n- `pos` - a system column that is part of every component view\n- `video` - the column inherited from our base table (all base table columns are visible in any of its views)\n- `frame_idx`, `pos_msec`, `pos_frame`, `frame` - these four columns are created by the `FrameIterator` class.\n\nLet's have a look at the new view:",
            "We'll now insert a single row into the videos table, containing a video of a busy intersection in Bangkok.",
            "Notice that both the `videos` table and `frames` view were automatically updated, expanding the single video into 461 rows in the view. Let's have a look at `videos` first.",
            "Now let's peek at the first five rows of `frames`:",
            "One advantage of using Pixeltable's component view mechanism is that Pixeltable does not physically store the frames. Instead, Pixeltable re-extracts the frames on retrieval using the frame index, which can be done very efficiently and avoids any storage overhead (which can be quite substantial for video frames).",
            "## Object Detection with Pixeltable\n\nNow let's apply an object detection model to our frames. Pixeltable includes built-in support for a number of models; we're going to use the YOLOX family of models, which are lightweight models with solid performance. We first import the `yolox` Pixeltable function.",
            "Pixeltable functions operate on columns and expressions using standard Python function call syntax. Here's an example that shows how we might experiment with applying one of the YOLOX models to the first few frames in our video, using Pixeltable's powerful `select` comprehension.",
            "It may appear that we just ran the YOLOX inference over the entire view of 461 frames, but remember that Pixeltable evaluates expressions lazily: in this case, it only ran inference over the 3 frames that we actually displayed.\n\nThe inference output looks like what we'd expect, so let's add a _computed column_ that runs inference over the entire view (computed columns are discussed in detail in the [Computed Columns](https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/fundamentals/computed-columns.ipynb) tutorial). Remember that once a computed column is created, Pixeltable will update it incrementally any time new rows are added to the view. This is a convenient way to incorporate inference (and other operations) into data workflows.\n\nThis _will_ cause Pixeltable to run inference over all 461 frames, so please be patient.",
            "The new column is now part of the schema of the `frames` view:",
            "The data in the computed column is now stored for fast retrieval.",
            "Now let's create a new set of images, in which we superimpose the detected bounding boxes on top of the original images. We'll use the handy built-in `draw_bounding_boxes` UDF for this. We could create a new computed column to hold the superimposed images, but we don't have to; sometimes it's easier just to use a `select` comprehension, as we did when we were first experimenting with the detection model.",
            "Our `select` comprehension ranged over the entire table, but just as before, Pixeltable computes the output lazily: image operations are performed at retrieval time, so in this case, Pixeltable drew the annotations just for the one frame that we actually displayed.",
            "Looking at individual frames gives us some idea of how well our detection algorithm works, but it would be more instructive to turn the visualization output back into a video.\n\nWe do that with the built-in function `make_video()`, which is an aggregation function that takes a frame index (actually: any expression that can be used to order the frames; a timestamp would also work) and an image, and then assembles the sequence of images into a video.",
            "## Comparing Object Detection Models",
            "The detections that we get out of `yolox_tiny` are passable, but a little choppy. Suppose we want to experiment with a more powerful object detection model, to see if there is any improvement in detection quality. We can create an additional column to hold the new inferences. The larger model takes longer to download and run, so please be patient.",
            "Let's see the results of the two models side-by-side.",
            "Running the videos side-by-side, we can see that the larger model is higher in quality: less flickering, with more stable boxes from frame to frame.\n\n## Evaluating Models Against a Ground Truth\n\nIn order to do a quantitative evaluation of model performance, we need a ground truth to compare them against. Let's generate some (synthetic) \"ground truth\" data by running against the largest YOLOX model available. It will take even longer to cache and evaluate this model.",
            "Let's have a look at our enlarged view, now with three `detections` columns.",
            "We're going to be evaluating the generated detections with the commonly-used [mean average precision](https://learnopencv.com/mean-average-precision-map-object-detection-model-evaluation-metric/) metric (mAP).\n\nThe mAP metric is based on per-frame metrics, such as true and false positives per detected class, which are then aggregated into a single (per-class) number. In Pixeltable, functionality is available via the `eval_detections()` and `mean_ap()` built-in functions.",
            "Let's take a look at the output.",
            "The computation of the mAP metric is now simply a query over the evaluation output, aggregated with the `mean_ap()` function.",
            "This two-step process allows you to compute mAP at every granularity: over your entire dataset, only for specific videos, only for videos that pass a certain filter, etc. Moreover, you can compute this metric any time, not just during training, and use it to guide your understanding of your dataset and how it affects the quality of your models."
          ],
          "code_blocks": [
            {
              "code": "%pip install -qU pixeltable pixeltable-yolox",
              "output": null,
              "explanation": ""
            },
            {
              "code": "import pixeltable as pxt\n\npxt.create_dir('detection_demo', if_exists='replace_force')\nvideos_table = pxt.create_table(\n    'detection_demo.videos',\n    {'video': pxt.Video}\n)",
              "output": "Connected to Pixeltable database at: postgresql+psycopg://postgres:@/pixeltable?host=/Users/asiegel/.pixeltable/pgdata\nCreated directory 'detection_demo'.\nCreated table `videos`.\n",
              "explanation": ""
            },
            {
              "code": "from pixeltable.iterators import FrameIterator\n\nframes_view = pxt.create_view(\n    'detection_demo.frames',\n    videos_table,\n    # `fps` determines the frame rate; a value of `0`\n    # indicates the native frame rate of the video.\n    iterator=FrameIterator.create(video=videos_table.video, fps=0)\n)",
              "output": "Created view `frames` with 0 rows, 0 exceptions.\n",
              "explanation": ""
            },
            {
              "code": "frames_view",
              "output": "View 'detection_demo.frames' (of 'detection_demo.videos')\n\n Column Name             Type Computed With\n         pos    Required[Int]              \n   frame_idx    Required[Int]              \n    pos_msec  Required[Float]              \n   pos_frame    Required[Int]              \n       frame  Required[Image]              \n       video            Video              ",
              "explanation": ""
            },
            {
              "code": "videos_table.insert([\n    {\n        'video': 'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/bangkok.mp4'\n    }\n])",
              "output": "Inserting rows into `videos`: 1 rows [00:00, 321.33 rows/s]\nInserting rows into `frames`: 461 rows [00:00, 14957.29 rows/s]\nInserted 462 rows with 0 errors.\n",
              "explanation": ""
            },
            {
              "code": "videos_table.show()",
              "output": "                                               video\n0  /Users/asiegel/.pixeltable/file_cache/afea8fab...",
              "explanation": ""
            },
            {
              "code": "frames_view.select(\n    frames_view.pos,\n    frames_view.frame,\n    frames_view.frame.width,\n    frames_view.frame.height\n).show(5)",
              "output": "   pos                                              frame  width  height\n0    0  <PIL.Image.Image image mode=RGB size=1280x720 ...   1280     720\n1    1  <PIL.Image.Image image mode=RGB size=1280x720 ...   1280     720\n2    2  <PIL.Image.Image image mode=RGB size=1280x720 ...   1280     720\n3    3  <PIL.Image.Image image mode=RGB size=1280x720 ...   1280     720\n4    4  <PIL.Image.Image image mode=RGB size=1280x720 ...   1280     720",
              "explanation": ""
            },
            {
              "code": "from pixeltable.ext.functions.yolox import yolox",
              "output": null,
              "explanation": ""
            },
            {
              "code": "# Show the results of applying the `yolox_tiny` model\n# to the first few frames in the table.\n\nframes_view.select(\n    frames_view.frame,\n    yolox(frames_view.frame, model_id='yolox_tiny')\n).head(3)",
              "output": "                                               frame  \\\n0  <PIL.Image.Image image mode=RGB size=1280x720 ...   \n1  <PIL.Image.Image image mode=RGB size=1280x720 ...   \n2  <PIL.Image.Image image mode=RGB size=1280x720 ...   \n\n                                               yolox  \n0  {'bboxes': [(338.1894836425781, 345.5997924804...  \n1  {'bboxes': [(-0.688212513923645, 552.143859863...  \n2  {'bboxes': [(-0.2689361572265625, 550.29583740...  ",
              "explanation": ""
            },
            {
              "code": "# Create a computed column to compute detections using the `yolox_tiny`\n# model.\n# We'll adjust the confidence threshold down a bit (the default is 0.5)\n# to pick up even more bounding boxes.\n\nframes_view.add_computed_column(detections_tiny=yolox(\n    frames_view.frame, model_id='yolox_tiny', threshold=0.25\n))",
              "output": "Added 461 column values with 0 errors.\n",
              "explanation": ""
            },
            {
              "code": "frames_view",
              "output": "View 'detection_demo.frames' (of 'detection_demo.videos')\n\n      Column Name             Type                                      Computed With\n              pos    Required[Int]                                                   \n        frame_idx    Required[Int]                                                   \n         pos_msec  Required[Float]                                                   \n        pos_frame    Required[Int]                                                   \n            frame  Required[Image]                                                   \n  detections_tiny   Required[Json]  yolox(frame, model_id='yolox_tiny', threshold=...\n            video            Video                                                   ",
              "explanation": ""
            },
            {
              "code": "frames_view.select(\n    frames_view.frame,\n    frames_view.detections_tiny\n).show(3)",
              "output": "                                               frame  \\\n0  <PIL.Image.Image image mode=RGB size=1280x720 ...   \n1  <PIL.Image.Image image mode=RGB size=1280x720 ...   \n2  <PIL.Image.Image image mode=RGB size=1280x720 ...   \n\n                                     detections_tiny  \n0  {'bboxes': [[338.1894836425781, 345.5997924804...  \n1  {'bboxes': [[-0.688212513923645, 552.143859863...  \n2  {'bboxes': [[-0.2689361572265625, 550.29583740...  ",
              "explanation": ""
            },
            {
              "code": "import pixeltable.functions as pxtf\n\nframes_view.select(\n    frames_view.frame,\n    pxtf.vision.draw_bounding_boxes(\n        frames_view.frame,\n        frames_view.detections_tiny.bboxes,\n        width=4\n    )\n).show(1)",
              "output": "                                               frame  \\\n0  <PIL.Image.Image image mode=RGB size=1280x720 ...   \n\n                                 draw_bounding_boxes  \n0  <PIL.Image.Image image mode=RGB size=1280x720 ...  ",
              "explanation": ""
            },
            {
              "code": "frames_view.group_by(videos_table).select(\n    pxt.functions.video.make_video(\n        frames_view.pos,\n        pxtf.vision.draw_bounding_boxes(\n            frames_view.frame,\n            frames_view.detections_tiny.bboxes,\n            width=4\n        )\n    )\n).show(1)",
              "output": "                                       make_video\n0  /Users/asiegel/.pixeltable/tmp/tmpipcu8dqy.mp4",
              "explanation": ""
            },
            {
              "code": "# Here we use the larger `yolox_m` (medium) model.\n\nframes_view.add_computed_column(detections_m=yolox(\n    frames_view.frame, model_id='yolox_m', threshold=0.25\n))",
              "output": "Added 461 column values with 0 errors.\n",
              "explanation": ""
            },
            {
              "code": "frames_view.group_by(videos_table).select(\n    pxt.functions.video.make_video(\n        frames_view.pos,\n        pxtf.vision.draw_bounding_boxes(\n            frames_view.frame,\n            frames_view.detections_tiny.bboxes,\n            width=4\n        )\n    ),\n    pxt.functions.video.make_video(\n        frames_view.pos,\n        pxtf.vision.draw_bounding_boxes(\n            frames_view.frame,\n            frames_view.detections_m.bboxes,\n            width=4\n        )\n    )\n).show(1)",
              "output": "                                       make_video  \\\n0  /Users/asiegel/.pixeltable/tmp/tmp3o4znodf.mp4   \n\n                                     make_video_1  \n0  /Users/asiegel/.pixeltable/tmp/tmpuxk57w0m.mp4  ",
              "explanation": ""
            },
            {
              "code": "frames_view.add_computed_column(detections_x=yolox(\n    frames_view.frame, model_id='yolox_x', threshold=0.25\n))",
              "output": "Added 461 column values with 0 errors.\n",
              "explanation": ""
            },
            {
              "code": "frames_view",
              "output": "View 'detection_demo.frames' (of 'detection_demo.videos')\n\n      Column Name             Type                                      Computed With\n              pos    Required[Int]                                                   \n        frame_idx    Required[Int]                                                   \n         pos_msec  Required[Float]                                                   \n        pos_frame    Required[Int]                                                   \n            frame  Required[Image]                                                   \n  detections_tiny   Required[Json]  yolox(frame, model_id='yolox_tiny', threshold=...\n     detections_m   Required[Json]   yolox(frame, model_id='yolox_m', threshold=0.25)\n     detections_x   Required[Json]   yolox(frame, model_id='yolox_x', threshold=0.25)\n            video            Video                                                   ",
              "explanation": ""
            },
            {
              "code": "from pixeltable.functions.vision import eval_detections, mean_ap\n\nframes_view.add_computed_column(eval_yolox_tiny=eval_detections(\n    pred_bboxes=frames_view.detections_tiny.bboxes,\n    pred_labels=frames_view.detections_tiny.labels,\n    pred_scores=frames_view.detections_tiny.scores,\n    gt_bboxes=frames_view.detections_x.bboxes,\n    gt_labels=frames_view.detections_x.labels\n))\n\nframes_view.add_computed_column(eval_yolox_m=eval_detections(\n    pred_bboxes=frames_view.detections_m.bboxes,\n    pred_labels=frames_view.detections_m.labels,\n    pred_scores=frames_view.detections_m.scores,\n    gt_bboxes=frames_view.detections_x.bboxes,\n    gt_labels=frames_view.detections_x.labels\n))",
              "output": "Added 461 column values with 0 errors.\nAdded 461 column values with 0 errors.\n",
              "explanation": ""
            },
            {
              "code": "frames_view.select(\n    frames_view.eval_yolox_tiny,\n    frames_view.eval_yolox_m\n).show(1)",
              "output": "                                     eval_yolox_tiny  \\\n0  [{'fp': [], 'tp': [], 'class': 0, 'scores': []...   \n\n                                        eval_yolox_m  \n0  [{'fp': [0, 0, 0, 0], 'tp': [1, 1, 1, 1], 'cla...  ",
              "explanation": ""
            },
            {
              "code": "frames_view.select(\n    mean_ap(frames_view.eval_yolox_tiny),\n    mean_ap(frames_view.eval_yolox_m)\n).show()",
              "output": "                                             mean_ap  \\\n0  {0: 0.10069413081214726, 2: 0.6215400355971905...   \n\n                                           mean_ap_1  \n0  {0: 0.563540497191315, 2: 0.9112411386848447, ...  ",
              "explanation": ""
            }
          ]
        }
      ],
      "api_usage": {},
      "key_concepts": [
        "incremental",
        "computed column",
        "multimodal",
        "iterator"
      ],
      "code_snippets": []
    },
    {
      "notebook": "use-cases/audio-transcriptions.ipynb",
      "title": "Transcribing and Indexing Audio and Video in Pixeltable",
      "description": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/use-cases/audio-transcriptions.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/use-cases/audio-transcriptions.ipynb) In this tutorial, we'll build an end-to-end workflow for creating and indexing audio transcriptions of video data. We'll demonstrate how Pixeltable can be used to: 1) Extract audio data from video files; 2) Transcribe the audio using OpenAI Whisper;",
      "workflows": [
        {
          "title": "Using the OpenAI API",
          "explanation": [
            "This concludes our tutorial using the locally installed Whisper library. Sometimes, it may be preferable to use the OpenAI API rather than a locally installed library. In this section we'll show how this can be done in Pixeltable, simply by using a different function to construct our computed columns.\n\nSince this section relies on calling out to the OpenAI API, you'll need to have an API key, which you can enter below.",
            "Now let's compare the results from the local model and the API side-by-side.",
            "They look pretty similar, which isn't surprising, since the OpenAI transcriptions endpoint runs on Whisper.\n\nOne difference is that the local library spits out a lot more information about the internal behavior of the model. Note that we've been selecting `video_table.transcription.text` in the preceding queries, which pulls out just the `text` field of the transcription results. The actual results are a sizable JSON structure that includes a lot of metadata. To see the full output, we can select `video_table.transcription` instead, to get the full JSON struct. Here's what it looks like (we'll select just one row, since it's a lot of output):"
          ],
          "code_blocks": [
            {
              "code": "import os\nimport getpass\n\nif 'OPENAI_API_KEY' not in os.environ:\n    os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')",
              "output": "OpenAI API Key: \u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\u00b7\n",
              "explanation": ""
            },
            {
              "code": "from pixeltable.functions import openai\n\nvideo_table.add_computed_column(\n    transcription_from_api=openai.transcriptions(\n        video_table.audio,\n        model='whisper-1'\n    )\n)",
              "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 3/3 [00:12<00:00,  4.14s/ cells]\nAdded 3 column values with 0 errors.\n",
              "explanation": ""
            },
            {
              "code": "video_table.select(\n    video_table.video,\n    video_table.transcription.text,\n    video_table.transcription_from_api.text\n).show()",
              "output": "                                               video  \\\n0  /Users/asiegel/.pixeltable/file_cache/ffc7607f...   \n1  /Users/asiegel/.pixeltable/file_cache/ffc7607f...   \n2  /Users/asiegel/.pixeltable/file_cache/ffc7607f...   \n\n                                  transcription_text  \\\n0   of experiencing self versus remembering self....   \n1   worse, the young adults had episodic memory. ...   \n2   about reusing information and making the most...   \n\n                           transcriptionfromapi_text  \n0  of experiencing self versus remembering self, ...  \n1  or worse, the young adults at episodic memory....  \n2  about reusing information and making the most ...  ",
              "explanation": ""
            },
            {
              "code": "video_table.select(\n    video_table.transcription,\n    video_table.transcription_from_api\n).show(1)",
              "output": "                                       transcription  \\\n0  {'text': ' of experiencing self versus remembe...   \n\n                              transcription_from_api  \n0  {'text': 'of experiencing self versus remember...  ",
              "explanation": ""
            }
          ]
        }
      ],
      "api_usage": {},
      "key_concepts": [
        "incremental",
        "embedding",
        "iterator",
        "computed column",
        "multimodal"
      ],
      "code_snippets": [
        {
          "code": "%pip install -q pixeltable openai openai-whisper sentence-transformers spacy",
          "context": "## Create a Table for Video Data\nLet's first install the Python packages we'll need for the demo. We're going to use the popular Whisper library, running locally. Later in the demo, we'll see how to use the OpenAI API endpoints as an alternative.",
          "output": null
        },
        {
          "code": "import numpy as np\nimport pixeltable as pxt\n\npxt.drop_dir('transcription_demo', force=True)  # Ensure a clean slate for the demo\npxt.create_dir('transcription_demo')\n\n# Create a table to store our videos and workflow\nvideo_table = pxt.create_table(\n    'transcription_demo.video_table',\n    {'video': pxt.Video}\n)\n\nvideo_table",
          "context": "Let's first install the Python packages we'll need for the demo. We're going to use the popular Whisper library, running locally. Later in the demo, we'll see how to use the OpenAI API endpoints as an alternative.\nNow we create a Pixeltable table to hold our videos.",
          "output": "Connected to Pixeltable database at: postgresql+psycopg://postgres:@/pixeltable?host=/Users/asiegel/.pixeltable/pgdata\nCreated directory `transcription_demo`.\nCreated table `video_table`.\n"
        },
        {
          "code": "videos = [\n    'https://github.com/pixeltable/pixeltable/raw/release/docs/resources/audio-transcription-demo/'\n    f'Lex-Fridman-Podcast-430-Excerpt-{n}.mp4'\n    for n in range(3)\n]\n\nvideo_table.insert({'video': video} for video in videos[:2])\nvideo_table.show()",
          "context": "Now we create a Pixeltable table to hold our videos.\nNext let's insert some video files into the table. In this demo, we'll be using one-minute excerpts from a Lex Fridman podcast. We'll begin by inserting two of them into our new table. In this demo, our videos are given as `https` links, but Pixeltable also accepts local files and S3 URLs as input.",
          "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 26.25 cells/s]\nInserting rows into `video_table`: 2 rows [00:00, 1073.67 rows/s]\nComputing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 4/4 [00:00<00:00, 25.69 cells/s]\nInserted 2 rows with 0 errors.\n"
        },
        {
          "code": "from pixeltable.functions.video import extract_audio\n\nvideo_table.add_computed_column(\n    audio=extract_audio(video_table.video, format='mp3')\n)\nvideo_table.show()",
          "context": "Next let's insert some video files into the table. In this demo, we'll be using one-minute excerpts from a Lex Fridman podcast. We'll begin by inserting two of them into our new table. In this demo, our videos are given as `https` links, but Pixeltable also accepts local files and S3 URLs as input.\nNow we'll add another column to hold extracted audio from our videos. The new column is an example of a _computed column_: it's updated automatically based on the contents of another column (or columns). In this case, the value of the `audio` column is defined to be the audio track extracted from whatever's in the `video` column.",
          "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00,  2.20 cells/s]\nAdded 2 column values with 0 errors.\n"
        },
        {
          "code": "video_table",
          "context": "Now we'll add another column to hold extracted audio from our videos. The new column is an example of a _computed column_: it's updated automatically based on the contents of another column (or columns). In this case, the value of the `audio` column is defined to be the audio track extracted from whatever's in the `video` column.\nIf we look at the structure of the video table, we see that the new column is a computed column.",
          "output": "Table\n'transcription_demo.video_table'\n\n Column Name   Type                       Computed With\n       video  Video                                    \n       audio  Audio  extract_audio(video, format='mp3')"
        },
        {
          "code": "from pixeltable.functions.audio import get_metadata\n\nvideo_table.add_computed_column(\n    metadata=get_metadata(video_table.audio)\n)\nvideo_table.show()",
          "context": "If we look at the structure of the video table, we see that the new column is a computed column.\nWe can also add another computed column to extract metadata from the audio streams.",
          "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 289.64 cells/s]\nAdded 2 column values with 0 errors.\n"
        },
        {
          "code": "from pixeltable.functions import whisper\n\nvideo_table.add_computed_column(\n    transcription=whisper.transcribe(\n        audio=video_table.audio,\n        model='base.en'\n    )\n)\n\nvideo_table.select(\n    video_table.video,\n    video_table.transcription.text\n).show()",
          "context": "We can also add another computed column to extract metadata from the audio streams.\n## Create Transcriptions\n\nNow we'll add a step to create transcriptions of our videos. As mentioned above, we're going to use the Whisper library for this, running locally. Pixeltable has a built-in function, `whisper.transcribe`, that serves as an adapter for the Whisper library's transcription capability. All we have to do is add a computed column that calls this function:",
          "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:04<00:00,  2.11s/ cells]\nAdded 2 column values with 0 errors.\n"
        },
        {
          "code": "from pixeltable.iterators.string import StringSplitter\n\nsentences_view = pxt.create_view(\n    'transcription_demo.sentences_view',\n    video_table,\n    iterator=StringSplitter.create(\n        text=video_table.transcription.text,\n        separators='sentence'\n    )\n)",
          "context": "## Create Transcriptions\n\nNow we'll add a step to create transcriptions of our videos. As mentioned above, we're going to use the Whisper library for this, running locally. Pixeltable has a built-in function, `whisper.transcribe`, that serves as an adapter for the Whisper library's transcription capability. All we have to do is add a computed column that calls this function:\nIn order to index the transcriptions, we'll first need to split them into sentences. We can do this using Pixeltable's built-in `StringSplitter` iterator.",
          "output": "Inserting rows into `sentences_view`: 25 rows [00:00, 9187.56 rows/s]\nCreated view `sentences_view` with 25 rows, 0 exceptions.\n"
        },
        {
          "code": "sentences_view.select(\n    sentences_view.pos,\n    sentences_view.text\n).show(8)",
          "context": "In order to index the transcriptions, we'll first need to split them into sentences. We can do this using Pixeltable's built-in `StringSplitter` iterator.\nThe `StringSplitter` creates a new view, with the audio transcriptions broken into individual, one-sentence chunks.",
          "output": "   pos                                               text\n0    0      of experiencing self versus remembering self.\n1    1  I was hoping you can give a simple answer of h...\n2    2  Based on the fact that our memories could be a...\n3    3  And maybe there is some wisdom in the fact tha...\n4    4  Oh, well, first I'll say I wish I could take y...\n5    5                 That was such a great description.\n6    6                      Can I be your opening answer?\n7    7       Oh my God, no, I'm gonna open for you, dude."
        },
        {
          "code": "from pixeltable.functions.huggingface import sentence_transformer\n\nsentences_view.add_embedding_index(\n    'text',\n    embedding=sentence_transformer.using(model_id='intfloat/e5-large-v2')\n)",
          "context": "## Add an Embedding Index\nNext, let's use the Huggingface `sentence_transformers` library to create an embedding index of our sentences, attaching it to the `text` column of our `sentences_view`.",
          "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 25/25 [00:03<00:00,  8.18 cells/s]\n"
        },
        {
          "code": "sim = sentences_view.text.similarity('What is happiness?')\n\n(\n    sentences_view\n    .order_by(sim, asc=False)\n    .limit(10)\n    .select(sentences_view.text,similarity=sim)\n    .collect()\n)",
          "context": "Next, let's use the Huggingface `sentence_transformers` library to create an embedding index of our sentences, attaching it to the `text` column of our `sentences_view`.\nWe can do a simple lookup to test our new index. The following snippet returns the results of a nearest-neighbor search on the input \"What is happiness?\"",
          "output": "                                                text  similarity\n0  Based on the fact that our memories could be a...    0.805073\n1  I was hoping you can give a simple answer of h...    0.792060\n2  Why would we have this period of time that's s...    0.789130\n3                                I want to really be    0.787846\n4                      Can I be your opening answer?    0.785402\n5      of experiencing self versus remembering self.    0.785325\n6                         I need a prefrontal cortex    0.785176\n7  And maybe there is some wisdom in the fact tha...    0.784597\n8                    What's the best way to do that?    0.783154\n9  And it's like, I realize I have to redefine wh...    0.775783"
        },
        {
          "code": "video_table.insert(video=videos[2])",
          "context": "We can do a simple lookup to test our new index. The following snippet returns the results of a nearest-neighbor search on the input \"What is happiness?\"\n## Incremental Updates\n\n_Incremental updates_ are a key feature of Pixeltable. Whenever a new video is added to the original table, all of its downstream computed columns are updated automatically. Let's demonstrate this by adding a third video to the table and seeing how the updates propagate through to the index.",
          "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:01<00:00,  2.58 cells/s]\nInserting rows into `video_table`: 1 rows [00:00, 277.18 rows/s]\nComputing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 5/5 [00:01<00:00,  2.57 cells/s]\nInserting rows into `sentences_view`: 8 rows [00:00, 978.69 rows/s]\nInserted 9 rows with 0 errors.\n"
        },
        {
          "code": "video_table.select(\n    video_table.video,\n    video_table.metadata,\n    video_table.transcription.text\n).show()",
          "context": "We can do a simple lookup to test our new index. The following snippet returns the results of a nearest-neighbor search on the input \"What is happiness?\"\n## Incremental Updates\n\n_Incremental updates_ are a key feature of Pixeltable. Whenever a new video is added to the original table, all of its downstream computed columns are updated automatically. Let's demonstrate this by adding a third video to the table and seeing how the updates propagate through to the index.",
          "output": "                                               video  \\\n0  /Users/asiegel/.pixeltable/file_cache/ffc7607f...   \n1  /Users/asiegel/.pixeltable/file_cache/ffc7607f...   \n2  /Users/asiegel/.pixeltable/file_cache/ffc7607f...   \n\n                                            metadata  \\\n0  {'size': 959276, 'streams': [{'type': 'audio',...   \n1  {'size': 959276, 'streams': [{'type': 'audio',...   \n2  {'size': 959276, 'streams': [{'type': 'audio',...   \n\n                                  transcription_text  \n0   of experiencing self versus remembering self....  \n1   worse, the young adults had episodic memory. ...  \n2   about reusing information and making the most...  "
        },
        {
          "code": "sim = sentences_view.text.similarity('What is happiness?')\n\n(\n    sentences_view\n    .order_by(sim, asc=False)\n    .limit(20)\n    .select(sentences_view.text, similarity=sim)\n    .collect()\n)",
          "context": "We can do a simple lookup to test our new index. The following snippet returns the results of a nearest-neighbor search on the input \"What is happiness?\"\n## Incremental Updates\n\n_Incremental updates_ are a key feature of Pixeltable. Whenever a new video is added to the original table, all of its downstream computed columns are updated automatically. Let's demonstrate this by adding a third video to the table and seeing how the updates propagate through to the index.",
          "output": "                                                 text  similarity\n0   Based on the fact that our memories could be a...    0.805073\n1   These are chemicals that are released during m...    0.797971\n2   I was hoping you can give a simple answer of h...    0.792060\n3   Why would we have this period of time that's s...    0.789130\n4                                 I want to really be    0.787846\n5                       Can I be your opening answer?    0.785402\n6       of experiencing self versus remembering self.    0.785325\n7                          I need a prefrontal cortex    0.785176\n8   And maybe there is some wisdom in the fact tha...    0.784597\n9   Essentially some mechanisms for which the brai...    0.783437\n10  Attention is a big factor as well, our ability...    0.783245\n11                    What's the best way to do that?    0.783154\n12  And it's like, I realize I have to redefine wh...    0.775783\n13   about reusing information and making the most...    0.774444\n14  so I can stay focused on the big picture and t...    0.771843\n15   I don't want to be constrained by goals as much.    0.767417\n16                                        Or optimal.    0.766948\n17                 That was such a great description.    0.766004\n18  And so that's why basically again, what you se...    0.758852\n19  So one of my colleagues, Amishi Jia, she wrote...    0.756444"
        }
      ]
    },
    {
      "notebook": "use-cases/rag-operations.ipynb",
      "title": "RAG Operations in Pixeltable",
      "description": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/use-cases/rag-operations.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/use-cases/rag-operations.ipynb) In this tutorial, we'll explore Pixeltable's flexible handling of RAG operations on unstructured text. In a traditional AI workflow, such operations might be implemented as a Python script that runs on a periodic schedule or in response to certain events. In Pixeltable, as with everything else, they are implemented as persistent table operations that update incrementally as new data becomes available. In our tutorial workflow, we'll chunk Wikipedia articles in various ways with a document splitter, then apply several kinds of embeddings to the chunks. We start by installing the necessary dependencies, creating a Pixeltable directory `rag_ops_demo` (if it doesn't already exist), and setting up the table structure for our new workflow.",
      "workflows": [
        {
          "title": "Creating Tables and Views",
          "explanation": [
            "If we take a peek at the `docs` table, we see its very simple structure.",
            "Next we create a view to represent chunks of our HTML documents. A Pixeltable view is a virtual table, which is dynamically derived from a source table by applying a transformation and/or selecting a subset of data. In this case, our view represents a one-to-many transformation from source documents into individual sentences. This is achieved using Pixeltable's built-in `DocumentSplitter` class.\n\nNote that the `docs` table is currently empty, so creating this view doesn't actually *do* anything yet: it simply defines an operation that we want Pixeltable to execute when it sees new data.",
            "Let's take a peek at the new `sentences` view.",
            "We see that `sentences` inherits the `source_doc` column from `docs`, together with some new fields:\n- `pos`: The position in the source document where the sentence appears.\n-  `text`: The text of the sentence.\n- `title`, `heading`, and `sourceline`: The metadata we requested when we set up the view.",
            "## Data Ingestion",
            "Ok, now it's time to insert some data into our workflow. A document in Pixeltable is just a URL; the following command inserts a single row into the `docs` table with the `source_doc` field set to the specified URL:",
            "We can see that two things happened. First, a single row was inserted into `docs`, containing the URL representing our source document. Then, the view `sentences` was incrementally updated by applying the `DocumentSplitter` according to the definition of the view. This illustrates an important principle in Pixeltable: by default, anytime Pixeltable sees new data, the update is incrementally propagated to any downstream views or computed columns.\n\nWe can see the effect of the insertion with the `select` command. There's a single row in `docs`:",
            "And here are the first 20 rows in `sentences`. The content of the article is broken into individual sentences, as expected.",
            "## Experimenting with Chunking",
            "Of course, chunking into sentences isn't the only way to split a document. Perhaps we want to experiment with different chunking methodologies, in order to see which one performs best in a particular application. Pixeltable makes it easy to do this, by creating several views of the same source table. Here are a few examples. Notice that as each new view is created, it is initially populated from the data already in `docs`.",
            "Now let's add a few more documents to our workflow. Notice how all of the downstream views are updated incrementally, processing just the new documents as they are inserted.",
            "## Further Experiments\n\nThis is a good time to mention another important guiding principle of Pixeltable. The preceding examples all used the built-in `DocumentSplitter` class with various configurations. That's probably fine as a first cut or to prototype an application quickly, and it might be sufficient for some applications. But other applications might want to do more sophisticated kinds of chunking, implementing their own specialized logic or leveraging third-party tools. Pixeltable imposes no constraints on the AI or RAG operations a workflow uses: the iterator interface is highly general, and it's easy to implement new operations or adapt existing code or third-party tools into the Pixeltable workflow.",
            "## Computing Embeddings",
            "Next, let's look at how embedding indices can be added seamlessly to existing Pixeltable workflows. To compute our embeddings, we'll use the Huggingface `sentence_transformer` package, running it over the `chunks` view that broke our documents up into larger paragraphs. Pixeltable has a built-in `sentence_transformer` adapter, and all we have to do is add a new column that leverages it. Pixeltable takes care of the rest, applying the new column to all existing data in the view.",
            "The new column is a *computed column*: it is defined as a function on top of existing data and updated incrementally as new data are added to the workflow. Let's have a look at how the new column affected the `chunks` view.",
            "Similarly, we might want to add a CLIP embedding to our workflow; once again, it's just another computed column:"
          ],
          "code_blocks": [
            {
              "code": "docs = pxt.create_table(\n    'rag_ops_demo.docs',\n    {'source_doc': pxt.Document}\n)",
              "output": "Created table `docs`.\n",
              "explanation": ""
            },
            {
              "code": "docs",
              "output": "Table\n'rag_ops_demo.docs'\n\n Column Name      Type Computed With\n  source_doc  Document              ",
              "explanation": ""
            },
            {
              "code": "from pixeltable.iterators.document import DocumentSplitter\n\nsentences = pxt.create_view(\n    'rag_ops_demo.sentences',  # Name of the view\n    docs,  # Table from which the view is derived\n    iterator=DocumentSplitter.create(\n        document=docs.source_doc,\n        separators='sentence',  # Chunk docs into sentences\n        metadata='title,heading,sourceline'\n    )\n)",
              "output": "Created view `sentences` with 0 rows, 0 exceptions.\n",
              "explanation": ""
            },
            {
              "code": "sentences",
              "output": "View\n'rag_ops_demo.sentences'\n(of 'rag_ops_demo.docs')\n\n Column Name              Type Computed With\n         pos     Required[Int]              \n        text  Required[String]              \n       title            String              \n     heading              Json              \n  sourceline               Int              \n  source_doc          Document              ",
              "explanation": ""
            },
            {
              "code": "docs.insert([{'source_doc': 'https://en.wikipedia.org/wiki/Marc_Chagall'}])",
              "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 14.50 cells/s]\nInserting rows into `docs`: 1 rows [00:00, 739.08 rows/s]\nComputing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 2/2 [00:00<00:00, 14.20 cells/s]\nInserting rows into `sentences`: 1460 rows [00:00, 3529.45 rows/s]\nInserted 1461 rows with 0 errors.\n",
              "explanation": ""
            },
            {
              "code": "docs.select(docs.source_doc.fileurl).show()",
              "output": "                           source_doc_fileurl\n0  https://en.wikipedia.org/wiki/Marc_Chagall",
              "explanation": ""
            },
            {
              "code": "sentences.select(sentences.text, sentences.heading).show(20)",
              "output": "                                                 text                 heading\n0   Marc Chagall - Wikipedia Jump to content Searc...                      {}\n1   Marc Chagall 81 languages Afrikaans Alemannisc...  {'h1': 'Marc Chagall'}\n2   Aragon\u00e9s \u0531\u0580\u0565\u0582\u0574\u057f\u0561\u0570\u0561\u0575\u0565\u0580\u0567\u0576 Asturianu Az\u0259rbaycanca...  {'h1': 'Marc Chagall'}\n3   Hrvatski Ido Bahasa Indonesia Interlingua Ital...  {'h1': 'Marc Chagall'}\n4      T\u00fcrk\u00e7e \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 Ti\u1ebfng Vi\u1ec7t Winaray \u5434\u8bed \u05d9\u05d9\u05b4\u05d3\u05d9\u05e9  {'h1': 'Marc Chagall'}\n5   \u7cb5\u8a9e \u4e2d\u6587 Edit links From Wikipedia, the free ency...  {'h1': 'Marc Chagall'}\n6      For other uses, see Chagall (disambiguation) .  {'h1': 'Marc Chagall'}\n7                       Marc Chagall Chagall, c. 1920  {'h1': 'Marc Chagall'}\n8   Born Moishe Shagal ( 1887-07-06 ) 6 July 1887 ...  {'h1': 'Marc Chagall'}\n9       [1] Died 28 March 1985 (1985-03-28) (aged\u00a097)  {'h1': 'Marc Chagall'}\n10  Saint-Paul-de-Vence , France Nationality Russi...  {'h1': 'Marc Chagall'}\n11                                       later French  {'h1': 'Marc Chagall'}\n12  [2] Known\u00a0for Painting stained glass Notable w...  {'h1': 'Marc Chagall'}\n13          \u200b Valentina (Vava) Brodsky \u200b \u200b ( m. 1952)  {'h1': 'Marc Chagall'}\n14                                                  \u200b  {'h1': 'Marc Chagall'}\n15                                     [3] Children 2  {'h1': 'Marc Chagall'}\n16                                                [4]  {'h1': 'Marc Chagall'}\n17                                       Marc Chagall  {'h1': 'Marc Chagall'}\n18  [a] (born Moishe Shagal ; 6 July\u00a0[ O.S. 24 Jun...  {'h1': 'Marc Chagall'}\n19  [b] An early modernist , he was associated wit...  {'h1': 'Marc Chagall'}",
              "explanation": ""
            },
            {
              "code": "chunks = pxt.create_view(\n    'rag_ops_demo.chunks', docs,\n    iterator=DocumentSplitter.create(\n        document=docs.source_doc,\n        separators='paragraph,token_limit',\n        limit=2048,\n        overlap=0,\n        metadata='title,heading,sourceline'\n    )\n)",
              "output": "Inserting rows into `chunks`: 205 rows [00:00, 16715.90 rows/s]\nCreated view `chunks` with 205 rows, 0 exceptions.\n",
              "explanation": ""
            },
            {
              "code": "short_chunks = pxt.create_view(\n    'rag_ops_demo.short_chunks', docs,\n    iterator=DocumentSplitter.create(\n        document=docs.source_doc,\n        separators='paragraph,token_limit',\n        limit=72,\n        overlap=0,\n        metadata='title,heading,sourceline'\n    )\n)",
              "output": "Inserting rows into `short_chunks`: 531 rows [00:00, 23858.59 rows/s]\nCreated view `short_chunks` with 531 rows, 0 exceptions.\n",
              "explanation": ""
            },
            {
              "code": "short_char_chunks = pxt.create_view(\n    'rag_ops_demo.short_char_chunks', docs,\n    iterator=DocumentSplitter.create(\n        document=docs.source_doc,\n        separators='paragraph,char_limit',\n        limit=72,\n        overlap=0,\n        metadata='title,heading,sourceline'\n    )\n)",
              "output": "Inserting rows into `short_char_chunks`: 1764 rows [00:00, 17427.53 rows/s]\nCreated view `short_char_chunks` with 1764 rows, 0 exceptions.\n",
              "explanation": ""
            },
            {
              "code": "chunks.select(chunks.text, chunks.heading).show(20)",
              "output": "                                                 text  \\\n0   Marc Chagall - Wikipedia Jump to content Searc...   \n1   Marc Chagall 81 languages Afrikaans Alemannisc...   \n2   Marc Chagall Chagall, c. 1920 Born Moishe Shag...   \n3   Marc Chagall [a] (born Moishe Shagal ; 6 July\u00a0...   \n4   Chagall was born in 1887, into a Jewish family...   \n5   Art critic Robert Hughes referred to Chagall a...   \n6                   Early life and education [ edit ]   \n7   Early life [ edit ] Marc Chagall's childhood h...   \n8                                 Art career [ edit ]   \n9                 Russian Empire (1906\u20131910) [ edit ]   \n10  Marc Chagall was born Moishe Shagal in 1887, i...   \n11  Chagall was the eldest of nine children. The f...   \n12  Day after day, winter and summer, at six o'clo...   \n13  One of the main sources of income for the Jewi...   \n14  Chagall wrote as a boy; \"I felt at every step ...   \n15  Most of what is known about Chagall's early li...   \n16  Chagall's art can be understood as the respons...   \n17  Art education [ edit ] Portrait of Chagall by ...   \n18  In the Russian Empire at that time, Jewish chi...   \n19  A turning point of his artistic life came when...   \n\n                                              heading  \n0                                                  {}  \n1                              {'h1': 'Marc Chagall'}  \n2                              {'h1': 'Marc Chagall'}  \n3                              {'h1': 'Marc Chagall'}  \n4                              {'h1': 'Marc Chagall'}  \n5                              {'h1': 'Marc Chagall'}  \n6   {'h1': 'Marc Chagall', 'h2': 'Early life and e...  \n7   {'h1': 'Marc Chagall', 'h2': 'Early life and e...  \n8    {'h1': 'Marc Chagall', 'h2': 'Art career[edit]'}  \n9   {'h1': 'Marc Chagall', 'h2': 'Art career[edit]...  \n10  {'h1': 'Marc Chagall', 'h2': 'Early life and e...  \n11  {'h1': 'Marc Chagall', 'h2': 'Early life and e...  \n12  {'h1': 'Marc Chagall', 'h2': 'Early life and e...  \n13  {'h1': 'Marc Chagall', 'h2': 'Early life and e...  \n14  {'h1': 'Marc Chagall', 'h2': 'Early life and e...  \n15  {'h1': 'Marc Chagall', 'h2': 'Early life and e...  \n16  {'h1': 'Marc Chagall', 'h2': 'Early life and e...  \n17  {'h1': 'Marc Chagall', 'h2': 'Early life and e...  \n18  {'h1': 'Marc Chagall', 'h2': 'Early life and e...  \n19  {'h1': 'Marc Chagall', 'h2': 'Early life and e...  ",
              "explanation": ""
            },
            {
              "code": "short_chunks.select(short_chunks.text, short_chunks.heading).show(20)",
              "output": "                                                 text  \\\n0   Marc Chagall - Wikipedia Jump to content Searc...   \n1   Marc Chagall 81 languages Afrikaans Alemannisc...   \n2   \u043e\u0440\u0442\u0441\u0430 \u0411\u0435\u043b\u0430\u0440\u0443\u0441\u043a\u0430\u044f \u0411\u0435\u043b\u0430\u0440\u0443\u0441\u043a\u0430\u044f (\u0442\u0430\u0440\u0430\u0448\u043a\u0435\u0432\u0456\u0446\u0430) \u0411\u044a\u043b\u0433...   \n3   \uad6d\uc5b4 \u0540\u0561\u0575\u0565\u0580\u0565\u0576 \u0939\u093f\u0928\u094d\u0926\u0940 Hrvatski Ido Bahasa Indonesi...   \n4   tzebuergesch Lietuvi\u0173 Magyar \u041c\u0430\u043a\u0435\u0434\u043e\u043d\u0441\u043a\u0438 Malaga...   \n5   \u00e8is Plattd\u00fc\u00fctsch Polski Portugu\u00eas Rom\u00e2n\u0103 Runa ...   \n6    Svenska \u0e44\u0e17\u0e22 T\u00fcrk\u00e7e \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 Ti\u1ebfng Vi\u1ec7t Wina...   \n7    . For other uses, see Chagall (disambiguation) .   \n8   Marc Chagall Chagall, c. 1920 Born Moishe Shag...   \n9   28) (aged\u00a097) Saint-Paul-de-Vence , France Nat...   \n10  entina (Vava) Brodsky \u200b \u200b ( m. 1952) \u200b [3] Chi...   \n11  Marc Chagall [a] (born Moishe Shagal ; 6 July\u00a0...   \n12   works in a wide range of artistic formats, in...   \n13  Chagall was born in 1887, into a Jewish family...   \n14   ideas of Eastern European and Jewish folklore...   \n15                                               :\u200a10   \n16   in 1923. During World War II , he escaped occ...   \n17  Art critic Robert Hughes referred to Chagall a...   \n18   Using the medium of stained glass, he produce...   \n19   modernism's \"golden age\" in Paris, where \"he ...   \n\n                                              heading  \n0                                                  {}  \n1                              {'h1': 'Marc Chagall'}  \n2                              {'h1': 'Marc Chagall'}  \n3                              {'h1': 'Marc Chagall'}  \n4                              {'h1': 'Marc Chagall'}  \n5                              {'h1': 'Marc Chagall'}  \n6                              {'h1': 'Marc Chagall'}  \n7                              {'h1': 'Marc Chagall'}  \n8                              {'h1': 'Marc Chagall'}  \n9                              {'h1': 'Marc Chagall'}  \n10                             {'h1': 'Marc Chagall'}  \n11                             {'h1': 'Marc Chagall'}  \n12                             {'h1': 'Marc Chagall'}  \n13                             {'h1': 'Marc Chagall'}  \n14                             {'h1': 'Marc Chagall'}  \n15  {'h1': 'Marc Chagall', 'h2': 'Art career[edit]...  \n16                             {'h1': 'Marc Chagall'}  \n17                             {'h1': 'Marc Chagall'}  \n18                             {'h1': 'Marc Chagall'}  \n19                             {'h1': 'Marc Chagall'}  ",
              "explanation": ""
            },
            {
              "code": "short_char_chunks.select(short_char_chunks.text, short_char_chunks.heading).show(20)",
              "output": "                                                 text                 heading\n0   Marc Chagall - Wikipedia Jump to content Searc...                      {}\n1   Marc Chagall 81 languages Afrikaans Alemannisc...  {'h1': 'Marc Chagall'}\n2   \u0570\u0561\u0575\u0565\u0580\u0567\u0576 Asturianu Az\u0259rbaycanca \u09ac\u09be\u0982\u09b2\u09be \u0411\u0430\u0448\u04a1\u043e\u0440\u0442\u0441\u0430...  {'h1': 'Marc Chagall'}\n3   \u0440\u0430\u0448\u043a\u0435\u0432\u0456\u0446\u0430) \u0411\u044a\u043b\u0433\u0430\u0440\u0441\u043a\u0438 Catal\u00e0 \u010ce\u0161tina Cymraeg Da...  {'h1': 'Marc Chagall'}\n4    Espa\u00f1ol Esperanto Euskara \u0641\u0627\u0631\u0633\u06cc Fran\u00e7ais Gale...  {'h1': 'Marc Chagall'}\n5   tski Ido Bahasa Indonesia Interlingua Italiano...  {'h1': 'Marc Chagall'}\n6   ili Latina Latvie\u0161u L\u00ebtzebuergesch Lietuvi\u0173 Ma...  {'h1': 'Marc Chagall'}\n7   \u0635\u0631\u0649 Nederlands Nedersaksies \u65e5\u672c\u8a9e Norsk bokm\u00e5l N...  {'h1': 'Marc Chagall'}\n8   kcha / \u045e\u0437\u0431\u0435\u043a\u0447\u0430 \u067e\u0646\u062c\u0627\u0628\u06cc Picard Piemont\u00e8is Plattd...  {'h1': 'Marc Chagall'}\n9   m\u00e2n\u0103 Runa Simi \u0420\u0443\u0441\u0441\u043a\u0438\u0439 Scots Shqip Sicilianu S...  {'h1': 'Marc Chagall'}\n10  loven\u0161\u010dina \u06a9\u0648\u0631\u062f\u06cc \u0421\u0440\u043f\u0441\u043a\u0438 / srpski Srpskohrvatsk...  {'h1': 'Marc Chagall'}\n11  venska \u0e44\u0e17\u0e22 T\u00fcrk\u00e7e \u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430 Ti\u1ebfng Vi\u1ec7t Winara...  {'h1': 'Marc Chagall'}\n12  ks From Wikipedia, the free encyclopedia Russi...  {'h1': 'Marc Chagall'}\n13  5) \"Chagall\" redirects here. For other uses, s...  {'h1': 'Marc Chagall'}\n14                                                ) .  {'h1': 'Marc Chagall'}\n15  Marc Chagall Chagall, c. 1920 Born Moishe Shag...  {'h1': 'Marc Chagall'}\n16  887 (N.S.) Liozna , Vitebsk Governorate , Russ...  {'h1': 'Marc Chagall'}\n17  1] Died 28 March 1985 (1985-03-28) (aged\u00a097) S...  {'h1': 'Marc Chagall'}\n18  e Nationality Russian Empire, later French [2]...  {'h1': 'Marc Chagall'}\n19  d glass Notable work See list of artworks by M...  {'h1': 'Marc Chagall'}",
              "explanation": ""
            },
            {
              "code": "urls = [\n    'https://en.wikipedia.org/wiki/Pierre-Auguste_Renoir',\n    'https://en.wikipedia.org/wiki/Henri_Matisse',\n    'https://en.wikipedia.org/wiki/Marcel_Duchamp'\n]\ndocs.insert({'source_doc': url} for url in urls)",
              "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:00<00:00, 53.63 cells/s]\nInserting rows into `docs`: 3 rows [00:00, 2365.21 rows/s]\nComputing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 6/6 [00:00<00:00, 52.39 cells/s]\nInserting rows into `sentences`: 2106 rows [00:02, 783.63 rows/s]\nInserting rows into `chunks`: 276 rows [00:00, 15888.61 rows/s]\nInserting rows into `short_chunks`: 812 rows [00:00, 22184.42 rows/s]\nInserting rows into `short_char_chunks`: 2638 rows [00:00, 13227.11 rows/s]\nInserted 5835 rows with 0 errors.\n",
              "explanation": ""
            },
            {
              "code": "from pixeltable.functions.huggingface import sentence_transformer\n\nchunks.add_computed_column(minilm_embed=sentence_transformer(\n    chunks.text,\n    model_id='paraphrase-MiniLM-L6-v2'\n))",
              "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 481/481 [00:02<00:00, 222.59 cells/s]\nAdded 481 column values with 0 errors.\n",
              "explanation": ""
            },
            {
              "code": "chunks",
              "output": "View\n'rag_ops_demo.chunks'\n(of 'rag_ops_demo.docs')\n\n   Column Name                            Type                                      Computed With\n           pos                   Required[Int]                                                   \n          text                Required[String]                                                   \n         title                          String                                                   \n       heading                            Json                                                   \n    sourceline                             Int                                                   \n  minilm_embed  Required[Array[(384,), Float]]  sentence_transformer(text, model_id='paraphras...\n    source_doc                        Document                                                   ",
              "explanation": ""
            },
            {
              "code": "chunks.select(chunks.text, chunks.heading, chunks.minilm_embed).head()",
              "output": "                                                text  \\\n0  Marc Chagall - Wikipedia Jump to content Searc...   \n1  Marc Chagall 81 languages Afrikaans Alemannisc...   \n2  Marc Chagall Chagall, c. 1920 Born Moishe Shag...   \n3  Marc Chagall [a] (born Moishe Shagal ; 6 July\u00a0...   \n4  Chagall was born in 1887, into a Jewish family...   \n5  Art critic Robert Hughes referred to Chagall a...   \n6                  Early life and education [ edit ]   \n7  Early life [ edit ] Marc Chagall's childhood h...   \n8  Marc Chagall was born Moishe Shagal in 1887, i...   \n9  Chagall was the eldest of nine children. The f...   \n\n                                             heading  \\\n0                                                 {}   \n1                             {'h1': 'Marc Chagall'}   \n2                             {'h1': 'Marc Chagall'}   \n3                             {'h1': 'Marc Chagall'}   \n4                             {'h1': 'Marc Chagall'}   \n5                             {'h1': 'Marc Chagall'}   \n6  {'h1': 'Marc Chagall', 'h2': 'Early life and e...   \n7  {'h1': 'Marc Chagall', 'h2': 'Early life and e...   \n8  {'h1': 'Marc Chagall', 'h2': 'Early life and e...   \n9  {'h1': 'Marc Chagall', 'h2': 'Early life and e...   \n\n                                        minilm_embed  \n0  [-0.2623971, -0.11875597, -0.1327094, 0.048251...  \n1  [-0.13631284, 0.40063256, -0.5300299, -0.18143...  \n2  [-0.0047689965, 0.33990884, -0.3152904, 0.1701...  \n3  [0.052763388, 0.13830872, -0.21864271, 0.19172...  \n4  [0.0128892455, 0.24784817, -0.69244295, 0.1426...  \n5  [-0.17184898, 0.34802842, -0.30670404, 0.03375...  \n6  [-0.21258691, 0.4176262, 0.09400387, 0.1349991...  \n7  [-0.04040356, 0.1428114, -0.3568075, 0.4118173...  \n8  [0.12289606, 0.19771104, -0.4960996, 0.1543681...  \n9  [-0.19016075, 0.26621026, -0.4000805, 0.129193...  ",
              "explanation": ""
            },
            {
              "code": "from pixeltable.functions.huggingface import clip\n\nchunks.add_computed_column(clip_embed=clip(\n    chunks.text, model_id='openai/clip-vit-base-patch32'\n))",
              "output": "Computing cells: 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 481/481 [00:05<00:00, 93.49 cells/s]\nAdded 481 column values with 0 errors.\n",
              "explanation": ""
            },
            {
              "code": "chunks",
              "output": "View\n'rag_ops_demo.chunks'\n(of 'rag_ops_demo.docs')\n\n   Column Name                            Type                                      Computed With\n           pos                   Required[Int]                                                   \n          text                Required[String]                                                   \n         title                          String                                                   \n       heading                            Json                                                   \n    sourceline                             Int                                                   \n  minilm_embed  Required[Array[(384,), Float]]  sentence_transformer(text, model_id='paraphras...\n    clip_embed  Required[Array[(512,), Float]]  clip(text, model_id='openai/clip-vit-base-patc...\n    source_doc                        Document                                                   ",
              "explanation": ""
            },
            {
              "code": "chunks.select(chunks.text, chunks.heading, chunks.clip_embed).head()",
              "output": "                                                text  \\\n0  Marc Chagall - Wikipedia Jump to content Searc...   \n1  Marc Chagall 81 languages Afrikaans Alemannisc...   \n2  Marc Chagall Chagall, c. 1920 Born Moishe Shag...   \n3  Marc Chagall [a] (born Moishe Shagal ; 6 July\u00a0...   \n4  Chagall was born in 1887, into a Jewish family...   \n5  Art critic Robert Hughes referred to Chagall a...   \n6                  Early life and education [ edit ]   \n7  Early life [ edit ] Marc Chagall's childhood h...   \n8  Marc Chagall was born Moishe Shagal in 1887, i...   \n9  Chagall was the eldest of nine children. The f...   \n\n                                             heading  \\\n0                                                 {}   \n1                             {'h1': 'Marc Chagall'}   \n2                             {'h1': 'Marc Chagall'}   \n3                             {'h1': 'Marc Chagall'}   \n4                             {'h1': 'Marc Chagall'}   \n5                             {'h1': 'Marc Chagall'}   \n6  {'h1': 'Marc Chagall', 'h2': 'Early life and e...   \n7  {'h1': 'Marc Chagall', 'h2': 'Early life and e...   \n8  {'h1': 'Marc Chagall', 'h2': 'Early life and e...   \n9  {'h1': 'Marc Chagall', 'h2': 'Early life and e...   \n\n                                          clip_embed  \n0  [0.43900785, -0.20442092, -0.3703841, -0.24842...  \n1  [0.1057676, 0.0062544323, -0.15221544, 0.04325...  \n2  [0.3011226, -0.2156774, -0.18620874, -0.097714...  \n3  [0.2906701, -0.17377448, -0.13740699, -0.06696...  \n4  [0.32602394, 0.06719909, -0.06100519, -0.04561...  \n5  [0.37371925, 0.03151837, -0.31062487, -0.09309...  \n6  [-0.11143046, -0.31773415, 0.043277875, 0.0328...  \n7  [-0.210673, 0.18138997, -0.23682432, 0.1584470...  \n8  [0.27152574, -0.1369829, -0.2572192, -0.086759...  \n9  [0.18217696, -0.07319681, -0.19468287, -0.1496...  ",
              "explanation": ""
            }
          ]
        }
      ],
      "api_usage": {},
      "key_concepts": [
        "incremental",
        "embedding",
        "iterator",
        "computed column",
        "tool calling",
        "multimodal"
      ],
      "code_snippets": [
        {
          "code": "%pip install -qU pixeltable sentence-transformers spacy tiktoken",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/use-cases/rag-operations.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/use-cases/rag-operations.ipynb)\n\n# RAG Operations in Pixeltable\n\nIn this tutorial, we'll explore Pixeltable's flexible handling of RAG operations on unstructured text. In a traditional AI workflow, such operations might be implemented as a Python script that runs on a periodic schedule or in response to certain events. In Pixeltable, as with everything else, they are implemented as persistent table operations that update incrementally as new data becomes available. In our tutorial workflow, we'll chunk Wikipedia articles in various ways with a document splitter, then apply several kinds of embeddings to the chunks.\n\n## Set Up the Table Structure\n\nWe start by installing the necessary dependencies, creating a Pixeltable directory `rag_ops_demo` (if it doesn't already exist), and setting up the table structure for our new workflow.",
          "output": null
        },
        {
          "code": "import pixeltable as pxt\n\n# Ensure a clean slate for the demo\npxt.drop_dir('rag_ops_demo', force=True)\n# Create the Pixeltable workspace\npxt.create_dir('rag_ops_demo')",
          "context": "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/use-cases/rag-operations.ipynb)[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/use-cases/rag-operations.ipynb)\n\n# RAG Operations in Pixeltable\n\nIn this tutorial, we'll explore Pixeltable's flexible handling of RAG operations on unstructured text. In a traditional AI workflow, such operations might be implemented as a Python script that runs on a periodic schedule or in response to certain events. In Pixeltable, as with everything else, they are implemented as persistent table operations that update incrementally as new data becomes available. In our tutorial workflow, we'll chunk Wikipedia articles in various ways with a document splitter, then apply several kinds of embeddings to the chunks.\n\n## Set Up the Table Structure\n\nWe start by installing the necessary dependencies, creating a Pixeltable directory `rag_ops_demo` (if it doesn't already exist), and setting up the table structure for our new workflow.",
          "output": "Connected to Pixeltable database at: postgresql+psycopg://postgres:@/pixeltable?host=/Users/asiegel/.pixeltable/pgdata\nCreated directory `rag_ops_demo`.\n"
        }
      ]
    }
  ]
}