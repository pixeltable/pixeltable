{
  "@context": "https://pixeltable.com/learn",
  "@type": "Tutorial",
  "@id": "object-detection-in-videos",
  "github_url": "https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/use-cases/object-detection-in-videos.ipynb",
  "title": "Object Detection in Videos",
  "objective": "Learn to detect objects in videos using YOLOX models, extract frames, and compare model performance",
  "difficulty": "intermediate",
  "categories": ["video-processing", "object-detection", "yolox", "model-evaluation", "frame-extraction"],
  "prerequisites": ["pixeltable-basics", "computed-columns"],
  "imports_required": ["pixeltable", "pixeltable-yolox", "pixeltable.iterators.FrameIterator", "pixeltable.functions.yolox", "pixeltable.functions.vision"],
  "performance_notes": {
    "typical_runtime": "10-15 minutes with GPU, 30+ minutes with CPU",
    "resource_requirements": "GPU recommended (Colab GPU instance), ~2GB model downloads, ~500MB storage for frames"
  },
  "key_learnings": [
    "Component views create rows for each frame without storing them physically",
    "FrameIterator handles automatic frame extraction from videos",
    "Computed columns can run complex ML models on every frame",
    "Model outputs can be reassembled back into videos",
    "Evaluation metrics like mAP can be computed against ground truth"
  ],
  "steps": [
    {
      "number": 1,
      "section_title": "Creating a tutorial directory and table",
      "intent": "Set up Pixeltable structure for video processing",
      "code": "import pixeltable as pxt\n\npxt.create_dir('detection_demo', if_exists='replace_force')\nvideos_table = pxt.create_table(\n    'detection_demo.videos',\n    {'video': pxt.Video}\n)",
      "imports_used": ["pixeltable"],
      "explanation": "Videos need a dedicated table with pxt.Video column type. The if_exists parameter handles reruns cleanly.",
      "actual_output": "Created directory 'detection_demo'.\nCreated table `videos`.",
      "output_type": "text",
      "learns": ["video table creation", "pxt.Video type", "directory management"],
      "gotchas": ["Must use pxt.Video not generic column type", "replace_force will delete existing data"],
      "performance": "Instant",
      "alternatives": null
    },
    {
      "number": 2,
      "section_title": "Creating a frame iterator view",
      "intent": "Create a view that automatically extracts frames from videos",
      "code": "from pixeltable.iterators import FrameIterator\n\nframes_view = pxt.create_view(\n    'detection_demo.frames',\n    videos_table,\n    # `fps` determines the frame rate; a value of `0`\n    # indicates the native frame rate of the video.\n    iterator=FrameIterator.create(video=videos_table.video, fps=0)\n)",
      "imports_used": ["pixeltable", "pixeltable.iterators.FrameIterator"],
      "explanation": "FrameIterator creates one row per frame. fps=0 uses native frame rate. Frames are extracted on-demand, not stored.",
      "actual_output": "Created view `frames` with 0 rows, 0 exceptions.",
      "output_type": "text",
      "learns": ["component views", "frame extraction", "lazy evaluation", "fps parameter"],
      "gotchas": ["fps=0 means native rate not zero fps", "frames not physically stored", "view initially empty until video inserted"],
      "performance": "Instant creation, extraction on demand",
      "alternatives": "Could use fps=1 for 1 frame per second, fps=30 for standard rate"
    },
    {
      "number": 3,
      "intent": "Insert video data via URL",
      "code": "videos_table.insert([\n    {\n        'video': 'https://raw.github.com/pixeltable/pixeltable/release/docs/resources/bangkok.mp4'\n    }\n])",
      "imports_used": ["pixeltable"],
      "explanation": "Single video insertion triggers automatic frame extraction in the view. Bangkok intersection video has 461 frames.",
      "actual_output": "Inserting rows into `videos`: 1 rows [00:00, 321.33 rows/s]\nInserting rows into `frames`: 461 rows [00:00, 14957.29 rows/s]\nInserted 462 rows with 0 errors.",
      "output_type": "text",
      "learns": ["automatic view updates", "frame count expansion", "URL video loading"],
      "gotchas": ["Video downloaded and cached locally", "Frame count depends on video length and fps"],
      "performance": "~1 second for download and frame indexing",
      "alternatives": "Can insert local files or S3 URLs"
    },
    {
      "number": 4,
      "intent": "Examine frame structure and metadata",
      "code": "frames_view.select(\n    frames_view.pos,\n    frames_view.frame,\n    frames_view.frame.width,\n    frames_view.frame.height\n).show(5)",
      "imports_used": ["pixeltable"],
      "explanation": "Frames have position index, PIL Image objects, and dimensional metadata. Each frame is 1280x720.",
      "actual_output": "   pos                                              frame  width  height\n0    0  <PIL.Image.Image image mode=RGB size=1280x720 ...   1280     720\n1    1  <PIL.Image.Image image mode=RGB size=1280x720 ...   1280     720",
      "output_type": "table",
      "learns": ["frame metadata access", "PIL Image integration", "position tracking"],
      "gotchas": ["Frames extracted on-demand during select", "PIL objects not raw bytes"],
      "performance": "~100ms per frame extraction",
      "alternatives": null
    },
    {
      "number": 5,
      "section_title": "Object Detection with Pixeltable",
      "intent": "Test YOLOX object detection on sample frames",
      "code": "from pixeltable.functions.yolox import yolox\n\n# Show the results of applying the `yolox_tiny` model\n# to the first few frames in the table.\n\nframes_view.select(\n    frames_view.frame,\n    yolox(frames_view.frame, model_id='yolox_tiny')\n).head(3)",
      "imports_used": ["pixeltable", "pixeltable.functions.yolox"],
      "explanation": "YOLOX returns JSON with bboxes, scores, labels. Lazy evaluation means only 3 frames processed despite full table select.",
      "actual_output": "frame                                               yolox  \n<PIL.Image...>  {'bboxes': [(338.189, 345.599...), ...], 'scores': [...], 'labels': [...]}",
      "output_type": "table",
      "learns": ["YOLOX integration", "lazy evaluation", "JSON detection output structure"],
      "gotchas": ["First run downloads model (~50MB)", "Only requested rows processed", "Returns complex nested JSON"],
      "performance": "First run: ~30s for model download, then ~200ms per frame",
      "alternatives": "Can use yolox_m, yolox_l, yolox_x for better accuracy"
    },
    {
      "number": 6,
      "intent": "Add computed column for persistent detection",
      "code": "# Create a computed column to compute detections using the `yolox_tiny`\n# model.\n# We'll adjust the confidence threshold down a bit (the default is 0.5)\n# to pick up even more bounding boxes.\n\nframes_view.add_computed_column(detections_tiny=yolox(\n    frames_view.frame, model_id='yolox_tiny', threshold=0.25\n))",
      "imports_used": ["pixeltable", "pixeltable.functions.yolox"],
      "explanation": "Computed column runs inference on ALL 461 frames and stores results. Threshold lowered to 0.25 for more detections.",
      "actual_output": "Added 461 column values with 0 errors.",
      "output_type": "text",
      "learns": ["computed column storage", "threshold tuning", "batch inference"],
      "gotchas": ["Processes entire table immediately", "Results cached permanently", "Lower threshold = more false positives"],
      "performance": "~90 seconds for 461 frames on GPU",
      "alternatives": "Could add incrementally with where clause"
    },
    {
      "number": 7,
      "intent": "Visualize detections with bounding boxes",
      "code": "import pixeltable.functions as pxtf\n\nframes_view.select(\n    frames_view.frame,\n    pxtf.vision.draw_bounding_boxes(\n        frames_view.frame,\n        frames_view.detections_tiny.bboxes,\n        width=4\n    )\n).show(1)",
      "imports_used": ["pixeltable", "pixeltable.functions"],
      "explanation": "draw_bounding_boxes overlays detection boxes on frames. Computed on-demand, not stored.",
      "actual_output": "frame                     draw_bounding_boxes\n<PIL.Image...>           <PIL.Image with boxes drawn>",
      "output_type": "table",
      "learns": ["visualization functions", "bbox overlay", "on-demand computation"],
      "gotchas": ["Boxes drawn at retrieval time", "Width parameter is line thickness"],
      "performance": "~50ms per frame for drawing",
      "alternatives": "Could save as computed column if needed frequently"
    },
    {
      "number": 8,
      "intent": "Reassemble frames into annotated video",
      "code": "frames_view.group_by(videos_table).select(\n    pxt.functions.video.make_video(\n        frames_view.pos,\n        pxtf.vision.draw_bounding_boxes(\n            frames_view.frame,\n            frames_view.detections_tiny.bboxes,\n            width=4\n        )\n    )\n).show(1)",
      "imports_used": ["pixeltable", "pixeltable.functions"],
      "explanation": "make_video aggregates annotated frames back into video file. Group by ensures one video per source.",
      "actual_output": "make_video\n/Users/asiegel/.pixeltable/tmp/tmpipcu8dqy.mp4",
      "output_type": "text",
      "learns": ["video reconstruction", "aggregation functions", "group_by usage"],
      "gotchas": ["Creates temporary file", "Must group_by source video", "Frame order matters (use pos)"],
      "performance": "~20 seconds for 461 frames",
      "alternatives": "Could save to specific path with additional parameters"
    },
    {
      "number": 9,
      "section_title": "Comparing Object Detection Models",
      "intent": "Add medium YOLOX model for comparison",
      "code": "# Here we use the larger `yolox_m` (medium) model.\n\nframes_view.add_computed_column(detections_m=yolox(\n    frames_view.frame, model_id='yolox_m', threshold=0.25\n))",
      "imports_used": ["pixeltable", "pixeltable.functions.yolox"],
      "explanation": "Larger model provides better detection quality with less flickering. Model is ~200MB download.",
      "actual_output": "Added 461 column values with 0 errors.",
      "output_type": "text",
      "learns": ["model comparison", "multiple computed columns", "quality vs speed tradeoff"],
      "gotchas": ["Larger model = slower inference", "More storage for results", "First run downloads new model"],
      "performance": "~3 minutes for 461 frames on GPU",
      "alternatives": "yolox_l and yolox_x available for even better quality"
    },
    {
      "number": 10,
      "intent": "Generate side-by-side comparison videos",
      "code": "frames_view.group_by(videos_table).select(\n    pxt.functions.video.make_video(\n        frames_view.pos,\n        pxtf.vision.draw_bounding_boxes(\n            frames_view.frame,\n            frames_view.detections_tiny.bboxes,\n            width=4\n        )\n    ),\n    pxt.functions.video.make_video(\n        frames_view.pos,\n        pxtf.vision.draw_bounding_boxes(\n            frames_view.frame,\n            frames_view.detections_m.bboxes,\n            width=4\n        )\n    )\n).show(1)",
      "imports_used": ["pixeltable", "pixeltable.functions"],
      "explanation": "Creates two videos for visual comparison. Medium model shows more stable boxes with less flickering.",
      "actual_output": "make_video                                    make_video_1\n/tmp/tmp3o4znodf.mp4                         /tmp/tmpuxk57w0m.mp4",
      "output_type": "table",
      "learns": ["multi-column video generation", "model comparison visualization"],
      "gotchas": ["Generates multiple temp files", "Need video player to compare side-by-side"],
      "performance": "~40 seconds for both videos",
      "alternatives": "Could concatenate videos with ffmpeg for true side-by-side"
    },
    {
      "number": 11,
      "section_title": "Evaluating Models Against a Ground Truth",
      "intent": "Create ground truth with largest model",
      "code": "frames_view.add_computed_column(detections_x=yolox(\n    frames_view.frame, model_id='yolox_x', threshold=0.25\n))",
      "imports_used": ["pixeltable", "pixeltable.functions.yolox"],
      "explanation": "YOLOX-X is the largest model (~800MB), used as synthetic ground truth for evaluation.",
      "actual_output": "Added 461 column values with 0 errors.",
      "output_type": "text",
      "learns": ["ground truth generation", "synthetic evaluation data"],
      "gotchas": ["Very large model download", "Slowest inference", "Not real ground truth but good approximation"],
      "performance": "~5-7 minutes on GPU",
      "alternatives": "Could use manual annotations for real ground truth"
    },
    {
      "number": 12,
      "intent": "Compute evaluation metrics for each model",
      "code": "from pixeltable.functions.vision import eval_detections, mean_ap\n\nframes_view.add_computed_column(eval_yolox_tiny=eval_detections(\n    pred_bboxes=frames_view.detections_tiny.bboxes,\n    pred_labels=frames_view.detections_tiny.labels,\n    pred_scores=frames_view.detections_tiny.scores,\n    gt_bboxes=frames_view.detections_x.bboxes,\n    gt_labels=frames_view.detections_x.labels\n))\n\nframes_view.add_computed_column(eval_yolox_m=eval_detections(\n    pred_bboxes=frames_view.detections_m.bboxes,\n    pred_labels=frames_view.detections_m.labels,\n    pred_scores=frames_view.detections_m.scores,\n    gt_bboxes=frames_view.detections_x.bboxes,\n    gt_labels=frames_view.detections_x.labels\n))",
      "imports_used": ["pixeltable", "pixeltable.functions.vision"],
      "explanation": "eval_detections computes per-frame true/false positives for mAP calculation. Compares each model against ground truth.",
      "actual_output": "Added 461 column values with 0 errors.\nAdded 461 column values with 0 errors.",
      "output_type": "text",
      "learns": ["evaluation metrics", "TP/FP calculation", "multi-field JSON access"],
      "gotchas": ["Must match pred and gt field names exactly", "Returns complex nested structure"],
      "performance": "~10 seconds per model",
      "alternatives": "Could evaluate against manual annotations"
    },
    {
      "number": 13,
      "intent": "Calculate mean average precision (mAP)",
      "code": "frames_view.select(\n    mean_ap(frames_view.eval_yolox_tiny),\n    mean_ap(frames_view.eval_yolox_m)\n).show()",
      "imports_used": ["pixeltable", "pixeltable.functions.vision"],
      "explanation": "mean_ap aggregates per-frame metrics into per-class mAP scores. Higher is better. Medium model significantly outperforms tiny.",
      "actual_output": "mean_ap                                          mean_ap_1\n{0: 0.1007, 2: 0.6215, 5: 0.2341, ...}         {0: 0.5635, 2: 0.9112, 5: 0.7823, ...}",
      "output_type": "table",
      "learns": ["mAP calculation", "aggregation functions", "model performance comparison"],
      "gotchas": ["Per-class scores in dictionary", "Class IDs are COCO dataset labels", "Requires ground truth"],
      "performance": "~1 second",
      "alternatives": "Could compute per-video or filtered subsets"
    }
  ],
  "patterns": [
    {
      "name": "video_frame_pipeline",
      "description": "Extract frames → Process each frame → Reassemble video",
      "code_template": "frames = pxt.create_view('frames', videos, iterator=FrameIterator(video=videos.video))\nframes.add_computed_column(processed=func(frames.frame))\nvideo = frames.group_by(videos).select(make_video(frames.pos, frames.processed))",
      "variations": ["Different fps rates", "Multiple processing steps", "Conditional frame selection"],
      "reusable": true
    },
    {
      "name": "model_comparison",
      "description": "Run multiple models on same data for comparison",
      "code_template": "view.add_computed_column(model1_output=model1(view.input))\nview.add_computed_column(model2_output=model2(view.input))\nview.select(metric(model1_output), metric(model2_output))",
      "variations": ["Different models", "Different parameters", "Different metrics"],
      "reusable": true
    },
    {
      "name": "lazy_visualization",
      "description": "Apply visualizations on-demand without storing",
      "code_template": "view.select(draw_function(view.data, view.annotations)).show(n)",
      "variations": ["Different drawing functions", "Multiple overlays", "Conditional rendering"],
      "reusable": true
    },
    {
      "name": "ground_truth_evaluation",
      "description": "Evaluate predictions against ground truth with metrics",
      "code_template": "view.add_computed_column(eval=eval_detections(pred=view.predictions, gt=view.ground_truth))\nview.select(mean_ap(view.eval))",
      "variations": ["Different metrics", "Per-class evaluation", "Filtered subsets"],
      "reusable": true
    }
  ],
  "common_errors": [
    {
      "error_type": "TypeError: 'NoneType' object is not subscriptable",
      "cause": "Trying to access detection fields when no objects detected",
      "solution": "Check if detections exist before accessing fields",
      "example": "if detections and detections['bboxes']: ..."
    },
    {
      "error_type": "Model download timeout",
      "cause": "Large model download on slow connection",
      "solution": "Retry or pre-download models",
      "example": "Models cached in ~/.cache/torch/hub/"
    },
    {
      "error_type": "CUDA out of memory",
      "cause": "GPU memory insufficient for model",
      "solution": "Use smaller model or CPU inference",
      "example": "Use yolox_tiny instead of yolox_x"
    },
    {
      "error_type": "Frame extraction hanging",
      "cause": "Corrupted video file or codec issues",
      "solution": "Verify video file integrity, try different format",
      "example": "ffmpeg -i video.mp4 -c copy fixed.mp4"
    }
  ],
  "test_questions": [
    "How do I extract frames from a video?",
    "How do I run object detection on video frames?",
    "How do I compare different YOLOX models?",
    "How do I turn processed frames back into a video?",
    "How do I calculate mAP for object detection?",
    "What's the difference between stored and on-demand computation?",
    "How do I handle different frame rates?"
  ],
  "power_tips": [
    "FrameIterator with fps=0 preserves original frame rate",
    "Computed columns cache results, select expressions compute on-demand",
    "Group_by is essential for video reconstruction from frames",
    "eval_detections + mean_ap provides standard COCO evaluation metrics",
    "Model downloads are cached in ~/.cache/torch/hub/",
    "Use threshold parameter to balance precision vs recall",
    "Lazy evaluation means you can experiment without processing everything"
  ],
  "cookies": "🍪"
}