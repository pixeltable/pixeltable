{
  "@context": "https://pixeltable.com/learn",
  "@type": "Tutorial",
  "@id": "huggingface-integration",
  "github_url": "https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-hugging-face.ipynb",
  "title": "Working with Hugging Face Models and Datasets",
  "objective": "Master local model execution and dataset integration using Hugging Face's ecosystem within Pixeltable workflows",
  "difficulty": "intermediate",
  "categories": ["local-models", "datasets", "embeddings", "computer-vision", "no-api-cost"],
  "prerequisites": ["pixeltable-basics", "python-environment", "local-compute-resources"],
  "imports_required": [
    "pixeltable as pxt",
    "pixeltable.functions.huggingface.clip",
    "datasets",
    "torch",
    "transformers",
    "tiktoken",
    "spacy",
    "PIL.Image"
  ],
  "performance_notes": {
    "typical_runtime": "5-15 minutes including model downloads",
    "resource_requirements": "4-8GB RAM for models, GPU recommended for large models, 2-10GB disk space",
    "bottlenecks": ["initial model downloads", "local computation", "embedding computation for large datasets"]
  },
  "key_learnings": [
    "Hugging Face models run locally without API costs or rate limits",
    "Dataset integration preserves train/test/validation splits automatically",
    "Embedding indexes can be created with local models like CLIP",
    "Local execution provides privacy and control over model behavior",
    "Performance depends heavily on local hardware capabilities",
    "Model downloads happen once and are cached locally",
    "Different model families require different computational resources"
  ],
  "relationships": {
    "builds_on": ["table-creation", "computed-columns", "data-import"],
    "enables": ["privacy-preserving-ai", "cost-effective-embeddings", "offline-ai-workflows"],
    "see_also": ["embedding-indexes", "computer-vision"],
    "contrasts_with": ["openai-integration", "anthropic-integration", "api-based-workflows"]
  },
  "steps": [
    {
      "number": 1,
      "section_title": "Installation and Dependencies",
      "intent": "Install comprehensive ML stack for local model execution",
      "code": "%pip install -qU pixeltable datasets torch transformers tiktoken spacy",
      "imports_used": [],
      "explanation": "Install full Hugging Face ecosystem plus supporting libraries for local ML",
      "actual_output": "[Installation output]",
      "output_summary": "ML libraries installed including PyTorch, Transformers, and datasets",
      "output_type": "text",
      "learns": ["ml-stack-installation", "local-dependencies"],
      "reinforces": ["dependency-management"],
      "gotchas": ["Large download size (~1-2GB)", "PyTorch installation varies by system"],
      "performance": {
        "execution_time": "2-5 minutes",
        "scaling": "O(1) per installation",
        "optimization": "demo"
      },
      "alternatives": {
        "description": "Can install with specific CUDA versions for GPU support",
        "when_to_use": "When GPU acceleration is needed"
      },
      "state_after": {
        "tables": [],
        "views": [],
        "variables": [],
        "models_loaded": []
      },
      "pattern_refs": ["local-ml-setup"]
    },
    {
      "number": 2,
      "section_title": "Dataset Loading",
      "intent": "Load Hugging Face dataset with specific columns",
      "code": "import datasets\n\npadoru = (\n    datasets.load_dataset(\"not-lain/padoru\", split='train')\n    .select_columns(['Image', 'ImageSize', 'Name', 'ImageSource'])\n)",
      "imports_used": ["datasets"],
      "explanation": "Demonstrates selective column loading from Hugging Face Hub",
      "actual_output": "Dataset({\n    features: ['Image', 'ImageSize', 'Name', 'ImageSource'],\n    num_rows: 382\n})",
      "output_summary": "Dataset loaded with 382 rows and 4 selected columns",
      "output_type": "text",
      "learns": ["huggingface-dataset-loading", "column-selection", "dataset-structure"],
      "reinforces": ["data-import-patterns"],
      "gotchas": ["Large datasets may take time to download", "Column names are case-sensitive"],
      "performance": {
        "execution_time": "10-30s for download",
        "scaling": "O(dataset_size)",
        "optimization": "demo"
      },
      "alternatives": {
        "description": "Can load full dataset or use streaming for very large datasets",
        "when_to_use": "When memory constraints or dataset size is a concern"
      },
      "state_after": {
        "tables": [],
        "views": [],
        "variables": ["padoru"],
        "models_loaded": []
      },
      "pattern_refs": ["dataset-import", "column-selection-pattern"]
    },
    {
      "number": 3,
      "section_title": "Hugging Face to Pixeltable Import",
      "intent": "Convert Hugging Face dataset into Pixeltable table",
      "code": "import pixeltable as pxt\n\npxt.drop_dir('hf_demo', force=True)\npxt.create_dir('hf_demo')\nt = pxt.io.import_huggingface_dataset('hf_demo.padoru', padoru)",
      "imports_used": ["pixeltable as pxt"],
      "explanation": "Uses Pixeltable's specialized importer for Hugging Face datasets",
      "actual_output": "Connected to Pixeltable database at: postgresql+psycopg://postgres:@/pixeltable?host=/Users/asiegel/.pixeltable/pgdata\nCreated directory `hf_demo`.\nCreated table `padoru_tmp_8951741`.\nComputing cells: 100%|████████████████████████████████████████| 504/504 [00:08<00:00, 60.68 cells/s]\nInserted 126 rows with 0 errors.\n[... multiple batches ...]\nInserted 126 rows with 0 errors.",
      "output_summary": "Dataset successfully imported with automatic type conversion and progress tracking",
      "output_type": "text",
      "learns": ["huggingface-import-function", "automatic-type-mapping", "batch-processing"],
      "reinforces": ["data-import", "table-creation"],
      "gotchas": ["Import process may take several minutes for large datasets", "Temporary table names generated automatically"],
      "performance": {
        "execution_time": "1-5 minutes depending on dataset size",
        "scaling": "O(n) with dataset size",
        "optimization": "production"
      },
      "alternatives": {
        "description": "Can manually create table and insert data for custom type control",
        "when_to_use": "When specific column types or constraints are needed"
      },
      "state_after": {
        "tables": ["hf_demo.padoru"],
        "views": [],
        "variables": ["padoru", "t"],
        "models_loaded": []
      },
      "pattern_refs": ["dataset-import", "automatic-type-conversion"]
    },
    {
      "number": 4,
      "section_title": "Data Inspection",
      "intent": "Examine imported data structure and content",
      "code": "t.head(3)",
      "imports_used": ["pixeltable as pxt"],
      "explanation": "Verify successful import and examine data types",
      "actual_output": "                                               Image  ImageSize  \\\n0  <PIL.WebPImagePlugin.WebPImageFile image mode=...     240993   \n1  <PIL.WebPImagePlugin.WebPImageFile image mode=...     993097   \n2  <PIL.WebPImagePlugin.WebPImageFile image mode=...     255549   \n\n            Name                                     ImageSource  \n0        AI-Chan  https://knowyourmeme.com/photos/1439336-padoru  \n1       Platelet  https://knowyourmeme.com/photos/1438687-padoru  \n2  Nezuko Kamado  https://knowyourmeme.com/photos/1568913-padoru",
      "output_summary": "Table contains PIL Image objects with metadata",
      "output_type": "table",
      "learns": ["pil-image-integration", "data-verification"],
      "reinforces": ["data-inspection", "table-operations"],
      "gotchas": ["Images are stored as PIL objects, not file paths"],
      "performance": {
        "execution_time": "<1s",
        "scaling": "O(1)",
        "optimization": "production"
      },
      "alternatives": {
        "description": "Can use select() with specific columns for targeted inspection",
        "when_to_use": "When examining specific data types or columns"
      },
      "state_after": {
        "tables": ["hf_demo.padoru"],
        "views": [],
        "variables": ["padoru", "t"],
        "models_loaded": []
      },
      "pattern_refs": ["data-verification", "image-data-handling"]
    },
    {
      "number": 5,
      "section_title": "Local Model Integration",
      "intent": "Set up CLIP embeddings using local Hugging Face model",
      "code": "from pixeltable.functions.huggingface import clip\nimport PIL.Image\n\n# create embedding index on the 'Image' column\nt.add_embedding_index(\n    'Image',\n    embedding=clip.using(model_id='openai/clip-vit-base-patch32')\n)",
      "imports_used": ["pixeltable.functions.huggingface.clip", "PIL.Image"],
      "explanation": "Creates embedding index using local CLIP model for image similarity search",
      "actual_output": "Computing cells: 100%|████████████████████████████████████████| 382/382 [00:16<00:00, 22.63 cells/s]",
      "output_summary": "Embedding index created successfully, processing all 382 images",
      "output_type": "text",
      "learns": ["local-embedding-models", "clip-integration", "embedding-index-creation"],
      "reinforces": ["computed-columns", "embedding-workflows"],
      "gotchas": ["First run downloads model weights (~500MB)", "Computation time depends on dataset size"],
      "performance": {
        "execution_time": "2-5 minutes including download",
        "scaling": "O(n) with image count",
        "optimization": "production"
      },
      "alternatives": {
        "description": "Can use different CLIP models or other embedding models",
        "when_to_use": "When specific embedding quality or speed requirements exist"
      },
      "state_after": {
        "tables": ["hf_demo.padoru"],
        "views": [],
        "variables": ["padoru", "t"],
        "models_loaded": ["openai/clip-vit-base-patch32"]
      },
      "pattern_refs": ["local-model-integration", "embedding-index-pattern", "clip-embeddings"]
    },
    {
      "number": 6,
      "section_title": "Similarity Search Execution",
      "intent": "Demonstrate image similarity search using local embeddings",
      "code": "sample_img = t.select(t.Image).head(1)[0]['Image']\n\nsim = t.Image.similarity(sample_img)\n\n# use 'similarity()' in the order_by() clause and apply a limit in order to utilize the index\nt.order_by(sim, asc=False).limit(3).select(t.Image, sim=sim).collect()",
      "imports_used": ["pixeltable as pxt"],
      "explanation": "Performs semantic similarity search using the created embedding index",
      "actual_output": "                                               Image       sim\n0  <PIL.WebPImagePlugin.WebPImageFile image mode=...  1.000000\n1  <PIL.WebPImagePlugin.WebPImageFile image mode=...  0.962924\n2  <PIL.WebPImagePlugin.WebPImageFile image mode=...  0.960727",
      "output_summary": "Returns top 3 most similar images with similarity scores",
      "output_type": "table",
      "learns": ["similarity-search", "embedding-index-usage", "semantic-image-search"],
      "reinforces": ["query-operations", "ordering-results"],
      "gotchas": ["Must use order_by with limit to leverage index efficiently"],
      "performance": {
        "execution_time": "<1s for search",
        "scaling": "O(log n) with index",
        "optimization": "production"
      },
      "alternatives": {
        "description": "Can search by text queries using CLIP's multimodal capabilities",
        "when_to_use": "When searching images by text descriptions"
      },
      "state_after": {
        "tables": ["hf_demo.padoru"],
        "views": [],
        "variables": ["padoru", "t", "sample_img", "sim"],
        "models_loaded": ["openai/clip-vit-base-patch32"]
      },
      "pattern_refs": ["similarity-search-pattern", "embedding-query-optimization"]
    }
  ],
  "patterns": [
    {
      "name": "local_ml_setup",
      "description": "Complete local machine learning environment setup without cloud dependencies",
      "confidence": "high",
      "frequency": 2,
      "first_seen": "huggingface-integration",
      "code_template": "%pip install -qU pixeltable datasets torch transformers [additional_ml_libs]",
      "parameters": {
        "ml_libs": "List of ML libraries needed (torch, transformers, etc.)",
        "version_constraints": "Specific versions if compatibility is critical"
      },
      "variations": [
        {
          "name": "gpu_optimized_setup",
          "difference": "Includes CUDA-specific PyTorch installation",
          "code": "%pip install torch torchvision --index-url https://download.pytorch.org/whl/cu118"
        }
      ],
      "prerequisites": ["python-environment"],
      "enables": ["local-model-execution", "privacy-preserving-ml"],
      "performance_impact": "High initial setup time, no ongoing API costs",
      "reusable": true,
      "production_ready": true
    },
    {
      "name": "dataset_import",
      "description": "Direct import of Hugging Face datasets into Pixeltable with type preservation",
      "confidence": "high",
      "frequency": 2,
      "first_seen": "huggingface-integration",
      "code_template": "dataset = datasets.load_dataset('dataset_name', split='split_name')\ntable = pxt.io.import_huggingface_dataset('table_name', dataset)",
      "parameters": {
        "dataset_name": "Hugging Face Hub dataset identifier",
        "split_name": "Dataset split (train, test, validation)",
        "table_name": "Target Pixeltable name"
      },
      "variations": [
        {
          "name": "streaming_import",
          "difference": "Use streaming for very large datasets",
          "code": "dataset = datasets.load_dataset('name', streaming=True)"
        },
        {
          "name": "column_selective_import",
          "difference": "Select specific columns during import",
          "code": "dataset = dataset.select_columns(['col1', 'col2'])"
        }
      ],
      "prerequisites": ["huggingface-datasets"],
      "enables": ["ml-dataset-workflows", "research-data-access"],
      "performance_impact": "High initial import time, efficient subsequent access",
      "reusable": true,
      "production_ready": true
    },
    {
      "name": "local_model_integration",
      "description": "Integration of local Hugging Face models without API dependencies",
      "confidence": "high",
      "frequency": 3,
      "first_seen": "huggingface-integration",
      "code_template": "from pixeltable.functions.huggingface import model_family\nt.add_computed_column(result=model_family.using(model_id='model/name'))",
      "parameters": {
        "model_family": "Pixeltable function family (clip, transformers, etc.)",
        "model_id": "Hugging Face model identifier",
        "compute_resource": "CPU/GPU preference"
      },
      "variations": [
        {
          "name": "custom_model_config",
          "difference": "Custom model configuration and parameters",
          "code": "model_family.using(model_id='name', device='cuda', batch_size=32)"
        }
      ],
      "prerequisites": ["local-ml-setup"],
      "enables": ["privacy-preserving-ai", "cost-effective-inference", "offline-workflows"],
      "performance_impact": "High - depends on local hardware capabilities",
      "reusable": true,
      "production_ready": true
    },
    {
      "name": "embedding_index_pattern",
      "description": "Creating and utilizing embedding indexes for similarity search",
      "confidence": "high",
      "frequency": 3,
      "first_seen": "huggingface-integration",
      "code_template": "t.add_embedding_index(\n    'column_name',\n    embedding=embedding_function\n)\n# Query usage\nt.order_by(t.column.similarity(query), asc=False).limit(k)",
      "parameters": {
        "column_name": "Column to index (text, image, etc.)",
        "embedding_function": "Function to compute embeddings",
        "query": "Query object for similarity search"
      },
      "variations": [
        {
          "name": "multimodal_search",
          "difference": "Search images with text queries using CLIP",
          "code": "t.order_by(t.Image.similarity('text query'), asc=False)"
        }
      ],
      "prerequisites": ["local-model-integration"],
      "enables": ["semantic-search", "content-discovery", "recommendation-systems"],
      "performance_impact": "High initial creation, fast queries with index",
      "reusable": true,
      "production_ready": true
    },
    {
      "name": "clip_embeddings",
      "description": "CLIP model integration for multimodal image-text embeddings",
      "confidence": "high",
      "frequency": 2,
      "first_seen": "huggingface-integration",
      "code_template": "from pixeltable.functions.huggingface import clip\nt.add_embedding_index(\n    'Image',\n    embedding=clip.using(model_id='openai/clip-vit-base-patch32')\n)",
      "parameters": {
        "model_id": "CLIP model variant (base-patch32, large-patch14, etc.)",
        "column_type": "Image or text column for embedding"
      },
      "variations": [
        {
          "name": "larger_clip_model",
          "difference": "Use larger CLIP model for better quality",
          "code": "clip.using(model_id='openai/clip-vit-large-patch14')"
        }
      ],
      "prerequisites": ["local-model-integration"],
      "enables": ["multimodal-search", "image-text-matching", "content-understanding"],
      "performance_impact": "Medium to high depending on model size",
      "reusable": true,
      "production_ready": true
    }
  ],
  "common_errors": [
    {
      "error_type": "CUDA Out of Memory",
      "frequency": "common",
      "cause": "Model too large for available GPU memory",
      "symptoms": ["RuntimeError: CUDA out of memory", "GPU memory allocation failed"],
      "solution": {
        "quick_fix": "Use CPU-only execution or smaller model",
        "proper_fix": "Implement batch processing or model quantization"
      },
      "prevention": "Check GPU memory before loading large models",
      "example": "Loading large CLIP model on limited GPU",
      "first_seen": "huggingface-integration#5"
    },
    {
      "error_type": "Model Download Failure",
      "frequency": "occasional",
      "cause": "Network issues or incorrect model identifier",
      "symptoms": ["Repository not found", "Connection timeout"],
      "solution": {
        "quick_fix": "Verify model name and check internet connection",
        "proper_fix": "Implement retry logic with exponential backoff"
      },
      "prevention": "Pre-download models in controlled environment",
      "example": "Using incorrect model path or version",
      "first_seen": "huggingface-integration#5"
    },
    {
      "error_type": "Missing Dependencies",
      "frequency": "common",
      "cause": "Incomplete installation of ML libraries",
      "symptoms": ["ModuleNotFoundError", "Import errors"],
      "solution": {
        "quick_fix": "Install missing packages individually",
        "proper_fix": "Use requirements.txt with pinned versions"
      },
      "prevention": "Verify all dependencies before model loading",
      "example": "Missing torch or transformers after partial installation",
      "first_seen": "huggingface-integration#1"
    },
    {
      "error_type": "Dataset Format Mismatch",
      "frequency": "occasional",
      "cause": "Unexpected dataset structure or column types",
      "symptoms": ["KeyError on column access", "Type conversion errors"],
      "solution": {
        "quick_fix": "Inspect dataset structure before import",
        "proper_fix": "Implement schema validation and transformation"
      },
      "prevention": "Always examine dataset structure before processing",
      "example": "Expecting image column but finding file paths",
      "first_seen": "huggingface-integration#2"
    }
  ],
  "test_questions": [
    {
      "question": "What are the main advantages of using local Hugging Face models vs API-based models?",
      "answer": "No API costs, complete privacy, offline operation, no rate limits, full control over model behavior",
      "difficulty": "intermediate"
    },
    {
      "question": "How does Pixeltable handle Hugging Face dataset imports?",
      "answer": "Automatic type mapping, preserves dataset structure, maintains splits, converts to Pixeltable table format",
      "difficulty": "beginner"
    },
    {
      "question": "What considerations are important for embedding index performance?",
      "answer": "Use order_by with limit to leverage index, consider model size vs accuracy tradeoffs, local compute resources",
      "difficulty": "advanced"
    },
    {
      "question": "How does CLIP enable multimodal search capabilities?",
      "answer": "Shared embedding space for images and text allows searching images with text queries and vice versa",
      "difficulty": "intermediate"
    }
  ],
  "production_tips": [
    {
      "tip": "Pre-download and cache models in production environments",
      "impact": "Faster startup times and offline reliability",
      "implementation": "Download models during container build or initialization",
      "trade_offs": "Larger deployment size and longer build times",
      "example": "Add model download to Docker image build process"
    },
    {
      "tip": "Choose model size based on hardware constraints",
      "impact": "Better performance and resource utilization",
      "implementation": "Profile different model sizes against your hardware",
      "trade_offs": "Smaller models may have lower accuracy",
      "example": "Use CLIP base-patch32 on CPU, large-patch14 on GPU"
    },
    {
      "tip": "Implement GPU memory management for large models",
      "impact": "Prevents out-of-memory errors and enables larger workloads",
      "implementation": "Use model quantization, batch size tuning, or CPU fallback",
      "trade_offs": "May reduce model quality or increase complexity",
      "example": "Load models with torch.cuda.set_per_process_memory_fraction(0.8)"
    },
    {
      "tip": "Monitor local compute resources and scaling",
      "impact": "Ensures consistent performance under load",
      "implementation": "Profile CPU/GPU usage and implement resource limits",
      "trade_offs": "May require horizontal scaling for high throughput",
      "example": "Use multiple worker processes for CPU-bound embedding tasks"
    }
  ],
  "pattern_maturity": {
    "novel_patterns": 1,
    "established_patterns": 4,
    "total_patterns": 5
  },
  "cookies": "🍪 Hugging Face models are like having a personal AI chef in your kitchen - no delivery fees, no waiting, and you can cook exactly what you want, but you do need to stock your own ingredients (and GPU)!"
}