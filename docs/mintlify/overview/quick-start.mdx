---
title: 'Quick Start'
description: 'Welcome to Pixeltable! In this tutorial, we will survey how to create tables, populate them with data, and enhance them with built-in and user-defined transformations and AI operations.'
icon: 'wand-magic-sparkles'
---

<Note>
This guide will get you from zero to a working AI application in under 5 minutes. Learn more by looking at this tutorial on [Github](/notebooks/pixeltable-basics).
</Note>

## Create Your First Multimodal AI Application

Let's build an image analysis application that combines object detection and OpenAI Vision.

## Installation

  Please refer to our installation section [here](/overview/installation).

  ```bash
  pip install -qU torch transformers openai pixeltable
  ```

<Steps>
  <Step title="Create Table Structure">
    ```python
    import pixeltable as pxt

    # Create directory for our tables
    pxt.drop_dir('demo', force=True)
    pxt.create_dir('demo')

    # Create table with image column
    t = pxt.create_table('demo.first', {'input_image': pxt.Image})
    ```

    <Tip>
    This creates a persistent and versioned table that holds data.
    </Tip>
  </Step>

  <Step title="Add Object Detection">
    ```python
    from pixeltable.functions import huggingface

    # Add ResNet-50 object detection
    t.add_computed_column(
        detections=huggingface.detr_for_object_detection(
            t.input_image,
            model_id='facebook/detr-resnet-50'
        )
    )

    # Extract just the labels
    t.add_computed_column(detections_text=t.detections.label_text)
    ```

    <Tip>
    Computed columns are populated whenever new data is added to their input columns.
    </Tip>
  </Step>

  <Step title="Add OpenAI Vision Analysis">
    ```python
    import os
    import getpass

    from pixeltable.functions import openai

    if 'OPENAI_API_KEY' not in os.environ:
      os.environ['OPENAI_API_KEY'] = getpass.getpass('Enter your OpenAI API key:')

    t.add_computed_column(
        vision=openai.vision(
            prompt="Describe what's in this image.",
            image=t.input_image,
            model='gpt-4o-mini'
        )
    )
    ```
  <Tip>
  Pixeltable handles parallelization, rate limiting, and incremental processing automatically.
  </Tip>
  </Step>

  <Step title="Use Your Application">
    ```python
  # Insert an image
  t.insert(input_image='https://raw.githubusercontent.com/pixeltable/pixeltable/release/docs/resources/images/000000000025.jpg')

  # Retrieve results
  t.select(
      t.input_image,
      t.detections_text,
      t.vision
  ).collect()
    ```
  <Tip>
  The query engine uses lazy evaluation, only computing what's needed.
  </Tip>
  </Step>
</Steps>

<Accordion title="What happened behind the scenes?">
Pixeltable automatically:
  1. Created a persistent table
  2. Downloaded and cached the ResNet model
  3. Orchestrated the OpenAI API call
  4. Created an efficient processing workflow
  5. Stored all results for future use
</Accordion>

## Key Features

<CardGroup cols={2}>
  <Card title="Persistent Storage" icon="database">
    All data and computed results are automatically stored and versioned. Your app state persists between sessions.
  </Card>
  <Card title="Computed Columns" icon="calculator">
    Define transformations once, they run automatically on new data. Perfect for AI orchestration.
  </Card>
  <Card title="Multimodal Support" icon="photo-film">
    Handle images, video, audio, and text seamlessly in one unified interface.
  </Card>
  <Card title="AI Integration" icon="brain">
    Built-in support for popular AI services like OpenAI, YOLOX, Hugging Face, Label Studio, Replicate, Anthropic...
  </Card>
</CardGroup>

## Custom Functions (UDFs)

Extend Pixeltable with your own functions using the `@pxt.udf` decorator:

```python
@pxt.udf
def top_detection(detect: dict) -> str:
    scores = detect['scores']
    label_text = detect['label_text']
    i = scores.index(max(scores))
    return label_text[i]

# Use it in a computed column
t.add_computed_column(top=top_detection(t.detections))
```

## Next Steps

<CardGroup cols={3}>
  <Card title="RAG Tutorial" icon="books" href="/notebooks/use-cases/rag-demo">
    Build a production-grade RAG system
  </Card>
  <Card title="Video Analysis" icon="video" href="/notebooks/use-cases/object-detection-in-videos">
    Process video with object detection
  </Card>
  <Card title="LLM Integration" icon="message" href="/integrations/frameworks#local-llm-runtimes">
    Work with OpenAI and other LLM providers
  </Card>
</CardGroup>