---
title: 'Pixeltable YOLOX'
description: 'Lightweight object detection library built on PyTorch'
icon: 'eye'
---

<Card title="Pixeltable YOLOX" icon="github" href="https://github.com/pixeltable/pixeltable-yolox">
  View the source code and contribute to Pixeltable YOLOX
</Card>

`pixeltable-yolox` is a lightweight, Apache-licensed object detection library built on PyTorch. It is a fork of the MegVii [YOLOX](https://github.com/Megvii-BaseDetection/YOLOX) package originally authored by Zheng Ge et al, modernized for recent versions of Python and refactored to be more easily usable as a Python library.

`pixeltable-yolox` is still a work in progress! Some features of YoloX have not been ported yet.

<Note>
Pixeltable YOLOX is designed for developers seeking a modern, easy-to-use object detection library. While still under development, it offers a robust foundation for both academic and commercial projects.
</Note>

### Installation

```bash
pip install pixeltable pixeltable-yolox
```

### Inference

```python
import pixeltable as pxt
from pixeltable.ext.functions.yolox import yolox
from yolox.data.datasets import COCO_CLASSES

# Create a table with an image column
table = pxt.create_table('object_detection', {'image': pxt.Image})

# Insert some images
prefix = 'https://upload.wikimedia.org/wikipedia/commons'
paths = [
    '/1/15/Cat_August_2010-4.jpg',
    '/e/e1/Example_of_a_Dog.jpg',
    '/thumb/b/bf/Bird_Diversity_2013.png/300px-Bird_Diversity_2013.png'
]
table.insert({'image': prefix + p} for p in paths)

# Add computed columns with YOLOX object detections using different models
table.add_computed_column(detect_yolox_tiny=yolox(table.image, model_id='yolox_tiny', threshold=0.25))
table.add_computed_column(detect_yolox_nano=yolox(table.image, model_id='yolox_nano', threshold=0.2))

# Access specific parts of the detection results
table.add_computed_column(yolox_tiny_bboxes=table.detect_yolox_tiny.bboxes)
table.add_computed_column(yolox_tiny_labels=table.detect_yolox_tiny.labels)
table.add_computed_column(yolox_tiny_scores=table.detect_yolox_tiny.scores)

# Convert numeric labels to human-readable class names
@pxt.udf
def get_class_names(labels: list[int]) -> list[str]:
    return [COCO_CLASSES[label] for label in labels]

table.add_computed_column(class_names=get_class_names(table.yolox_tiny_labels))

# For an existing table with images (e.g., frames from a video)
frames_view = pxt.get_table('video_frames')
frames_view.add_computed_column(
    detect_yolox_tiny=yolox(frames_view.frame, model_id='yolox_tiny', threshold=0.25)
)
```

This yields the following output structure for each detection:

```python
{
  'bboxes': [
    [273.2366943359375, 399.3636169433594, 3241.008056640625, 1544.01220703125]
  ],
  'scores': [0.8520765346409291],
  'labels': [15]  # 15 corresponds to 'cat' in COCO_CLASSES
}
```

The labels are COCO category indices. Here's a real example of detection results from our test:

```python
# Cat image detection
{
  'bboxes': [[273.2366943359375, 399.3636169433594, 3241.008056640625, 1544.01220703125]],
  'scores': [0.8520765346409291],
  'labels': [15]  # 15 corresponds to 'cat' in COCO_CLASSES
}

# Dog image detection
{
  'bboxes': [[94.6337890625, 93.9306640625, 698.3662109375, 841.0693359375]],
  'scores': [0.9095208167701685],
  'labels': [16]  # 16 corresponds to 'dog' in COCO_CLASSES
}

# Bird image detection (multiple birds detected)
{
  'bboxes': [[...], [...], ...],  # Multiple bounding boxes
  'scores': [0.8699160892732074, 0.866650915694251, ...],  # Multiple confidence scores
  'labels': [14, 14, ...]  # 14 corresponds to 'bird' in COCO_CLASSES
}
```

You can convert the numeric labels to human-readable class names:

```python
from yolox.data.datasets import COCO_CLASSES

# Convert a single label
print(COCO_CLASSES[15])  # Output: 'cat'

# Or use a UDF to convert all labels in a column
@pxt.udf
def get_class_names(labels: list[int]) -> list[str]:
    return [COCO_CLASSES[label] for label in labels]

table.add_computed_column(class_names=get_class_names(table.yolox_tiny_labels))
```

### Complete Example with Visualization

Here's a complete example that demonstrates how to use YOLOX for object detection and visualize the results:

```python
import pixeltable as pxt
from pixeltable.ext.functions.yolox import yolox
from yolox.data.datasets import COCO_CLASSES
import PIL.Image
import PIL.ImageDraw
import numpy as np

# Create a table with an image column
t = pxt.create_table('image_detection', {'image': pxt.Image}, if_exists='replace')

# Insert some images
prefix = 'https://upload.wikimedia.org/wikipedia/commons'
paths = [
    '/1/15/Cat_August_2010-4.jpg',
    '/e/e1/Example_of_a_Dog.jpg',
    '/thumb/b/bf/Bird_Diversity_2013.png/300px-Bird_Diversity_2013.png'
]
t.insert({'image': prefix + p} for p in paths)

# Add computed column with YOLOX object detections
t.add_computed_column(detections=yolox(t.image, model_id='yolox_tiny', threshold=0.25))

# Function to draw bounding boxes and labels on images
@pxt.udf
def visualize_detections(image: PIL.Image.Image, detections: dict) -> PIL.Image.Image:
    # Create a copy of the image to draw on
    img_draw = image.copy()
    draw = PIL.ImageDraw.Draw(img_draw)
    
    # Get detection data
    bboxes = detections['bboxes']
    labels = detections['labels']
    scores = detections['scores']
    
    # Define colors for different classes (using a simple hash function)
    def get_color(label_id):
        np.random.seed(label_id)
        return tuple(np.random.randint(0, 255, 3).tolist())
    
    # Draw each detection
    for bbox, label, score in zip(bboxes, labels, scores):
        # Extract coordinates
        x1, y1, x2, y2 = [round(coord) for coord in bbox]
        
        # Get color for this class
        color = get_color(label)
        
        # Draw rectangle
        draw.rectangle([x1, y1, x2, y2], outline=color, width=3)
        
        # Prepare label text
        label_text = f"{COCO_CLASSES[label]}: {score:.2f}"
        
        # Draw label background
        text_size = draw.textsize(label_text) if hasattr(draw, 'textsize') else (100, 15)  # Fallback size
        draw.rectangle([x1, y1, x1 + text_size[0], y1 + text_size[1]], fill=color)
        
        # Draw label text
        draw.text([x1, y1], label_text, fill=(255, 255, 255))
    
    return img_draw

# Add computed column with visualized detections
t.add_computed_column(visualized=visualize_detections(t.image, t.detections))

# Display the results
results = t.select(t.image, t.visualized, t.detections).collect()
```

This example:
1. Creates a table with an image column
2. Adds sample images
3. Runs YOLOX object detection
4. Visualizes the detections by drawing bounding boxes and labels on the images
5. Stores both the original images and the visualized results

The `visualize_detections` UDF draws bounding boxes around detected objects, with different colors for different classes, and adds labels showing the class name and confidence score.

## Training

YOLOX supports training on standard datasets like COCO. The training process is straightforward and can be customized based on your needs.

### Preparing the Dataset

First, unpack a COCO dataset into `./datasets/COCO`:

```
COCO/
  annotations/
    instances_train2017.json
    instances_val2017.json
  train2017/
    # image files
  val2017/
    # image files
```

### Training Command

After installing `pixeltable-yolox`, you can use the command-line interface to train models:

```bash
yolox train -c yolox-s -d 8 -b 64 --fp16 -o
```

Where:
- `-c yolox-s` specifies the model configuration (other options include `yolox_nano`, `yolox_tiny`, `yolox_m`, `yolox_l`, `yolox_x`)
- `-d 8` sets the number of devices (GPUs) to use
- `-b 64` sets the batch size
- `--fp16` enables mixed precision training
- `-o` enables occupy mode (pre-allocate GPU memory)

### Available Models

YOLOX provides several pre-trained models with different sizes and performance characteristics:

| Model | Size | Speed | Accuracy | Use Case |
|-------|------|-------|----------|----------|
| yolox_nano | Smallest | Fastest | Lower | Mobile/edge devices |
| yolox_tiny | Very small | Very fast | Moderate | Resource-constrained environments |
| yolox_s | Small | Fast | Good | Balanced performance |
| yolox_m | Medium | Moderate | Better | Standard desktop/server |
| yolox_l | Large | Slower | High | High accuracy needs |
| yolox_x | Largest | Slowest | Highest | Maximum accuracy |

### Command-line Help

For more training options and detailed help:

```bash
yolox train -h
```

## Background

<Note>
The original YOLOX implementation, while powerful, has been updated only sporadically since 2022 and now faces compatibility issues with current Python environments, dependencies, and platforms like Google Colab. This fork aims to provide a reliable, up-to-date, and easy-to-use version of YOLOX that maintains its Apache license, ensuring it remains accessible for academic and commercial use.
</Note>

## Who are we and why are we doing this?

Pixeltable, Inc. is a venture-backed AI infrastructure startup. Our core product is [Pixeltable](https://pixeltable.com), a database and orchestration system purpose-built for multimodal AI workloads.

Pixeltable integrates with numerous AI services and open source technologies. In the course of integrating with YOLOX, it became clear that there is a strong need in the vision community for a lightweight object detection library with an untainted open source license. It also became clear that while YOLOX is an excellent foundation, it would benefit greatly from code modernization and more regular updates.

We chose to build upon YOLOX both to simplify our own integration, and also to give something back to the community that will (hopefully) prove useful. The Pixeltable team has decades of collective experience in open source development. Our backgrounds include companies such as Google, Cloudera, Twitter, Amazon, and Airbnb, that have a strong commitment to open source development and collaboration.

## Contributing

<Card title="Contribute" icon="github" href="https://github.com/pixeltable/pixeltable-yolox">
  Join our community and contribute to Pixeltable YOLOX
</Card>

We welcome contributions from the community! If you're interested in helping maintain and improve `pixeltable-yolox`, please stay tuned for our contributor's guide which will be published after the initial release.