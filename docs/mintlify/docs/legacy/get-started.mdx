---
title: "Introduction"
description: "A declarative approach to multimodal workloads."
---

[](/edit/get-started)

## Open Source AI Data Infrastructure

Express complex operations through simple table operations and computed columns:

* Data transformations
* Model inference
* Custom logic
* Multimodal data handling

### üéØ The Problem

Building AI applications today requires juggling multiple tools and writing complex infrastructure code to:

* Process and store different types of data (text, images, video, audio)
* Track changes and maintain data lineage
* Scale processing efficiently
* Move from development to production

### üí° The Solution

Pixeltable unifies all these operations under a simple, declarative interface. Pixeltable features built-in versioning, lineage tracking, and incremental updates, enabling users to store, transform, index, and iterate on data for their ML workflows. It combines data storage, versioning, indexing, and orchestration under a unified table interface, enabling data scientists and ML engineers to focus on modeling and experimentation rather than data plumbing.

<CodeGroup>
  ```python Python
  import pixeltable as pxt

  # Create a video table
  videos = pxt.create_table('videos', {'video': pxt.VideoType()})

  # Automatic frame extraction
  frames = pxt.create_view(
      'frames', 
      videos, 
      iterator=FrameIterator.create(video=videos.video)
  )

  # Add AI processing - only runs on new data
  frames_view.add_computed_column(detect_yolox_tiny=yolox(
      frames_view.frame, model_id='yolox_tiny', threshold=0.25
  ))
  ```
</CodeGroup>

### üöÄ Quick Start

<CodeGroup>
  ```python python
  pip install pixeltable
  ```
</CodeGroup>

## Core Use Cases

### 1. LLM Development & RAG

#### Industry Challenge

Organizations implementing LLM applications face significant hurdles in managing document processing, tracking model decisions, and maintaining efficient RAG systems. Traditional approaches lead to:

* Costly reprocessing of entire document bases for minor changes
* Lack of transparency in model decision-making
* Complex management of chunking strategies and embeddings
* Difficulty comparing performance across different approaches

#### Pixeltable Solution

<CodeGroup>
  ```python python
  # Declarative document processing with automatic versioning
  docs = pxt.create_table('knowledge_base', {'document': pxt.DocumentType()})

  # Flexible chunking strategies with computed views
  chunks = pxt.create_view(
      'chunks',
      docs,
      iterator=DocumentSplitter.create(
          document=docs.document,
          separators='token_limit',
          limit=300
      )
  )

  # Automatic embedding generation and indexing
  chunks.add_embedding_index(
      'text',
      idx_name='minilm_idx',
      string_embed=sentence_transformer.using(model_id='sentence-transformers/all-MiniLM-L12-v2')
  )

  [...]

  # Add a computed column that calls OpenAI
  queries_t.add_computed_column(
      response=openai.chat_completions(model='gpt-4o-mini', messages=messages)
  )
  ```
</CodeGroup>

#### Business Impact

* **Cost Reduction**: 70%+ reduction in processing costs through incremental updates
* **Quality Improvement**: Complete lineage tracking ensures answer accuracy
* **Development Speed**: Rapid experimentation with different strategies
* **Operational Efficiency**: Built-in versioning eliminates manual tracking

### 2. Computer Vision Workflows

#### Industry Challenge

Computer vision teams struggle with:

* Managing large-scale video and image datasets
* Tracking model versions and annotations
* Maintaining consistency between development and production
* Efficiently processing incremental updates

#### Pixeltable Solution

<CodeGroup>
  ```python python
  # Unified video processing pipeline
  videos = pxt.create_table('videos', {'video': pxt.VideoType()})

  # Automatic frame extraction and management
  frames = pxt.create_view(
      'frames',
      videos,
      iterator=FrameIterator.create(video=videos.video)
  )

  # Integrated object detection and annotation
  frames_view.add_computed_column(detect_yolox_tiny=yolox(
      frames_view.frame, model_id='yolox_tiny', threshold=0.25
  ))

  @pxt.udf
  def draw_boxes(
      img: PIL.Image.Image, boxes: list[list[float]]
  ) -> PIL.Image.Image:
      result = img.copy()  # Create a copy of `img`
      d = PIL.ImageDraw.Draw(result)
      for box in boxes:
          # Draw bounding box rectangles on the copied image
          d.rectangle(box, width=3)
      return result

  frames_view.group_by(videos_table).select(
      pxt.functions.video.make_video(
          frames_view.pos,
          draw_boxes(
              frames_view.frame,
              frames_view.detect_yolox_tiny.bboxes
          )
      )
  ).show(1)
  ```
</CodeGroup>

#### Business Impact

* **Resource Optimization**: Lazy evaluation reduces storage and compute costs
* **Quality Assurance**: Automatic lineage tracking for all model outputs
* **Development Efficiency**: Seamless integration with annotation tools
* **Deployment Confidence**: Production parity with development environment

### 3. Multimodal AI Applications

#### Industry Challenge

Organizations building multimodal AI applications face:

* Complex integration of different data types
* Difficult relationship management between modalities
* Lack of unified search capabilities
* Complex pipeline maintenance

#### Pixeltable Solution

<CodeGroup>
  ```python python
  # Unified multimodal data management
  t = pxt.create_table('content', {
      'video': pxt.Video
  })

  # Automated cross-modal processing
  t['audio'] = extract_audio(t.video, format='mp3')
  t['metadata'] = get_metadata(t.audio)
  t['transcription'] = openai.transcriptions(audio=t.audio, model='whisper-1')
  t['transcription_text'] = t.transcription.text
  [...]
  t['response'] = openai.chat_completions(messages=t.message, model='gpt-4o-mini-2024-07-18', max_tokens=500)
  ```
</CodeGroup>

#### Business Impact

* **Simplified Architecture**: Single interface for all data types
* **Enhanced Search**: Unified search across modalities
* **Reduced Complexity**: Automated pipeline management
* **Faster Development**: Built-in transformations between modalities

## First Steps

* 10-Minute Tutorial: [Get started with Pixeltable](/docs/pixeltable-basics)
* Core Concepts: [Learn the fundamental building blocks](/docs/tables-and-data-operations)
* Example Gallery: [Explore real-world applications](https://huggingface.co/Pixeltable)

### üìö Popular Tutorials

#### Computer Vision

* [Comparing Object Detection Models](/docs/object-detection-in-videos)
* [Audio and Video Transcript Indexing](/docs/transcribing-and-indexing-audio-and-video)
* [Label Studio Integration](/docs/using-label-studio-for-annotations-with-pixeltable)

#### Natural Language Processing

* [Document Indexing and RAG](/docs/document-indexing-and-rag)
* [Embedding and Vector Indexes](/docs/embedding-vector-indexes)
* [Incremental Prompt Engineering and Model Comparison](https://github.com/mistralai/cookbook/blob/main/third_party/Pixeltable/incremental_prompt_engineering_and_model_comparison.ipynb)

#### Multimodal Applications

* [Text and Image Similarity Search on Video Frames](https://huggingface.co/spaces/Pixeltable/Text-image-similarity-search-on-video-frames-embedding-indexes)
* [Multimodal Powerhouse](https://huggingface.co/spaces/Pixeltable/Multimodal-Powerhouse)
* [Document to Audio Synthesis](https://huggingface.co/spaces/Pixeltable/Document-to-Audio-Synthesis)

### üîç Next Steps

* Installation Guide: [https://docs.pixeltable.com/docs/installation](/docs/installation)
* API Reference: [https://pixeltable.github.io/pixeltable/api/pixeltable/](https://pixeltable.github.io/pixeltable/api/pixeltable/)
* GitHub Repository: [https://github.com/pixeltable/pixeltable](https://github.com/pixeltable/pixeltable)
* Community & Support: [Contributions and Discussions](/docs/contributions-discussions)

***

* [Table of Contents](#)

* * [Open Source AI Data Infrastructure](#open-source-ai-data-infrastructure)

    * [üéØ The Problem](#-the-problem)
    * [üí° The Solution](#-the-solution)
    * [üöÄ Quick Start](#-quick-start)

  * [Core Use Cases](#core-use-cases)

    * [1. LLM Development & RAG](#1-llm-development--rag)
    * [2. Computer Vision Workflows](#2-computer-vision-workflows)
    * [3. Multimodal AI Applications](#3-multimodal-ai-applications)

  * [First Steps](#first-steps)

    * [üìö Popular Tutorials](#-popular-tutorials)
    * [üîç Next Steps](#-next-steps)
