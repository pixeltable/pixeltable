---
title: 'Embedding/Vector Index'
description: 'Learn how to create, populate and query embedding indexes in Pixeltable'
icon: 'cube'
---

<Info>
Learn more about embedding/vector indexes with this [in-depth guide](https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/feature-guides/embedding-indexes.ipynb).
</Info>

## What are Embedding Indexes?

Embedding indexes in Pixeltable enable semantic search and similarity-based retrieval across different data modalities. They store vector representations of your content, allowing you to find related items based on meaning rather than exact matching.

Unlike traditional indexes that search by keywords, embedding indexes capture the semantic essence of your data, making it possible to:
- Find content with similar meaning even when using different words
- Match content across different modalities (text-to-image, image-to-text)
- Rank results based on semantic relevance
- Build powerful retrieval systems for RAG applications

Pixeltable makes working with embeddings simple by:
- Managing the lifecycle of embedding computations
- Automatically updating indexes when data changes
- Providing a unified interface for different embedding models
- Supporting multiple index types on the same column

```python
import pixeltable as pxt
from pixeltable.functions.huggingface import sentence_transformer

# Create a table and add an embedding index
knowledge_base = pxt.create_table('kb', {
    'content': pxt.String,
    'category': pxt.String
})

# Add an embedding index using a pre-trained model
knowledge_base.add_embedding_index(
    column='content',
    string_embed=sentence_transformer.using(
        model_id='sentence-transformers/all-MiniLM-L12-v2'
    )
)
```

# Overview

Embedding indexes in Pixeltable are:
- **Declarative**: Define the index structure and embedding functions once
- **Maintainable**: Pixeltable automatically keeps indexes up-to-date on changes
- **Flexible**: Support multiple index types on the same column
- **Multimodal**: Handle text, images, audio, and documents

<Note>
In this guide, we'll create a semantic search system for images and text. Make sure you have the required dependencies installed:
```bash
pip install pixeltable sentence_transformers
```
</Note>

## Phase 1: Setup

The setup phase defines your schema and creates embedding indexes.

<Tabs>
  <Tab title="Basic Setup">
    ```python
    import pixeltable as pxt
    from pixeltable.functions.huggingface import sentence_transformer
    
    # Create a directory to organize data (optional)
    pxt.drop_dir('knowledge_base', force=True)
    pxt.create_dir('knowledge_base')
    
    # Create table
    docs = pxt.create_table(
        path_str="knowledge_base.documents",
        {
            "content": pxt.String,
            "metadata": pxt.Json
        }
    )
    
    # Create embedding index
    embed_model = sentence_transformer.using(
        model_id="intfloat/e5-large-v2"
    )
    docs.add_embedding_index(
        column='content',
        string_embed=embed_model
    )
    ```
  </Tab>
  
  <Tab title="Multiple Indexes">
    ```python
    # Create multiple indexes on same column
    docs.add_embedding_index(
        column='content',
        idx_name='minilm_idx',
        string_embed=sentence_transformer.using(
            model_id='sentence-transformers/all-MiniLM-L12-v2'
        )
    )
    
    docs.add_embedding_index(
        column='content',
        idx_name='e5_idx',
        string_embed=sentence_transformer.using(
            model_id='intfloat/e5-large-v2'
        )
    )
    ```
  </Tab>
  
  <Tab title="Custom Embeddings">
    ```python
    import tensorflow as tf
    import tensorflow_hub as hub
    import tensorflow_text
    
    @pxt.udf
    def bert(input: str) -> pxt.Array[(512,), pxt.Float]:
        """Computes text embeddings using small_bert."""
        preprocessor = hub.load(
            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'
        )
        bert_model = hub.load(
            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/2'
        )
        tensor = tf.constant([input])
        result = bert_model(preprocessor(tensor))['pooled_output']
        return result.numpy()[0, :]
    
    # Add custom embedding index
    docs.add_embedding_index(
        column='content',
        idx_name='bert_idx',
        string_embed=bert
    )
    ```
  </Tab>
</Tabs>

### Supported Index Options

<CardGroup cols={1}>
  <Card title="Similarity Metrics" icon="calculator">
    ```python
    # Available metrics:
    docs.add_embedding_index(
        column='content',
        metric='cosine'  # Default
        # Other options:
        # metric='ip'    # Inner product
        # metric='l2'    # L2 distance
    )
    ```
  </Card>
  
  <Card title="Index Configuration" icon="gear">
    ```python
    # Optional parameters
    docs.add_embedding_index(
        column='content',
        idx_name='custom_name',  # Optional name
        string_embed=embed_model,
        image_embed=img_model,   # For image columns
    )
    ```
  </Card>
</CardGroup>

## Phase 2: Insert

The insert phase populates your indexes with data. Pixeltable automatically computes embeddings and maintains index consistency.

```python
# Single insertion
docs.insert({
    "content": "Your document text here",
    "metadata": {"source": "web", "category": "tech"}
})

# Batch insertion
docs.insert([
    {
        "content": "First document",
        "metadata": {"source": "pdf", "category": "science"}
    },
    {
        "content": "Second document", 
        "metadata": {"source": "web", "category": "news"}
    }
])

# Image insertion
image_urls = [
    'https://example.com/image1.jpg',
    'https://example.com/image2.jpg'
]
images.insert({'image': url} for url in image_urls)
```

<Warning>
Large batch insertions are more efficient than multiple single insertions as they reduce the number of embedding computations.
</Warning>

## Phase 3: Query

The query phase allows you to search your indexed content using the `similarity()` function.

<Tabs>
  <Tab title="Basic Search">
    ```python
    @pxt.query
    def semantic_search(query: str, k: int = 5):
        # Calculate similarity scores
        sim = docs.content.similarity(query)
        
        # Return top-k most similar documents
        return (
            docs
            .order_by(sim, asc=False)
            .select(docs.content, docs.metadata, score=sim)
            .limit(k)
        )
    
    # Use it
    results = semantic_search("quantum computing")
    ```
  </Tab>
  
  <Tab title="Multi-Index Search">
    ```python
    @pxt.query
    def compare_models(query: str):
        # Search with different models
        minilm_sim = docs.content.similarity(
            query, idx='minilm_idx'
        )
        e5_sim = docs.content.similarity(
            query, idx='e5_idx'
        )
        
        return (
            docs
            .select(
                docs.content,
                minilm_score=minilm_sim,
                e5_score=e5_sim
            )
            .limit(5)
        )
    ```
  </Tab>
  
  <Tab title="Filtered Search">
    ```python
    @pxt.query
    def filtered_search(query: str, category: str):
        sim = docs.content.similarity(query)
        
        return (
            docs
            .where(docs.metadata['category'] == category)
            .order_by(sim, asc=False)
            .select(docs.content, score=sim)
            .limit(5)
        )
    ```
  </Tab>
</Tabs>

### Advanced Query Patterns

<AccordionGroup>
  <Accordion title="Hybrid Search" icon="layer-group">
    ```python
    @pxt.query
    def hybrid_search(query: str):
        # Combine different similarity scores
        text_sim = docs.content.similarity(query)
        title_sim = docs.title.similarity(query)
        
        # Weighted combination
        combined_sim = (text_sim * 0.7) + (title_sim * 0.3)
        
        return docs.order_by(combined_sim, asc=False).limit(5)
    ```
  </Accordion>

  <Accordion title="Cross-Modal Search" icon="arrows-repeat">
    ```python
    @pxt.query
    def image_to_text(image_query: PIL.Image.Image):
        # Search text using image embedding
        sim = docs.content.similarity(image_query)
        return docs.order_by(sim, asc=False).limit(5)
    ```
  </Accordion>
</AccordionGroup>

## Management Operations

<CardGroup cols={1}>
  <Card title="Drop Index" icon="trash">
    ```python
    # Drop by name
    docs.drop_embedding_index(idx_name='e5_idx')
    
    # Drop by column (if single index)
    docs.drop_embedding_index(column='content')
    ```
  </Card>
  
  <Card title="Update Index" icon="arrows-rotate">
    ```python
    # Indexes auto-update on changes
    docs.update({
        'content': docs.content + ' Updated!'
    })
    ```
  </Card>
</CardGroup>

## Best Practices

<Warning>
- Cache embedding models in production UDFs
- Use batching for better performance
- Consider index size vs. search speed tradeoffs 
- Monitor embedding computation time
</Warning>

## Additional Resources

<CardGroup cols={3}>
  <Card title="API Documentation" icon="book" href="https://pixeltable.github.io/pixeltable/">
    Complete API reference
  </Card>
  <Card title="Multimodal Indexes" icon="lightbulb" href="/docs/examples/chat/multimodal">
    Examples of multimodal embedding indexes
  </Card>
  <Card title="Model Hub" icon="brain" href="https://huggingface.co/models">
    Connect with your favorite Hugging Face models
  </Card>
</CardGroup>