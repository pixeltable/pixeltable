---
title: "pixeltable.functions.huggingface.clip"
sidebarTitle: "clip"
description: "Computes a CLIP embedding for the specified text or image. `model_id` should be a reference to a pretrained"
icon: "circle-u"
---
## ⚠️ Documentation Issues

<Warning>
- Examples section exists in docstring but was not parsed by docstring_parser
</Warning>

[CLIP Model](https://huggingface.co/docs/transformers/model_doc/clip).

**Requirements:**

- `pip install torch transformers`


## Signature

```python
# Polymorphic function with 2 signatures:

# Signature 1:
pixeltable.functions.huggingface.clip(text: String, model_id: String) -> Array[(None,), Float]

# Signature 2:
pixeltable.functions.huggingface.clip(image: Image, model_id: String) -> Array[(None,), Float]
```

## Args

<Note>
Parameter optional/required status may not be accurate if docstring doesn't specify defaults.
</Note>

<ParamField path="text" type="String" required>
  The string to embed.


</ParamField>

<ParamField path="model_id" type="String" required>
  The pretrained model to use for the embedding.


</ParamField>

## Returns

<ResponseField name="return" type="any" required>
  An array containing the output of the embedding model.


</ResponseField>

## Examples

```python
Add a computed column that applies the model `openai/clip-vit-base-patch32` to an existing
Pixeltable column `tbl.text` of the table `tbl`:

>>> tbl.add_computed_column(
...     result=clip(tbl.text, model_id='openai/clip-vit-base-patch32')
... )

The same would work with an image column `tbl.image` in place of `tbl.text`.
```

