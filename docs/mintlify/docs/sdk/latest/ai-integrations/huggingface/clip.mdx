---
title: "pixeltable.functions.huggingface.clip"
sidebarTitle: "clip"
description: "Computes a CLIP embedding for the specified text or image. `model_id` should be a reference to a pretrained"
icon: "circle-u"
---
## ⚠️ Documentation Issues

<Warning>
- Examples section exists in docstring but was not parsed by docstring_parser
</Warning>

[CLIP Model](https://huggingface.co/docs/transformers/model_doc/clip).

**Requirements:**

- `pip install torch transformers`


## Signature

```python
pixeltable.functions.huggingface.clip(*args: 'Any', **kwargs: 'Any') -> "'exprs.FunctionCall'"
```

## Args

<Note>
Parameter optional/required status may not be accurate if docstring doesn't specify defaults.
</Note>

<ParamField path="text" type="string" required>
  The string to embed.


</ParamField>

<ParamField path="model_id" type="string" required>
  The pretrained model to use for the embedding.


</ParamField>

## Returns

<ResponseField name="return" type="any" required>
  An array containing the output of the embedding model.


</ResponseField>

## Examples

```python
Add a computed column that applies the model `openai/clip-vit-base-patch32` to an existing
Pixeltable column `tbl.text` of the table `tbl`:

>>> tbl.add_computed_column(
...     result=clip(tbl.text, model_id='openai/clip-vit-base-patch32')
... )

The same would work with an image column `tbl.image` in place of `tbl.text`.
```

