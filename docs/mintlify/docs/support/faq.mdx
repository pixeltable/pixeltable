---
title: 'FAQ'
description: 'Frequently asked questions about Pixeltable'
icon: 'circle-question'
---

## Core Concepts

<AccordionGroup>
  <Accordion title="What is Pixeltable?">
    Pixeltable is open-source AI data infrastructure providing a declarative, incremental approach for multimodal workloads. It unifies data management, transformation, and AI model execution under a table-like interface. Key features:

    - **Unified Interface**: Handle text, images, video, and audio in a single framework
    - **Declarative Design**: Express transformations and model inference as computed columns
    - **Automatic Orchestration**: Built-in versioning, caching, and incremental updates
    - **Type-Safe Operations**: Rich type system for multimodal data handling

    ```python
    import pixeltable as pxt
    from pixeltable.iterators import DocumentSplitter
    
    # Create multimodal table for RAG
    docs = pxt.create_table('chatbot.documents', {
        'document': pxt.Document,  # PDF/Text files
        'video': pxt.Video,        # MP4 videos  
        'audio': pxt.Audio,        # Audio files
        'timestamp': pxt.Timestamp
    })
    
    # Create view for document chunking
    chunks = pxt.create_view(
        'chatbot.chunks',
        docs,
        iterator=DocumentSplitter.create(
            document=docs.document,
            separators='sentence',
            metadata='title,heading'
        )
    )

    # Add embedding index for search
    chunks.add_embedding_index(
        'text',
        string_embed=sentence_transformer
    )
    ```
  </Accordion>

  <Accordion title="How does Pixeltable handle data storage and processing?">
    Pixeltable provides key data management capabilities:

    - **Media Files**: References and validates external files (videos, images, documents)
    - **Incremental Processing**: Only recomputes affected parts of the workflow
    - **Type System**: Manages tensors, embeddings, and structured data
    - **Computed Columns**: Define transformations that update automatically
    - **Built-in Functions**: Common operations for ML workflows

    ```python
    # Example video processing
    frames = pxt.create_view(
        'video_search.frames',
        videos,
        iterator=FrameIterator.create(
            video=videos.video, 
            fps=1  # Extract 1 frame per second
        )
    )
    
    # Add multimodal search 
    frames.add_embedding_index(
        'frame',
        string_embed=clip_text,     # Text-to-image search
        image_embed=clip_image      # Image-to-image search
    )
    ```
  </Accordion>

  <Accordion title="What are Pixeltable views and computed columns?">
    Views and computed columns form the backbone of Pixeltable's declarative approach:

    **Views**
    - Virtual tables based on iterators (DocumentSplitter, FrameIterator)
    - Enable efficient chunking and processing
    - Support embedding indexes for similarity search
    
    ```python
    # Create document chunks view
    chunks = pxt.create_view(
        'docs.chunks',
        docs,
        iterator=DocumentSplitter.create(
            document=docs.document,
            separators='sentence'
        )
    )
    ```

    **Computed Columns**
    - Automatic transformations that update incrementally
    - Built-in operations for common ML tasks
    - Custom logic via User-Defined Functions (UDFs)
    
    ```python
    # Example computed columns for RAG
    docs_table.add_computed_column(
        embeddings=openai.embeddings(
            docs_table.text,
            model='text-embedding-3-small'
        )
    )

    # Add search capabilities
    docs_table.add_embedding_index(
        'text',
        string_embed=sentence_transformer
    )
    ```
  </Accordion>
</AccordionGroup>

## Features & Capabilities

<AccordionGroup>
  <Accordion title="What are Pixeltable's key features for AI applications?">
    **Multimodal Data Management**
    - Unified interface for text, images, video, and audio
    - Built-in versioning and lineage tracking
    - Efficient storage and caching

    **RAG Support**
    - Automatic document chunking 
    - Embedding generation and indexing
    - Efficient similarity search
    - Integration with LLM providers

    **Video & Audio Processing**
    - Frame extraction and analysis
    - Audio transcription 
    - Cross-modal search capabilities

    **Development Features**
    - Declarative computations
    - Incremental updates
    - Type-safe UDFs
    - Rich query capabilities
  </Accordion>

  <Accordion title="How does Pixeltable support RAG applications?">
    Pixeltable streamlines RAG development:

    ```python
    # Create chunks view 
    chunks = pxt.create_view(
        'chatbot.chunks',
        docs,
        iterator=DocumentSplitter.create(
            document=docs.document,
            separators='sentence',
            metadata='title,heading'
        )
    )
    
    # Add embedding index
    chunks.add_embedding_index(
        'text', 
        string_embed=sentence_transformer
    )
    
    # Retrieve context
    @chunks.query
    def get_context(query: str):
        sim = chunks.text.similarity(query)
        return chunks.order_by(sim, asc=False).limit(5)
    
    # Generate response
    docs['context'] = get_context(docs.question)
    docs['response'] = openai.chat_completions(
        messages=create_prompt(
            docs.context,
            docs.question
        )
    )
    ```
  </Accordion>

  <Accordion title="What video and image processing capabilities does Pixeltable offer?">
    Pixeltable excels at video/image workflows:

    ```python
    # Frame extraction
    frames = pxt.create_view(
        'video_search.frames',
        videos,
        iterator=FrameIterator.create(
            video=videos.video,
            fps=1
        )
    )
    
    # Object detection 
    frames.add_computed_column(
        detections=yolox(
            frames.frame,
            model_id='yolox_tiny',
            threshold=0.25
        )
    )
    
    # Cross-modal search
    frames.add_embedding_index(
        'frame',
        string_embed=clip_text,    # Text-to-video search
        image_embed=clip_image     # Image-to-video search
    )
    ```
  </Accordion>
</AccordionGroup>

## Integration & Deployment

<AccordionGroup>
  <Accordion title="What AI services can I integrate with Pixeltable?">
    Pixeltable provides managed integrations:

    ```python
    from pixeltable.functions import openai, anthropic
    from pixeltable.functions.huggingface import (
        sentence_transformer,
        clip_image,
        clip_text
    )
    
    # OpenAI
    table.add_computed_column(
        embeddings=openai.embeddings(
            table.text,
            model='text-embedding-3-small'
        )
    )
    
    # Claude
    table.add_computed_column(
        analysis=anthropic.messages(
            model='claude-3-sonnet',
            messages=table.prompt
        )
    )
    
    # Hugging Face
    table.add_computed_column(
        image_embeddings=clip_image(
            table.image, 
            model_id='openai/clip-vit-base-patch32'
        )
    )
    ```
  </Accordion>

  <Accordion title="How can I use Pixeltable with web frameworks?">
    Pixeltable integrates seamlessly with FastAPI and Gradio:

    ```python
    # FastAPI + Pixeltable example
    @app.post("/chat")
    async def chat(message: ChatMessage):
        # Store message
        conversations.insert([{
            'role': 'user',
            'content': message.message,
            'timestamp': datetime.now()
        }])
        
        # Get response via RAG
        chat_table['context'] = chunks.queries.get_context(
            chat_table.question
        )
        
        chat_table['response'] = openai.chat_completions(
            messages=create_prompt(
                chat_table.context,
                chat_table.question
            )
        )
        
        return JSONResponse(
            status_code=200,
            content={"response": response}
        )
    ```
  </Accordion>
</AccordionGroup>

## Using Pixeltable

<AccordionGroup>
  <Accordion title="Who should use Pixeltable?">
    Pixeltable serves multiple user groups:

    **ML Engineers & Data Scientists**
    - Streamlined experimentation
    - Reduced infrastructure overhead
    - Version control for data and models
    - Quick iteration on ML workflows

    **AI Application Developers**
    - Unified workflow for multimodal data
    - Simplified RAG development 
    - Easy integration with ML models
    - Production-ready features

    **Enterprise Teams**
    - Scalable data management
    - Complete lineage tracking
    - Cost optimization
    - Governance support
  </Accordion>

  <Accordion title="What makes Pixeltable unique?">
    Key differentiators:

    1. **Declarative Design**
    - Express complex workflows as computed columns
    - Automatic dependency management
    - Simple table-based operations
    - Complete lineage tracking

    2. **Multimodal Support**
    - Unified interface for all data types
    - Built-in transformations
    - Cross-modal operations
    - Efficient storage and processing

    3. **Incremental Processing**
    - Smart recomputation
    - Built-in caching
    - Automatic versioning
    - Cost optimization
  </Accordion>

  <Accordion title="What are Pixeltable's core use cases?">
    Common applications include:

    **RAG Development**
    - Document processing and chunking
    - Embedding generation and search
    - LLM integration and context management
    - Multi-modal RAG (text, video, audio)

    **Video/Image Analysis**
    - Frame extraction and processing
    - Object detection and tracking
    - Cross-modal search
    - Video transcription and analysis

    **ML Applications**
    - Training data management
    - Feature engineering workflows
    - Model inference orchestration
    - Experiment tracking
  </Accordion>
</AccordionGroup>