---
title: "Website"
description: "Build a web content search system using smart chunking and vector embeddings"
icon: "globe"
---

# Building a Website Search Workflow

Pixeltable website search works in two phases:
1. Define your workflow structure (once)
2. Query your content database (anytime)

<Steps>
  <Step title="Install Dependencies">
    ```bash
    pip install pixeltable tiktoken sentence-transformers
    ```
  </Step>

  <Step title="Define Your Workflow" icon="diagram-project">
    Create `table.py`:
    ```python
    import pixeltable as pxt
    from pixeltable.iterators import DocumentSplitter
    from pixeltable.functions.huggingface import sentence_transformer

    # Initialize app structure
    pxt.drop_dir("web_search", force=True)
    pxt.create_dir("web_search")

    # Create website table
    websites_t = pxt.create_table(
        "web_search.websites", 
        {"website": pxt.Document}
    )

    # Create chunked view for efficient processing
    websites_chunks = pxt.create_view(
        "web_search.website_chunks",
        websites_t,
        iterator=DocumentSplitter.create(
            document=websites_t.website,
            separators="token_limit",
            limit=300  # Tokens per chunk
        )
    )

    # Configure embedding model
    embed_model = sentence_transformer.using(
        model_id="intfloat/e5-large-v2"
    )

    # Add search capability
    websites_chunks.add_embedding_index(
        column="text",
        string_embed=embed_model
    )

    ```
  </Step>

  <Step title="Use Your Workflow" icon="play">
    Create `app.py`:
    ```python
    import pixeltable as pxt
    import time

    # Connect to your tables
    websites_t = pxt.get_table("web_search.websites")
    websites_chunks = pxt.get_table("web_search.website_chunks")

    # Add websites with rate limiting
    urls = [
        "https://quotes.toscrape.com/",
        "https://example.com",
    ]

    websites_t.insert({"website": url} for url in urls)

    # Search content
    query = "Find inspirational quotes about life"
    sim = websites_chunks.text.similarity(query)
    top_k = 3
    results = (
        websites_chunks.order_by(sim, asc=False)
        .select(
            websites_chunks.text,
            websites_chunks.website,
            similarity=sim
        )
        .limit(top_k)
    ).collect()

    # Print results
    for r in results:
        print(f"Similarity: {r['similarity']:.3f}")
        print(f"Source: {r['website']}")
        print(f"Content: {r['text']}\n")
```
  </Step>
</Steps>

## What Makes This Different?

<CardGroup cols={1}>
  <Card title="Web Scraping" icon="spider-web">
    Automatic content extraction:
    ```python
    websites_t.insert([{"website": "https://example.com"}])
    ```
  </Card>

  <Card title="Smart Chunking" icon="puzzle-piece">
    Token-aware content splitting:
    ```python
    iterator=DocumentSplitter.create(
        document=websites_t.website,
        separators="token_limit"
    )
    ```
  </Card>

  <Card title="Vector Search" icon="magnifying-glass">
    Natural language search:
    ```python
    websites_chunks.add_embedding_index(
        column="text",
        string_embed=embed_model
    )
    ```
  </Card>
</CardGroup>

## Workflow Components

<AccordionGroup>
  <Accordion title="Web Processing" icon="globe">
    Advanced web handling:
    - HTML content extraction
    - Text cleaning and normalization
    - Structure preservation
    - Automatic encoding detection
  </Accordion>

  <Accordion title="Content Chunking" icon="scissors">
    Intelligent text splitting:
    - Token-aware segmentation
    - Configurable chunk sizes
    - Context preservation
    - Multiple chunking strategies
  </Accordion>

  <Accordion title="Vector Search" icon="network-wired">
    High-quality search:
    - E5 text embeddings
    - Fast similarity search
    - Natural language queries
    - Configurable similarity thresholds
  </Accordion>
</AccordionGroup>

## Advanced Usage

### Custom Chunking Strategies

Configure different chunking approaches:

```python
# Chunk by paragraphs
chunks_by_para = pxt.create_view(
    "web_search.para_chunks",
    websites_t,
    iterator=DocumentSplitter.create(
        document=websites_t.website,
        separators="paragraph"
    )
)

# Chunk by fixed size
chunks_by_size = pxt.create_view(
    "web_search.size_chunks",
    websites_t,
    iterator=DocumentSplitter.create(
        document=websites_t.website,
        separators="fixed",
        size=1000  # characters
    )
)
```

### Advanced Search Functions

Create specialized search functions:

```python
@pxt.query
def search_with_metadata(
    query: str,
    min_similarity: float,
    limit: int
):
    sim = websites_chunks.text.similarity(query)
    return (
        websites_chunks.where(sim >= min_similarity)
        .order_by(sim, asc=False)
        .select(
            websites_chunks.text,
            websites_chunks.website,
            similarity=sim
        )
        .limit(limit)
    )
```