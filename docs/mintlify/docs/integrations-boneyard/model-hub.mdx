---
title: 'Model Hub & Repositories'
description: 'Access pre-trained models and repositories available in Pixeltable'
icon: 'cube'
---

Pixeltable provides seamless access to thousands of pre-trained models through major model hubs and repositories. Use state-of-the-art models directly in your data workflows without complex setup.

## Model Hubs

<CardGroup cols={2}>
  <Card
    title="Hugging Face Hub"
    icon="face-smile"
    href="https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-hugging-face.ipynb"
  >
    Access thousands of pre-trained models across vision, text, and audio domains
  </Card>

  <Card
    title="Replicate"
    icon="clone"
    href="https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-replicate.ipynb"
  >
    Deploy and run ML models through Replicate's cloud infrastructure
  </Card>
</CardGroup>

## Hugging Face Models

Pixeltable provides built-in UDFs for popular Hugging Face model types. These functions let you use state-of-the-art models directly in computed columns.

<Note>
Requirements: Install dependencies with `pip install transformers`. Some models may require additional packages like `sentence-transformers` or `torch`.
</Note>

### CLIP Models
Perfect for multimodal applications combining text and image understanding.

```python
from pixeltable.functions.huggingface import clip

# Text embedding
t.add_computed_column(
    text_embedding=clip(
        t.text_column,
        model_id='openai/clip-vit-base-patch32'
    )
)

# Image embedding  
t.add_computed_column(
    image_embedding=clip(
        t.image_column,
        model_id='openai/clip-vit-base-patch32'
    )
)
```

### Object Detection (DETR)
End-to-end transformer architecture for powerful object detection.

```python
from pixeltable.functions.huggingface import detr_for_object_detection

t.add_computed_column(
    detections=detr_for_object_detection(
        t.image,
        model_id='facebook/detr-resnet-50',
        threshold=0.8
    )
)

# Convert to COCO format if needed
t.add_computed_column(
    coco_format=detr_to_coco(t.image, t.detections)
)
```

### Sentence Transformers
State-of-the-art sentence and document embeddings for semantic search.

```python
from pixeltable.functions.huggingface import sentence_transformer

t.add_computed_column(
    embeddings=sentence_transformer(
        t.text,
        model_id='sentence-transformers/all-mpnet-base-v2',
        normalize_embeddings=True
    )
)
```

### Speech Recognition
Support for both transcription and translation of audio content.

```python
from pixeltable.functions.huggingface import speech2text_for_conditional_generation

# Basic transcription
t.add_computed_column(
    transcript=speech2text_for_conditional_generation(
        t.audio,
        model_id='facebook/s2t-small-librispeech-asr'
    )
)

# Multilingual translation
t.add_computed_column(
    translation=speech2text_for_conditional_generation(
        t.audio,
        model_id='facebook/s2t-medium-mustc-multilingual-st',
        language='fr'
    )
)
```

### Vision Transformer (ViT)
Modern image classification using transformer architecture.

```python
from pixeltable.functions.huggingface import vit_for_image_classification

t.add_computed_column(
    classifications=vit_for_image_classification(
        t.image,
        model_id='google/vit-base-patch16-224',
        top_k=5
    )
)
```

### Cross-Encoders
Ideal for semantic similarity tasks and sentence pair classification.

```python
from pixeltable.functions.huggingface import cross_encoder

t.add_computed_column(
    similarity_score=cross_encoder(
        t.sentence1,
        t.sentence2,
        model_id='cross-encoder/ms-marco-MiniLM-L-4-v2'
    )
)
```

## ðŸ’€ Model Output Formats ðŸ’€

Understanding the structure of model outputs helps you work with results effectively:

<AccordionGroup>
  <Accordion title="Object Detection Output">
    ```python
    # DETR Object Detection Output
    {
        'scores': [0.99, 0.98],  # confidence scores
        'labels': [25, 30],      # class labels
        'label_text': ['cat', 'dog'], # human-readable labels
        'boxes': [[x1, y1, x2, y2], ...] # bounding boxes
    }
    ```
  </Accordion>

  <Accordion title="Image Classification Output">
    ```python
    # ViT Classification Output
    {
        'scores': [0.8, 0.15],   # class probabilities
        'labels': [340, 353],    # class IDs
        'label_text': ['zebra', 'gazelle'] # class names
    }
    ```
  </Accordion>

  <Accordion title="Embedding Output">
    ```python
    # CLIP/Sentence Transformer Output
    # Returns numpy array of shape (embedding_dim,)
    array([0.1, -0.3, 0.7, ...])  # 512 or 768 dimensions typically
    ```
  </Accordion>
</AccordionGroup>

## ðŸ’€ Model Selection Guide ðŸ’€

<CardGroup cols={3}>
  <Card title="Text/Image Similarity" icon="arrows-left-right">
    **Use:** CLIP models
    
    **Best for:** Multimodal search, content matching, zero-shot classification
  </Card>

  <Card title="Object Detection" icon="camera">
    **Use:** DETR models
    
    **Best for:** Computer vision pipelines, automated annotation, video analysis
  </Card>

  <Card title="Text Embeddings" icon="text">
    **Use:** Sentence Transformers
    
    **Best for:** Semantic search, document similarity, clustering
  </Card>

  <Card title="Speech Processing" icon="waveform">
    **Use:** Speech2Text models
    
    **Best for:** Audio transcription, multilingual translation, accessibility
  </Card>

  <Card title="Image Classification" icon="image">
    **Use:** Vision Transformers
    
    **Best for:** Content categorization, automated tagging, quality control
  </Card>

  <Card title="Text Similarity" icon="scale">
    **Use:** Cross-Encoders
    
    **Best for:** Precise similarity scoring, reranking, question answering
  </Card>
</CardGroup>
