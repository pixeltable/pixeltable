---
title: 'Building with Pixeltable'
description: 'Stop choosing between months of data infrastructure work or limited AI frameworks. Pixeltable gives you a **declarative**, **incremental** approach for building **multimodal** AI products in days, not months. One python library for storage, orchestration, vector search, retrieval, and more.'
icon: "rocket"
---

<CardGroup cols={2}>
<Card 
  title="Open Source Data Infrastructure" 
  icon="database"
  href="/docs/datastore/tables-and-operations"
>
  - All AI data & models in one table interface
</Card>

<Card 
  title="Storage meets Orchestration" 
  icon="brain-circuit"
  href="/docs/datastore/computed-columns"
>
  - A declarative interface for AI workloads
</Card>
</CardGroup>

<Note>
The below steps will get you started in 1 minute. Learn more by looking at this tutorial on [Github](https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/pixeltable-basics.ipynb).
</Note>

## Getting Started

<Steps>
  <Step title="Start Building (Step 1)" icon="code">
    ```bash
    pip install pixeltable
    ```
    Get up and running with basic [tables](/docs/datastore/tables-and-operations), [queries](/docs/datastore/filtering-and-selecting), and [computed columns](docs/datastore/computed-columns).

    <CodeGroup>
    ```python table
    # Create a table to hold data
    t = pxt.create_table('films_table', {
        'name': pxt.String,
        'revenue': pxt.Float,
        'budget': pxt.Float
    })
    ```
    ```python query
    # Insert data into a table
    t.insert([
      {'name': 'Inside Out', 'revenue': 800.5, 'budget': 200.0},
      {'name': 'Toy Story', 'revenue': 1073.4, 'budget': 200.0}
    ])

  # Retrieves all the rows in the table.
  t.collect()
    ```
    ```python transform
    # Add a new column for the profit calculation
    t.add_computed_column(profit=(t.revenue - t.budget))

    # It will automatically compute its value for all rows
    t.select(t.profit).head()
    ``` 
    </CodeGroup>

  <Info>
  All data and computed results are automatically stored and versioned.
  </Info>
  </Step>
  
  <Step title="Add Processing (Step 2)" icon="sparkles">
    Add [LLMs](docs/integrations/frameworks#cloud-llm-providers), [computer vision](/docs/examples/vision/yolox), [embeddings indices](docs/datastore/embedding-index), and build your first [multimodal app](/docs/examples/chat/multimodal).
      <CodeGroup>
    ```python embedding index
    from pixeltable.functions.huggingface import clip
    import PIL.Image

    # create embedding index on the 'img' column of table 't'
    t.add_embedding_index(
        'img',
        embedding=clip.using(model_id='openai/clip-vit-base-patch32')
    )

    # index is kept up-to-date enabling relevant searches
    sim = t.img.similarity(sample_img)

    res = (
        t.order_by(sim, asc=False)  # Order by similarity
        .where(t.id != 6)  # Metadata filtering
        .limit(2)  # Limit number of results to 2
        .select(t.id, t.img, sim)
        .collect()  # Retrieve results now
    )
    ```
    ```python llms
    from pixeltable.functions import openai

    # Assemble the prompt and instructions
    messages = [
        {
            'role': 'system',
            'content': 'Please read the following passages.'
        },
        {
            'role': 'user',
            'content': t.prompt # generated from the 'prompt' column
        }
    ]

    # Add a computed column that calls OpenAI
    t.add_computed_column(
        response=openai.chat_completions(model='gpt-4o-mini',
        messages=messages)
    )
        ```
        ```python computer vision
    from pixeltable.ext.functions.yolox import yolox

    # compute object detections using the `yolox_tiny` model
    frames_view.add_computed_column(detect_yolox_tiny=yolox(
        frames_view.frame, model_id='yolox_tiny', threshold=0.25
    ))

    # The inference in the computed column is now stored
    frames_view.select(
        frames_view.frame,
        frames_view.detect_yolox_tiny
    ).show(3)
    ``` 
    </CodeGroup>
  <Info>
  Define transformations once, they run on new data. Perfect for AI orchestration.
  </Info>
  </Step>
  
  <Step title="Scale Up (Step 3" icon="chart-mixed">
    Handle [production data](/docs/datastore/bringing-data) volumes, and deploy your application.
      <CodeGroup>
    ```python bring cloud data
    # Import media data (videos, images, audio...)
    v = pxt.create_table('videos', {'video': pxt.Video})

    prefix = 's3://multimedia-commons/'
    paths = [
        'data/videos/mp4/ffe/ffb/ffeffbef41bbc269810b2a1a888de.mp4',
        'data/videos/mp4/ffe/feb/ffefebb41485539f964760e6115fbc44.mp4',
        'data/videos/mp4/ffe/f73/ffef7384d698b5f70d411c696247169.mp4'
    ]
    v.insert({'video': prefix + p} for p in paths)
    ```
    ```python chunking with views
    # Optimize large-scale data processing
    from pixeltable.iterators import DocumentSplitter

    # Create chunked views for efficient processing
    doc_chunks = pxt.create_view(
        'chunks',
        analysis,
        iterator=DocumentSplitter.create(
            document=analysis.document,
            separators='sentence',
            limit=500  # Control chunk size
        )
    )
    ```
    ```python serving
    from fastapi import FastAPI, HTTPException
    from pydantic import BaseModel

    app = FastAPI()

    class AnalysisRequest(BaseModel):
        document: str
        metadata: dict = {}
    
    @app.post("/analyze")
    async def analyze_document(request: AnalysisRequest):
        try:
            # Insert document for processing
            analysis.insert([{
                'document': request.document,
                'metadata': request.metadata,
                'timestamp': datetime.now()
            }])
            
            # Get analysis results using computed columns
            result = analysis.select(
                analysis.embeddings,
                analysis.summary,
                analysis.sentiment
            ).tail(1)
            
            return {
                "status": "success",
                "results": result.to_dict('records')[0]
            }
        except Exception as e:
            raise HTTPException(status_code=0, detail=str(e))
    ```
    </CodeGroup>
  <Info>
Handle images, video, audio, numbers, array and text seamlessly in one interface.
</Info>
  </Step>
</Steps>

## Popular Use Cases

<CardGroup cols={2}>
<Card 
  title="Enterprise Chat Systems" 
  icon="message"
  href="/docs/examples/chat/multimodal"
>
  <CardGroup cols={1}>
    <Card title="Multi-model RAG Chatbot" icon="brain-circuit" href="https://huggingface.co/spaces/Pixeltable/Multi-LLM-RAG-with-Groundtruth-Comparison">
      Build RAG systems that compare multiple LLMs with ground truth evaluation.
    </Card>
    <Card title="Discord AI Assistant" icon="discord" href="https://github.com/pixeltable/pixeltable/tree/main/examples/discord-bot">
      Create context-aware chat bots with semantic search and memory.
    </Card>
    <Card title="Agentic Workflows" icon="toolbox" href="/docs/examples/chat/tools">
      Build a swarm of agents with access to unlimited tools
    </Card>
  </CardGroup>
</Card>

<Card 
  title="Visual Understanding" 
  icon="camera"
  href="https://huggingface.co/spaces/Pixeltable/object-detection-in-videos-with-yolox"
>
  <CardGroup cols={1}>
    <Card title="Video Object Detection" icon="video" href="http://localhost:3000/docs/examples/vision/yolox">
      Real-time object detection in videos using YOLOX.
    </Card>
    <Card title="Visual Search Engine" icon="magnifying-glass" href="https://huggingface.co/spaces/Pixeltable/Text-image-similarity-search-on-video-frames-embedding-indexes">
      Text and image similarity search on video frames.
    </Card>
    <Card title="Call Analysis Tool" icon="phone" href="https://huggingface.co/spaces/Pixeltable/Call-Analysis-AI-Tool">
      Analyze video calls with automatic transcription and insights.
    </Card>
  </CardGroup>
</Card>

<Card 
  title="Document Intelligence" 
  icon="file-lines"
  href="https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/use-cases/audio-transcriptions.ipynb"
>
  <CardGroup cols={1}>
    <Card title="Document Audio Synthesis" icon="file-audio" href="http://localhost:3000/docs/fundamentals/sample-apps">
      Convert documents to natural speech with context-aware processing.
    </Card>
    <Card title="Social Media Generator" icon="share-nodes" href="https://huggingface.co/spaces/Pixeltable/video-to-social-media-post-generator">
      Generate social posts from video content analysis.
    </Card>
    <Card title="Collaborative Writing" icon="pen-to-square" href="https://huggingface.co/spaces/Pixeltable/Collaborative-Story-Builder">
      Build AI-powered collaborative writing tools.
    </Card>
  </CardGroup>
</Card>

<Card 
  title="Vertical AI Products" 
  icon="code"
  href="https://huggingface.co/spaces/Pixeltable/Prompt-Engineering-and-LLM-Studio"
>
  <CardGroup cols={1}>
    <Card title="LLM Studio" icon="flask" href="https://huggingface.co/spaces/Pixeltable/Prompt-Engineering-and-LLM-Studio">
      Test and compare LLM performance with structured evaluation.
    </Card>
    <Card title="AI-Based Trading Chrome Extension" icon="chart-line" href="https://chromewebstore.google.com/detail/ai-based-day-trading-insi/floglldkiolbdpcfeanilapjmilliiac">
      Real-time trading analysis using AI for technical indicators.
    </Card>
    <Card title="RPG Adventure" icon="gamepad" href="https://huggingface.co/spaces/Pixeltable/AI-RPG-Adventure">
      Create interactive AI storytelling experiences.
    </Card>
  </CardGroup>
</Card>
</CardGroup>

## Next Steps

<CardGroup cols={3}>
  <Card 
    title="Explore Examples" 
    icon="rocket" 
    href="http://localhost:3000/docs/fundamentals/sample-apps"
  >
    Check out our apps
  </Card>
  <Card 
    title="Join Discord" 
    icon="discord" 
    href="https://discord.gg/QPyqFYx2UN"
  >
    Get help and mingle
  </Card>
  <Card 
    title="GitHub" 
    icon="github" 
    href="https://github.com/pixeltable/pixeltable"
  >
    Star us on GitHub
  </Card>
</CardGroup>