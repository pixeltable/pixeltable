---
title: '10-min tour of Pixeltable'
description: 'Welcome to Pixeltable! In this tutorial, we will survey how to create tables, populate them with data, and enhance them with built-in and user-defined transformations and AI operations.'
icon: 'rocket'
---

<Note>
This guide will get you from zero to a working AI application in under 5 minutes. Learn more by looking at this tutorial on [Github](https://github.com/pixeltable/pixeltable/blob/main/docs/notebooks/pixeltable-basics.ipynb).
</Note>

## Create Your First AI Workfload

Let's build a simple image analysis app that combines object detection and GPT-4 Vision.

## Installation

  ```bash
  pip install -qU torch transformers openai pixeltable
  ```

<Steps>
  <Step title="Create Table Structure">
    ```python
    import pixeltable as pxt
    from pixeltable.functions import huggingface, openai
    
    # Create directory for our app
    pxt.drop_dir('demo', force=True)  
    pxt.create_dir('demo')
    
    # Create table with image column
    t = pxt.create_table('demo.first', {'input_image': pxt.Image})
    ```

    <Tip>
    Create a persistent and verstioned table that holds data.
    </Tip>
  </Step>

  <Step title="Add Object Detection">
    ```python
    # Add ResNet-50 object detection
    t.add_computed_column(
        detections=huggingface.detr_for_object_detection(
            t.input_image, 
            model_id='facebook/detr-resnet-50'
        )
    )
    
    # Extract just the labels
    t.add_computed_column(detections_text=t.detections.label_text)
    ```

    <Tip>
    Computed columns are automatically populated whenever new data is added to their input columns.
    </Tip>
  </Step>

 <Step title="Add your OpenAI Key">
  ```python
  import os
  import getpass
  if 'OPENAI_API_KEY' not in os.environ:
    os.environ['OPENAI_API_KEY'] = getpass.getpass('Enter your OpenAI API key:')
  ```
  </Step>

  <Step title="Add GPT-4 Vision Analysis">
    ```python
    # Add GPT-4 Vision analysis
    t.add_computed_column(
        vision=openai.vision(
            prompt="Describe what's in this image.",
            image=t.input_image,
            model='gpt-4o-mini'
        )
    )
    ```
  </Step>

  <Step title="Use Your App">
    ```python
  # Insert an image
  t.insert(input_image='https://raw.github.com/pixeltable/pixeltable/release/docs/resources/images/000000000025.jpg')

  # Retrieve results
  t.select(
      t.input_image,
      t.detections_text,
      t.vision
  ).collect()
    ```
  </Step>
</Steps>

<Accordion title="What happened behind the scenes?">
  - Pixeltable automatically:
    1. Created a persistent table
    2. Downloaded and cached the ResNet model
    3. Orchestrate the GPT-4 API call
    4. Created an efficient processing pipeline
    5. Stored all results for future use
</Accordion>

## Key Features

<CardGroup cols={2}>
  <Card title="Persistent Storage" icon="database">
    All data and computed results are automatically stored and versioned. Your app state persists between sessions.
  </Card>
  <Card title="Computed Columns" icon="calculator">
    Define transformations once, they run automatically on new data. Perfect for ML pipelines.
  </Card>
  <Card title="Multimodal Support" icon="photo-film">
    Handle images, video, audio, and text seamlessly in one unified interface.
  </Card>
  <Card title="AI Integration" icon="brain">
    Built-in support for popular AI services like OpenAI, Hugging Face, and Anthropic.
  </Card>
</CardGroup>

## Custom Functions

Extend Pixeltable with your own functions using the `@pxt.udf` decorator:

```python
@pxt.udf
def top_detection(detect: dict) -> str:
    scores = detect['scores']
    label_text = detect['label_text']
    i = scores.index(max(scores))
    return label_text[i]

# Use it in a computed column
t.add_computed_column(top=top_detection(t.detections))
```

## See It In Action

<iframe
  width="560"
  height="315"
  src="https://www.youtube.com/embed/4KzFe50RQkQ"
  title="YouTube video player"
  frameborder="0"
  allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
  allowfullscreen
></iframe>

## Next Steps

<CardGroup cols={3}>
  <Card title="RAG Tutorial" icon="books" href="/tutorials/rag">
    Build a production-grade RAG system
  </Card>
  <Card title="Video Analysis" icon="video" href="/tutorials/video">
    Process video with object detection
  </Card>
  <Card title="LLM Integration" icon="message" href="/tutorials/llm">
    Work with OpenAI and other LLM providers
  </Card>
</CardGroup>