---
title: 'Integrating Custom Embedding Models'
description: 'Learn how to integrate custom embedding models with Pixeltable'
icon: 'brain'
---

Pixeltable provides extensive built-in support for popular embedding models, but you can also easily integrate your own custom embedding models. This guide shows you how to create and use custom embedding functions for any model architecture.

## Quick Start

Here's a simple example using a custom BERT model:

```python
import tensorflow as tf
import tensorflow_hub as hub
import pixeltable as pxt

@pxt.udf
def custom_bert_embed(text: str) -> pxt.Array[(512,), pxt.Float]:
    """Basic BERT embedding function"""
    preprocessor = hub.load('https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3')
    model = hub.load('https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/2')
    
    tensor = tf.constant([text])
    result = model(preprocessor(tensor))['pooled_output']
    return result.numpy()[0, :]

# Create table and add embedding index
docs = pxt.create_table('documents', {'text': pxt.String})
docs.add_embedding_index('text', string_embed=custom_bert_embed)
```

## Production Best Practices 

<Note>
The quick start example works but isn't production-ready. Below we'll cover how to optimize your custom embedding UDFs.
</Note>

### Model Caching

Always cache your model instances to avoid reloading on every call:

```python
@pxt.udf
def optimized_bert_embed(text: str) -> pxt.Array[(512,), pxt.Float]:
    """BERT embedding function with model caching"""
    if not hasattr(optimized_bert_embed, 'model'):
        # Load models once
        optimized_bert_embed.preprocessor = hub.load(
            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'
        )
        optimized_bert_embed.model = hub.load(
            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/2'
        )
    
    tensor = tf.constant([text])
    result = optimized_bert_embed.model(
        optimized_bert_embed.preprocessor(tensor)
    )['pooled_output']
    return result.numpy()[0, :]
```

### Batch Processing

Use Pixeltable's batching capabilities for better performance:

```python
from pixeltable.func import Batch

@pxt.udf(batch_size=32)
def batched_bert_embed(texts: Batch[str]) -> Batch[pxt.Array[(512,), pxt.Float]]:
    """BERT embedding function with batching"""
    if not hasattr(batched_bert_embed, 'model'):
        batched_bert_embed.preprocessor = hub.load(
            'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3'
        )
        batched_bert_embed.model = hub.load(
            'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/2'
        )
    
    # Process entire batch at once
    tensor = tf.constant(list(texts))
    results = batched_bert_embed.model(
        batched_bert_embed.preprocessor(tensor)
    )['pooled_output']
    return [r for r in results.numpy()]
```

## Error Handling

Always implement proper error handling in production UDFs:

```python
@pxt.udf
def robust_bert_embed(text: str) -> pxt.Array[(512,), pxt.Float]:
    """BERT embedding with error handling"""
    try:
        if not text or len(text.strip()) == 0:
            raise ValueError("Empty text input")
            
        if not hasattr(robust_bert_embed, 'model'):
            # Model initialization...
            pass
            
        tensor = tf.constant([text])
        result = robust_bert_embed.model(
            robust_bert_embed.preprocessor(tensor)
        )['pooled_output']
        
        return result.numpy()[0, :]
        
    except Exception as e:
        logger.error(f"Embedding failed: {str(e)}")
        raise
```

## Custom Model Examples

<AccordionGroup>
  <Accordion title="PyTorch Model">
    ```python
    import torch
    from transformers import AutoTokenizer, AutoModel
    
    @pxt.udf
    def pytorch_embed(text: str) -> pxt.Array[(768,), pxt.Float]:
        if not hasattr(pytorch_embed, 'model'):
            pytorch_embed.tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
            pytorch_embed.model = AutoModel.from_pretrained('bert-base-uncased')
        
        inputs = pytorch_embed.tokenizer(text, return_tensors='pt', padding=True, truncation=True)
        with torch.no_grad():
            outputs = pytorch_embed.model(**inputs)
            # Use CLS token embedding
            embedding = outputs.last_hidden_state[:, 0, :].squeeze()
        
        return embedding.numpy()
    ```
  </Accordion>

  <Accordion title="OpenAI Embeddings">
    ```python
    import openai
    
    @pxt.udf
    def openai_embed(text: str) -> pxt.Array[(1536,), pxt.Float]:
        """OpenAI text-embedding-3-small"""
        response = openai.embeddings.create(
            model="text-embedding-3-small",
            input=text
        )
        return response.data[0].embedding
    ```
  </Accordion>

  <Accordion title="Sentence Transformers">
    ```python
    from sentence_transformers import SentenceTransformer
    
    @pxt.udf
    def sbert_embed(text: str) -> pxt.Array[(384,), pxt.Float]:
        if not hasattr(sbert_embed, 'model'):
            sbert_embed.model = SentenceTransformer('all-MiniLM-L6-v2')
        
        embedding = sbert_embed.model.encode(text)
        return embedding
    ```
  </Accordion>
</AccordionGroup>

## Integration with Embedding Indexes

Once you have a custom embedding function, you can use it with Pixeltable's embedding indexes:

```python
# Create table
docs = pxt.create_table('documents', {
    'text': pxt.String,
    'title': pxt.String
})

# Add embedding index using custom function
docs.add_embedding_index('text', string_embed=custom_bert_embed)

# Insert some documents
docs.insert([
    {'text': 'Machine learning is fascinating', 'title': 'ML Article'},
    {'text': 'Deep learning transforms AI', 'title': 'DL Guide'}
])

# Perform similarity search
results = (
    docs.order_by(docs.text.similarity('artificial intelligence'))
    .limit(5)
    .select(docs.title, docs.text)
    .collect()
)
```

## Performance Tips

<Steps>
  <Step title="Model Caching">
    Always cache model instances using function attributes to avoid reloading
  </Step>
  
  <Step title="Batch Processing">
    Use Pixeltable's `@pxt.udf(batch_size=N)` for better throughput
  </Step>
  
  <Step title="GPU Utilization">
    Move models to GPU if available: `model.to('cuda')`
  </Step>

  <Step title="Error Handling">
    Implement robust error handling for production stability
  </Step>
</Steps>
