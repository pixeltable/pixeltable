{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90dbf9c4",
   "metadata": {},
   "source": [
    "# Comparing Object Detection Models for Video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f1dd62f",
   "metadata": {},
   "source": [
    "In this tutorial we'll demonstrate how to use Pixeltable to do frame-by-frame object detection, made simple through Pixeltable's video-related functionality:\n",
    "* automatic frame extraction\n",
    "* running complex functions against frames (in this case, an object detection model)\n",
    "* reassembling frames back into videos\n",
    "\n",
    "We'll be working with a single video file (from Pixeltable's test data directory). Let's download that now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ae7b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "\n",
    "download_url = 'https://raw.github.com/pixeltable/pixeltable/master/docs/source/data/bangkok.mp4'\n",
    "filename, _ = urllib.request.urlretrieve(download_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "638362bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29054d06",
   "metadata": {},
   "source": [
    "## Creating a tutorial directory and table\n",
    "\n",
    "In Pixeltable, all data resides in tables, which in turn located inside directories.\n",
    "\n",
    "Let's start by creating a client and a `video_tutorial` directory:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da120aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pixeltable as pxt\n",
    "\n",
    "cl = pxt.Client()\n",
    "cl.create_dir('model_comparison', ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217820e1",
   "metadata": {},
   "source": [
    "We create a table for our videos, with a single column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a8aea61",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = 'model_comparison.videos'\n",
    "frame_path = 'model_comparison.frames'\n",
    "cl.drop_table(frame_path, ignore_errors=True)\n",
    "cl.drop_table(video_path, ignore_errors=True)\n",
    "v = cl.create_table(video_path, {'video': pxt.VideoType()})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "655ce9c7",
   "metadata": {},
   "source": [
    "In order to interact with the frames, we take advantage of Pixeltable's component view concept: we create a \"view\" of our video table that contains one row for each frame. Pixeltable provides the built-in `FrameIterator` class for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2796b660",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixeltable.iterators import FrameIterator\n",
    "args = {'video': v.video, 'fps': 0}\n",
    "f = cl.create_view(frame_path, v, iterator_class=FrameIterator, iterator_args=args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ff4da5",
   "metadata": {},
   "source": [
    "The `fps` parameter determines the frame rate, with `0` indicating the native frame rate.\n",
    "\n",
    "Running this creates a view with six columns:\n",
    "- `frame_idx`, `pos_msec`, `pos_frame` and `frame` are created by the `FrameIterator` class.\n",
    "- `pos` is a system column in every component view\n",
    "- `video` is the column for our base table (all base table columns are visible in the view, to facilitate querying)\n",
    "\n",
    "Note that you could create additional views on the `videos` table, each with its own frame rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3b30a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74c5d16",
   "metadata": {},
   "source": [
    "We now insert a single row containing the name of the video file we just downloaded, which is expanded into 462 frames/rows in the `frames` view.\n",
    "\n",
    "In general, `insert()` takes as its first argument a list of rows, each of which is a dictionary mapping column names to column values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a49849e",
   "metadata": {},
   "outputs": [],
   "source": [
    "v.insert([{'video': filename}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cae0de07",
   "metadata": {},
   "source": [
    "We loaded a video that shows a busy intersection in Bangkok. Let's look at the first frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4778d647",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.where(f.pos == 200).select(f.frame, f.frame.width, f.frame.height).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d3efad",
   "metadata": {},
   "source": [
    "When we create the `frames` view, Pixeltable does not physically store the frames. Instead, Pixeltable re-extracts the frames on retrieval using the `pos` column value, which can be done very efficiently and avoids any storage overhead (which would be very substantial for video frames)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755edf69",
   "metadata": {},
   "source": [
    "## Object detection with Pixeltable\n",
    "\n",
    "Pixeltable comes pre-packaged with a number of object detection models. We're going to explore one from the YoloX family."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8405cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixeltable.functions.nos.object_detection_2d import yolox_tiny as model1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0157b86",
   "metadata": {},
   "source": [
    "We can then use `model1()` in the Pixeltable index operator using standard Python function call syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd2ab9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.where(f.frame_idx == 0).select(f.frame, model1(f.frame)).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13f07d2e",
   "metadata": {},
   "source": [
    "This works as expected, and we now add the detections as a computed column `detections_1` to the table (there'll be a `detections_2` later).\n",
    "\n",
    "Running model inference is generally an expensive operation; adding it as a computed column makes sure it only runs once, at the time the row is inserted. After that, the result is available as part of the stored table data.\n",
    "\n",
    "Note that for computed columns of any type other than `image`, the computed values are **always** stored (ie, `stored=True`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dab11f",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.add_column(detections_1=model1(f.frame))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bc1180",
   "metadata": {},
   "source": [
    "The column is now part of `f`'s schema:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0550162",
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3555b0e8",
   "metadata": {},
   "source": [
    "We can create a simple user-defined function `draw_boxes()` to visualize detections:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63db8593",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL.ImageDraw\n",
    "\n",
    "@pxt.udf(return_type=pxt.ImageType(), param_types=[pxt.ImageType(), pxt.JsonType()])\n",
    "def draw_boxes(img, boxes):\n",
    "    result = img.copy()\n",
    "    d = PIL.ImageDraw.Draw(result)\n",
    "    for box in boxes:\n",
    "        d.rectangle(box, width=3)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a2a429c",
   "metadata": {},
   "source": [
    "This function takes two arguments:\n",
    "- `img` has type `image` and receives an instance of `PIL.Image.Image`\n",
    "- `boxes` has type `json` and receives a JSON-serializable structure, in this case a list of 4-element lists of floats\n",
    "\n",
    "When we \"call\" this function, we need to pass in the frame and the bounding boxes identified in that frame. The latter can be selected with the JSON path expression `t.detections.boxes`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bbd1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.where(f.pos == 0).select(f.frame, draw_boxes(f.frame, f.detections_1.bboxes)).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6943594",
   "metadata": {},
   "source": [
    "Looking at individual frames gives us some idea of how well our detection algorithm works, but it would be more instructive to turn the visualization output back into a video.\n",
    "\n",
    "We do that with the built-in function `make_video()`, which is an aggregation function that takes a frame index (actually: any expression that can be used to order the frames; a timestamp would also work) and an image, and then assembles the sequence of images into a video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce224393",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.select(pxt.make_video(f.pos, draw_boxes(f.frame, f.detections_1.bboxes))).group_by(v).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf3bfbe",
   "metadata": {},
   "source": [
    "## Comparing multiple detection models\n",
    "\n",
    "The output of YoloX-tiny seems reasonable, but we're curious how much better a slightly larger model, such as YoloX-medium, would be for our particular use case. Instead of creating another table and reloading the data, etc., we can simply add another column to our existing table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4500c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixeltable.functions.nos.object_detection_2d import yolox_medium as model2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22cbe6a3",
   "metadata": {},
   "source": [
    "We're using the alternative form of adding table columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c4d37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "f['detections_2'] = model2(f.frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd99a78d",
   "metadata": {},
   "source": [
    "We don't have ground truth data yet, but visualizing the output in the form of a video gives us some clue how much a smaller model affects the result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb774696",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f.select(\n",
    "    pxt.make_video(f.frame_idx, draw_boxes(f.frame, f.detections_1.bboxes)),\n",
    "    pxt.make_video(f.frame_idx, draw_boxes(f.frame, f.detections_2.bboxes)),\n",
    ").group_by(v).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32e72df6",
   "metadata": {},
   "source": [
    "# Evaluating the models against ground truth\n",
    "\n",
    "In order to have something to base the evaluation on, let's generate some 'ground truth' data by running the largest YoloX model available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02497076",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixeltable.functions.nos.object_detection_2d import yolox_xlarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c688d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f['gt'] = yolox_xlarge(f.frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea60aba",
   "metadata": {},
   "source": [
    "We now have two columns with detections, `detections_1` and `detections_2`, and one column `gt` with synthetic ground-truth data, which we're going to use as the basis for evaluation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d0ae04",
   "metadata": {},
   "outputs": [],
   "source": [
    "f"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc38c3a",
   "metadata": {},
   "source": [
    "We're going to be evaluating the generated detections with the commonly-used [mean average precision metric](https://learnopencv.com/mean-average-precision-map-object-detection-model-evaluation-metric/) (mAP).\n",
    "\n",
    "The mAP metric is based on per-frame metrics, such as true and false positives per detected class, which are then aggregated into a single (per-class) number. In Pixeltable, functionality is available via the `eval_detections()` and `mean_ap()` built-in functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80bae1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixeltable.functions.eval import eval_detections, mean_ap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6598b1b",
   "metadata": {},
   "source": [
    "The `eval_detections()` function computes the required per-frame metrics, and we're going to add those as computed columns in order to cache the output (and avoid having to re-type the call to `eval_detections()` repeatedly later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770a4dcc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f['eval_1'] = eval_detections(\n",
    "    f.detections_1.bboxes, f.detections_1.labels, f.detections_1.scores, f.gt.bboxes, f.gt.labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c135a70",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f['eval_2'] = eval_detections(\n",
    "    f.detections_2.bboxes, f.detections_2.labels, f.detections_2.scores, f.gt.bboxes, f.gt.labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ff24a4",
   "metadata": {},
   "source": [
    "Let's take a look at the output:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a84c07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.select(f.eval_1, f.eval_2).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d0f9d6",
   "metadata": {},
   "source": [
    "The computation of the mAP metric is now simply a query over the evaluation output, aggregated with the `mean_ap()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c83f9c47",
   "metadata": {},
   "outputs": [],
   "source": [
    "f.select(mean_ap(f.eval_1), mean_ap(f.eval_2)).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d701e034",
   "metadata": {},
   "source": [
    "This two-step process allows you to compute mAP at every granularity: over your entire dataset, only for specific videos, only for videos that pass a certain filter, etc. Moreover, you can compute this metric any time, not just during training, and use it to guide your understand of your dataset and how it affects the quality of your models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763ac9ab",
   "metadata": {},
   "source": [
    "# Exporting Detection Data as a COCO Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74862c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pxt.udf(return_type=pxt.JsonType(nullable=False), param_types=[pxt.JsonType(nullable=False)])\n",
    "def yolo_to_coco(detections):\n",
    "    bboxes, labels = detections['bboxes'], detections['labels']\n",
    "    num_annotations = len(detections['bboxes'])\n",
    "    assert num_annotations == len(detections['labels'])\n",
    "    result = []\n",
    "    for i in range(num_annotations):\n",
    "        bbox = bboxes[i]\n",
    "        ann = {\n",
    "            'bbox': [round(bbox[0]), round(bbox[1]), round(bbox[2] - bbox[0]), round(bbox[3] - bbox[1])],\n",
    "            'category': labels[i],\n",
    "        }\n",
    "        result.append(ann)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bbf5226",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
