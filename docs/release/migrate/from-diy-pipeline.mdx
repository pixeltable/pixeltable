---
title: 'Coming from a DIY Pipeline'
description: 'How to replace hand-rolled S3 + Lambda + SQS + API pipelines with Pixeltable'
---

If you've been wrangling multimodal data with a hand-built pipeline -- S3 for storage, Lambda or scripts for processing, direct API calls to OpenAI/Anthropic with manual retries, SQS for queuing, embeddings dumped into Parquet files or a Postgres table -- this guide is for you.

This is the most common starting point. There's no single framework to "migrate from", just a collection of AWS services, Python scripts, and glue code that somehow works. Here's how all of that maps to Pixeltable.

---

## Concept Mapping

| Your DIY Pipeline | Pixeltable | Notes |
|---|---|---|
| S3 buckets for images/video/audio/PDFs | `pxt.Image`, `pxt.Video`, `pxt.Audio`, `pxt.Document` columns | Native media types with automatic format handling. Can still read from and write to S3. |
| Lambda functions or cron scripts for processing | Computed columns (`add_computed_column()`) | Runs automatically on new data. No scheduling, no triggers to wire up. |
| SQS / Step Functions for orchestration | Column dependency graph | Pixeltable resolves execution order from column references. No queues to manage. |
| Direct `openai.chat.completions.create()` calls | `openai.chat_completions()` as a computed column | Built-in rate limiting, error handling, and result caching. |
| Manual retry logic (`tenacity`, `backoff`, try/except loops) | Automatic retries on transient failures | Pixeltable retries failed computations; successful results are cached. |
| Embeddings stored in Parquet/CSV/Postgres/S3 | `add_embedding_index()` | Built-in vector storage with HNSW indexing and `.similarity()` search. |
| `boto3` for uploading/downloading media | `insert()` with S3 URLs or local paths | Pixeltable handles file references, caching, and format detection. |
| Custom ffmpeg scripts for video frames | `FrameIterator` via `create_view()` | Declarative: specify FPS, frames are extracted automatically for all current and future videos. |
| Manual PDF chunking with `PyPDF2` / `pdfplumber` | `DocumentSplitter` via `create_view()` | Sentence-aware splitting with token limits and overlap. |
| JSON files or Postgres tables for metadata | Table columns alongside media columns | Structured and unstructured data in the same row. |
| No versioning (or manual timestamped copies) | Automatic versioning with `history()`, `revert()`, snapshots | Every insert, update, and schema change is tracked. |
| Re-run everything when the model or prompt changes | Drop and re-add the computed column | Only the changed column recomputes; everything else is untouched. |
| Jupyter notebooks with cells you run in order | Computed column DAG | Define the pipeline once; it runs on every new row automatically. |

---

## Side by Side: Build a Multimodal Pipeline

### The DIY Approach

A typical hand-built pipeline for processing documents with an LLM, generating embeddings, and enabling search:

```python
# The usual pile of dependencies
import boto3
import json
import os
import time
from openai import OpenAI
from PyPDF2 import PdfReader
import numpy as np

s3 = boto3.client('s3')
client = OpenAI()

BUCKET = 'my-data-bucket'
EMBED_MODEL = 'text-embedding-3-small'

# 1. Download PDFs from S3
def download_pdfs(prefix='documents/'):
    objs = s3.list_objects_v2(Bucket=BUCKET, Prefix=prefix)
    paths = []
    for obj in objs.get('Contents', []):
        local = f"/tmp/{obj['Key'].split('/')[-1]}"
        s3.download_file(BUCKET, obj['Key'], local)
        paths.append(local)
    return paths

# 2. Extract and chunk text (manual splitting)
def chunk_pdf(path, max_chars=1000):
    reader = PdfReader(path)
    text = ' '.join(p.extract_text() or '' for p in reader.pages)
    chunks = []
    for i in range(0, len(text), max_chars):
        chunks.append({'source': path, 'text': text[i:i + max_chars]})
    return chunks

# 3. Generate embeddings with manual retry
def embed_with_retry(texts, max_retries=3):
    for attempt in range(max_retries):
        try:
            response = client.embeddings.create(input=texts, model=EMBED_MODEL)
            return [e.embedding for e in response.data]
        except Exception as e:
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)
            else:
                raise

# 4. Store embeddings (just... somewhere)
all_chunks = []
all_embeddings = []
for path in download_pdfs():
    chunks = chunk_pdf(path)
    texts = [c['text'] for c in chunks]
    embeddings = embed_with_retry(texts)
    all_chunks.extend(chunks)
    all_embeddings.extend(embeddings)

# Save to disk because there's no better option
np.save('/tmp/embeddings.npy', np.array(all_embeddings))
with open('/tmp/chunks.json', 'w') as f:
    json.dump(all_chunks, f)
# ... then upload back to S3
s3.upload_file('/tmp/embeddings.npy', BUCKET, 'embeddings/latest.npy')
s3.upload_file('/tmp/chunks.json', BUCKET, 'embeddings/chunks.json')

# 5. Search (load everything into memory, brute-force cosine sim)
def search(query, top_k=5):
    embs = np.load('/tmp/embeddings.npy')
    with open('/tmp/chunks.json') as f:
        chunks = json.load(f)
    q_emb = embed_with_retry([query])[0]
    scores = embs @ np.array(q_emb)
    top_idx = scores.argsort()[-top_k:][::-1]
    return [{'text': chunks[i]['text'], 'score': float(scores[i])} for i in top_idx]

# 6. Ask an LLM with context (manual prompt assembly)
def ask(question):
    results = search(question)
    context = '\n\n'.join(r['text'] for r in results)
    response = client.chat.completions.create(
        model='gpt-4o-mini',
        messages=[{'role': 'user',
                   'content': f'Answer based on context:\n{context}\n\nQuestion: {question}'}],
    )
    return response.choices[0].message.content

# ⚠️ New PDFs? Re-run the entire script.
# ⚠️ Changed the embedding model? Re-run everything.
# ⚠️ Embeddings and chunks can get out of sync.
# ⚠️ No versioning. No lineage. No incremental updates.
# ⚠️ Search is brute-force in memory.
```

**Things you're managing:** S3 uploads/downloads, PDF parsing, chunking logic, retry logic, embedding storage format, file synchronization, search implementation, prompt assembly, no caching, no versioning.

### The Same Thing in Pixeltable

```python
import pixeltable as pxt
from pixeltable.functions.openai import chat_completions, embeddings
from pixeltable.functions.document import document_splitter

# 1. Create table — documents can come from S3, URLs, or local paths
docs = pxt.create_table('pipeline.docs', {'pdf': pxt.Document})

# 2. Chunk documents (automatic for all current and future rows)
chunks = pxt.create_view('pipeline.chunks', docs,
    iterator=document_splitter(docs.pdf, separators='sentence,token_limit', limit=300))

# 3. Add embedding index (built-in vector search, auto-maintained)
chunks.add_embedding_index('text',
    string_embed=embeddings.using(model='text-embedding-3-small'))

# 4. Load documents — from S3, URLs, or local paths
docs.insert([
    {'pdf': 's3://my-data-bucket/documents/report1.pdf'},
    {'pdf': 's3://my-data-bucket/documents/report2.pdf'},
    {'pdf': '/local/path/to/doc.pdf'},
    {'pdf': 'https://example.com/spec.pdf'},
])

# 5. Search
sim = chunks.text.similarity(string='What were the key findings?')
results = chunks.order_by(sim, asc=False).limit(5).select(chunks.text, score=sim).collect()

# 6. Build a RAG pipeline with computed columns
@pxt.query
def retrieve_context(question: str, top_k: int = 5):
    sim = chunks.text.similarity(string=question)
    return chunks.order_by(sim, asc=False).limit(top_k).select(chunks.text)

qa = pxt.create_table('pipeline.qa', {'question': pxt.String})
qa.add_computed_column(context=retrieve_context(qa.question))

@pxt.udf
def build_prompt(question: str, context: list[dict]) -> str:
    ctx = '\n\n'.join(c['text'] for c in context)
    return f'Answer based on context:\n{ctx}\n\nQuestion: {question}'

qa.add_computed_column(prompt=build_prompt(qa.question, qa.context))
qa.add_computed_column(response=chat_completions(
    messages=[{'role': 'user', 'content': qa.prompt}], model='gpt-4o-mini'))
qa.add_computed_column(answer=qa.response.choices[0].message.content)

# Ask a question — retrieval, prompt assembly, and LLM call happen automatically
qa.insert([{'question': 'What were the key findings?'}])
qa.select(qa.question, qa.answer).collect()

# ✅ New PDFs? Just docs.insert([...]) — everything downstream runs automatically.
# ✅ Changed the embedding model? Drop and re-add the index — only embeddings recompute.
# ✅ Retries are automatic. Results are cached. Everything is versioned.
```

**Things you're managing:** Your application logic. That's it.

---

## What You Gain

- **No infrastructure to manage.** No S3 upload/download code, no Lambda functions, no SQS queues, no Step Functions. Pixeltable handles storage, scheduling, and orchestration.
- **No retry logic.** Transient failures (rate limits, timeouts) are retried automatically. Successful results are cached and never recomputed.
- **No embedding plumbing.** No manually calling the embedding API, no storing vectors in files or custom tables, no brute-force search. `add_embedding_index()` handles storage and HNSW-indexed search.
- **No re-running scripts.** Insert new data and every downstream computation (chunking, embedding, LLM calls) runs incrementally on just the new rows.
- **No sync bugs.** Chunks, embeddings, and LLM outputs are derived columns — they can't get out of sync with the source data because Pixeltable maintains the dependency graph.
- **Versioning for free.** Every change is tracked. Roll back with `revert()`, inspect with `history()`, or query any past version.
- **Still works with S3.** Pixeltable reads from and writes to S3, GCS, Azure, R2, and Tigris. You can use `destination='s3://...'` on computed columns to write results back to your buckets.

---

## Common Patterns

### Replacing a Lambda + S3 processing pipeline

<Tabs>
  <Tab title="DIY (Lambda + S3)">
    ```python
    # Lambda function triggered by S3 upload
    def handler(event, context):
        bucket = event['Records'][0]['s3']['bucket']['name']
        key = event['Records'][0]['s3']['object']['key']

        # Download
        s3.download_file(bucket, key, '/tmp/input.jpg')

        # Process (resize, run model, etc.)
        img = Image.open('/tmp/input.jpg')
        thumbnail = img.resize((256, 256))
        thumbnail.save('/tmp/thumb.jpg')

        # Upload result
        s3.upload_file('/tmp/thumb.jpg', bucket, f'thumbnails/{key}')

        # Run OpenAI vision (with manual retry)
        for attempt in range(3):
            try:
                caption = client.chat.completions.create(...)
                break
            except:
                time.sleep(2 ** attempt)

        # Store metadata somewhere...
        dynamodb.put_item(TableName='image_metadata', Item={...})
    ```
  </Tab>
  <Tab title="Pixeltable">
    ```python
    images = pxt.create_table('app.images', {'image': pxt.Image})

    # All of this runs automatically on every insert:
    images.add_computed_column(thumbnail=images.image.resize((256, 256)))
    images.add_computed_column(caption=openai.vision(
        prompt='Describe this image.', image=images.image, model='gpt-4o-mini'))

    # Insert triggers everything — from S3 or anywhere
    images.insert([{'image': 's3://my-bucket/photos/img001.jpg'}])
    ```
  </Tab>
</Tabs>

### Replacing manual video frame extraction

<Tabs>
  <Tab title="DIY (ffmpeg + custom scripts)">
    ```python
    import subprocess
    import glob

    def extract_frames(video_path, fps=1):
        out_dir = f'/tmp/frames/{os.path.basename(video_path)}/'
        os.makedirs(out_dir, exist_ok=True)
        subprocess.run([
            'ffmpeg', '-i', video_path,
            '-vf', f'fps={fps}',
            f'{out_dir}/frame_%04d.jpg'
        ], check=True)
        return sorted(glob.glob(f'{out_dir}/*.jpg'))

    # For each video: extract, embed, store, track which are done...
    for video in get_video_list():
        frames = extract_frames(video)
        for frame_path in frames:
            emb = embed_image(frame_path)
            store_somewhere(frame_path, emb)
    ```
  </Tab>
  <Tab title="Pixeltable">
    ```python
    from pixeltable.functions.video import frame_iterator

    videos = pxt.create_table('app.videos', {'video': pxt.Video})
    frames = pxt.create_view('app.frames', videos,
        iterator=frame_iterator(videos.video, fps=1))

    # Insert a video — frames are extracted automatically
    videos.insert([{'video': 's3://my-bucket/videos/demo.mp4'}])

    # All frames are queryable
    frames.select(frames.frame).head(10)
    ```
  </Tab>
</Tabs>

### Replacing manual embedding + search

<Tabs>
  <Tab title="DIY (numpy + files)">
    ```python
    import numpy as np

    # Generate embeddings, save to disk
    embeddings = []
    for chunk in chunks:
        emb = get_embedding(chunk['text'])
        embeddings.append(emb)
    np.save('embeddings.npy', np.array(embeddings))

    # Search: load into memory, brute-force cosine similarity
    def search(query):
        embs = np.load('embeddings.npy')
        q = np.array(get_embedding(query))
        scores = embs @ q / (np.linalg.norm(embs, axis=1) * np.linalg.norm(q))
        top = scores.argsort()[-5:][::-1]
        return [chunks[i] for i in top]
    ```
  </Tab>
  <Tab title="Pixeltable">
    ```python
    chunks.add_embedding_index('text',
        string_embed=embeddings.using(model='text-embedding-3-small'))

    sim = chunks.text.similarity(string='my search query')
    results = chunks.order_by(sim, asc=False).limit(5).collect()
    ```
  </Tab>
</Tabs>

---

## You Can Still Use S3

Pixeltable isn't asking you to abandon your cloud storage. You can:

- **Read from S3/GCS/Azure:** `docs.insert([{'pdf': 's3://bucket/doc.pdf'}])`
- **Write computed results to S3:** `t.add_computed_column(thumb=t.image.resize((256, 256)), destination='s3://bucket/thumbs/')`
- **Configure default storage:** Set `PIXELTABLE_OUTPUT_MEDIA_DEST` to route all generated media to your bucket

See the [Cloud Storage guide](/integrations/cloud-storage) for setup.

---

## Next Steps

<CardGroup cols={2}>
  <Card title="Quick Start" icon="bolt" href="/overview/quick-start">
    Install and run your first pipeline in 5 minutes
  </Card>
  <Card title="10-Minute Tour" icon="play" href="/overview/ten-minute-tour">
    Hands-on walkthrough of tables, computed columns, and views
  </Card>
  <Card title="Cloud Storage" icon="cloud" href="/integrations/cloud-storage">
    S3, GCS, Azure, R2, Tigris configuration
  </Card>
  <Card title="Data Import" icon="download" href="/howto/cookbooks/data/data-import-s3">
    Load data from S3 buckets
  </Card>
</CardGroup>
