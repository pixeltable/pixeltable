---
title: 'Coming from LanceDB'
description: 'How multimodal data wrangling concepts map from LanceDB and custom scripts to Pixeltable'
---

If you've been managing multimodal datasets with LanceDB, HuggingFace Datasets, or custom Python scripts for processing images, video, and audio, this guide shows how those concepts translate to Pixeltable.

---

## Concept Mapping

| LanceDB + Custom Scripts | Pixeltable | Notes |
|---|---|---|
| `lancedb.connect()` + `db.create_table()` | `pxt.create_table()` with typed columns | Schema includes native `Image`, `Video`, `Audio`, `Document` types |
| Binary/blob columns for media | `pxt.Image`, `pxt.Video`, `pxt.Audio`, `pxt.Document` | First-class types with built-in operations (resize, format conversion, etc.) |
| Custom ffmpeg scripts for frame extraction | `FrameIterator` via `create_view()` | Declarative: specify FPS, and frames are extracted automatically |
| Manual embedding generation scripts | Computed columns or `add_embedding_index()` | Embeddings are generated on insert and cached |
| `EmbeddingFunctionRegistry` + OpenCLIP | `add_embedding_index()` with any embedding function | Supports OpenAI, sentence-transformers, CLIP, and custom models |
| `table.search(query).limit(k)` | `.similarity()` + `.order_by()` + `.limit()` | Same idea, Python-native syntax |
| Re-run pipeline scripts when data changes | Automatic — computed columns update incrementally | Only new or changed rows are reprocessed |
| HuggingFace Hub for sharing datasets | `pxt.publish()` / `pxt.replicate()` | Built-in cloud sharing with push/pull sync |
| FiftyOne for dataset curation | Embedding search + `.where()` filters | Or export to FiftyOne: [integration guide](/howto/working-with-fiftyone) |
| Custom export scripts | `export_parquet()`, `to_pytorch_dataset()`, `to_coco_dataset()` | Built-in export to training formats |

---

## Side by Side: Build a Multimodal Pipeline

### The LanceDB + Custom Scripts Approach

A typical video-to-searchable-frames pipeline requires multiple tools and manual orchestration:

```python
# Requirements: lancedb, opencv-python, pillow, open_clip_torch, torch, numpy

import cv2
import lancedb
import numpy as np
import open_clip
import torch
from PIL import Image

# 1. Extract frames with OpenCV
def extract_frames(video_path, fps=1):
    cap = cv2.VideoCapture(video_path)
    video_fps = cap.get(cv2.CAP_PROP_FPS)
    interval = int(video_fps / fps)
    frames, frame_idx = [], 0
    while cap.isOpened():
        ret, frame = cap.read()
        if not ret:
            break
        if frame_idx % interval == 0:
            rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frames.append(Image.fromarray(rgb))
        frame_idx += 1
    cap.release()
    return frames

frames = extract_frames('product_demo.mp4', fps=1)

# 2. Generate CLIP embeddings
model, _, preprocess = open_clip.create_model_and_transforms(
    'ViT-B-32', pretrained='openai'
)
tokenizer = open_clip.get_tokenizer('ViT-B-32')

def embed_image(img):
    tensor = preprocess(img).unsqueeze(0)
    with torch.no_grad():
        return model.encode_image(tensor).squeeze().numpy()

embeddings = [embed_image(f) for f in frames]

# 3. Store in LanceDB
db = lancedb.connect('my_db')
data = [{'frame_idx': i, 'vector': emb, 'image': np.array(f).tobytes()}
        for i, (f, emb) in enumerate(zip(frames, embeddings))]
table = db.create_table('video_frames', data)

# 4. Search
query_emb = embed_image(some_query_image)
results = table.search(query_emb).limit(5).to_list()

# ⚠️ Adding new videos means re-running the entire script
# ⚠️ Images stored as raw bytes — no type safety or built-in operations
# ⚠️ Must manage frame extraction, embedding, and storage separately
```

**Packages involved:** `lancedb`, `opencv-python`, `pillow`, `open_clip_torch`, `torch`, `numpy`

### The Same Thing in Pixeltable

```python
# Requirements: pixeltable

import pixeltable as pxt
from pixeltable.functions.video import frame_iterator

# 1. Create video table with native Video type
videos = pxt.create_table('media.videos', {'video': pxt.Video})

# 2. Extract frames (view auto-processes new videos)
frames = pxt.create_view('media.frames', videos,
    iterator=frame_iterator(videos.video, fps=1))

# 3. Add embedding index for image search
from pixeltable.functions.huggingface import clip

frames.add_embedding_index('frame',
    embedding=clip.using(model_id='openai/clip-vit-base-patch32'))

# 4. Insert a video — frames and embeddings are generated automatically
videos.insert([{'video': 'product_demo.mp4'}])

# 5. Search by text or image
sim = frames.frame.similarity(string='person holding product')
results = frames.order_by(sim, asc=False).limit(5).select(frames.frame).collect()

# ✅ Adding new videos: just videos.insert([...]) — frames + embeddings auto-update
# ✅ Images are first-class types — resize, display, export natively
# ✅ No manual frame extraction or embedding scripts
```

**Packages involved:** `pixeltable`

---

## What You Gain

- **Native media types.** `Image`, `Video`, `Audio`, and `Document` are column types, not raw bytes. You get built-in operations like resize, format conversion, and display.
- **Declarative frame extraction.** `FrameIterator` replaces custom ffmpeg/OpenCV scripts. Specify the FPS and Pixeltable handles the rest, including for new videos.
- **Incremental processing.** Insert a new video and frames, embeddings, and any downstream computed columns run automatically — only on the new data.
- **No pipeline scripts to maintain.** The pipeline *is* the table definition. There's no separate script to re-run when data changes.
- **Built-in sharing.** `pxt.publish()` and `pxt.replicate()` replace manual dataset upload/download workflows.
- **Export to training formats.** `to_pytorch_dataset()`, `export_parquet()`, and `to_coco_dataset()` are built-in.
- **Test before you commit.** Preview any transformation on a sample before materializing it: `frames.select(frames.frame, thumb=frames.frame.resize((256, 256))).head(5)`. Nothing is stored until you call `add_computed_column()`.
- **Annotation and curation integrations.** Export to [Label Studio](/howto/using-label-studio-with-pixeltable) for human annotation or [FiftyOne](/howto/working-with-fiftyone) for visual curation — then sync results back.
- **Swap AI providers freely.** Use OpenAI, Anthropic, Gemini, HuggingFace, or [20+ other providers](/integrations/frameworks) for vision analysis, captioning, or classification with the same computed column pattern.

---

## Common Patterns

### Processing images with computed columns

<Tabs>
  <Tab title="LanceDB + Custom Scripts">
    ```python
    # Manual processing + re-storage
    from PIL import Image

    def make_thumbnail(img_bytes):
        img = Image.frombytes('RGB', (w, h), img_bytes)
        img.thumbnail((256, 256))
        return np.array(img).tobytes()

    thumbnails = [make_thumbnail(row['image']) for row in table.to_list()]
    # Must update table manually...
    ```
  </Tab>
  <Tab title="Pixeltable">
    ```python
    # Declarative — runs automatically on all rows (existing and future)
    frames.add_computed_column(
        thumbnail=frames.frame.resize((256, 256))
    )
    ```
  </Tab>
</Tabs>

### Adding AI analysis to frames

<Tabs>
  <Tab title="LanceDB + Custom Scripts">
    ```python
    # Separate script for each model
    from openai import OpenAI
    client = OpenAI()

    captions = []
    for row in table.to_list():
        response = client.chat.completions.create(
            model='gpt-4o-mini',
            messages=[{'role': 'user', 'content': [
                {'type': 'text', 'text': 'Describe this image.'},
                {'type': 'image_url', 'image_url': {'url': row['image_url']}}
            ]}]
        )
        captions.append(response.choices[0].message.content)
    # Must store captions back into the table...
    ```
  </Tab>
  <Tab title="Pixeltable">
    ```python
    # One line — runs on all existing and future frames
    from pixeltable.functions.openai import vision

    frames.add_computed_column(
        caption=vision(prompt='Describe this image.', image=frames.frame, model='gpt-4o-mini')
    )
    ```
  </Tab>
</Tabs>

### Exporting to PyTorch

<Tabs>
  <Tab title="LanceDB + Custom Scripts">
    ```python
    # Manual dataset class
    from torch.utils.data import Dataset, DataLoader

    class FrameDataset(Dataset):
        def __init__(self, table):
            self.data = table.to_list()
        def __len__(self):
            return len(self.data)
        def __getitem__(self, idx):
            row = self.data[idx]
            img = Image.frombytes('RGB', (w, h), row['image'])
            return transform(img), row['label']

    loader = DataLoader(FrameDataset(table), batch_size=32)
    ```
  </Tab>
  <Tab title="Pixeltable">
    ```python
    # Built-in
    ds = frames.select(frames.frame, frames.label).to_pytorch_dataset()
    loader = DataLoader(ds, batch_size=32)
    ```
  </Tab>
</Tabs>

---

## Next Steps

<CardGroup cols={2}>
  <Card title="Extract Video Frames" icon="film" href="/howto/cookbooks/video/video-extract-frames">
    Frame extraction with FPS control and keyframe mode
  </Card>
  <Card title="Similar Image Search" icon="images" href="/howto/cookbooks/search/search-similar-images">
    CLIP-based image similarity search
  </Card>
  <Card title="Import from HuggingFace" icon="face-smile" href="/howto/cookbooks/data/data-import-huggingface">
    Load datasets from HuggingFace Hub
  </Card>
  <Card title="Export to PyTorch" icon="fire" href="/howto/cookbooks/data/data-export-pytorch">
    Convert tables to PyTorch DataLoaders
  </Card>
</CardGroup>
