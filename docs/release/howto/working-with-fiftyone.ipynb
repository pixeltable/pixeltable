{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a79ab1e4-8163-47b1-a2e4-58091032a0fd",
   "metadata": {},
   "source": [
    "# Working with Voxel51 for Visualization in Pixeltable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e39678f-a206-413e-bada-4e090052b900",
   "metadata": {},
   "source": [
    "Pixeltable can export data directly from tables and views to the popular [Voxel51](https://voxel51.com/) frontend, providing a way to visualize and explore image and video datasets. In this tutorial, we'll learn how to:\n",
    "\n",
    "- Export data from Pixeltable to Voxel51\n",
    "- Apply labels from image classification and object detection models to exported data\n",
    "\n",
    "We begin by installing the necessary libraries for this tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01125f39-e870-4b2a-a5cc-0130683095d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU pixeltable fiftyone torch transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13eee6f-fc48-4a14-9633-faca0afb79c6",
   "metadata": {},
   "source": [
    "## Example 1: An Image Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dae15b-8f07-441f-87bb-fd1cc9cc903e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import fiftyone as fo\n",
    "import pixeltable as pxt\n",
    "\n",
    "# Create a Pixeltable directory for the demo. We first drop the directory if it\n",
    "# exists, in order to ensure a clean environment.\n",
    "\n",
    "pxt.drop_dir('fo_demo', force=True)\n",
    "pxt.create_dir('fo_demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15b10733-98e1-4288-b06a-b21e4da99ac9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create a Pixeltable table for our dataset and insert some sample images.\n",
    "\n",
    "url_prefix = 'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images'\n",
    "\n",
    "urls = [\n",
    "    'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images/000000000019.jpg',\n",
    "    'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images/000000000025.jpg',\n",
    "    'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images/000000000030.jpg',\n",
    "    'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images/000000000034.jpg',\n",
    "]\n",
    "\n",
    "t = pxt.create_table('fo_demo.images', {'image': pxt.Image})\n",
    "t.insert({'image': url} for url in urls)\n",
    "t.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadb8a3b-f6ae-4dfb-80a7-40d0e40a7a3d",
   "metadata": {},
   "source": [
    "Now we export our new table to a Voxel51 dataset and load it into a new Voxel51 session within our demo notebook. Once it's been loaded, the images can be interactively navigated as with any other Voxel51 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f50d18f5-2248-4f9d-b83b-1a580479bcdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fo_dataset = pxt.io.export_images_as_fo_dataset(t, t.image)\n",
    "session = fo.launch_app(fo_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73987ac2-38b3-44dc-93a8-97ee0c93ee21",
   "metadata": {},
   "source": [
    "## Adding Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bafe001-be1f-4258-898a-774feb9d5071",
   "metadata": {},
   "source": [
    "We'll now show how Voxel51 labels can be attached to the exported dataset. Currently, Pixeltable supports only classification and detection labels; other Voxel51 label types may be added in the future.\n",
    "\n",
    "First, let's generate some labels by applying two models from the Huggingface `transformers` library: A ViT model for image classification and a DETR model for object detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0107d2ac-a542-453a-9cdc-5bb1e01652b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixeltable.functions.huggingface import vit_for_image_classification, detr_for_object_detection\n",
    "\n",
    "t.add_computed_column(classifications=vit_for_image_classification(\n",
    "    t.image, model_id='google/vit-base-patch16-224'\n",
    "))\n",
    "t.add_computed_column(detections=detr_for_object_detection(\n",
    "    t.image, model_id='facebook/detr-resnet-50'\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78299f8-a41f-4a4c-837e-6c2524af678b",
   "metadata": {},
   "source": [
    "Both models output JSON containing the model results. Let's peek at the contents of our table now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9784ae-d0d5-4814-ba5a-0bd60aa5162c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88e8ba3-1c66-48c0-be37-1d4fc1b99a4a",
   "metadata": {},
   "source": [
    "Now we need to transform our model data into the format the Voxel51 API expects (see the Pixeltable documentation for [pxt.io.export_images_as_fo_dataset](https://docs.pixeltable.com/sdk/latest/io#func-export-images-as-fo-dataset) for details). We'll use Pixeltable UDFs to do the appropriate conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf23c2d-b63d-42a8-9170-5384d362b012",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pxt.udf\n",
    "def vit_to_fo(vit_labels: list) -> list:\n",
    "    return [\n",
    "        {'label': label, 'confidence': score}\n",
    "        for label, score in zip(vit_labels['label_text'], vit_labels['scores'])\n",
    "    ]\n",
    "\n",
    "@pxt.udf\n",
    "def detr_to_fo(img: pxt.Image, detr_labels: dict) -> list:\n",
    "    result = []\n",
    "    for label, box, score in zip(detr_labels['label_text'], detr_labels['boxes'], detr_labels['scores']):\n",
    "        # DETR gives us bounding boxes in (x1,y1,x2,y2) absolute (pixel) coordinates.\n",
    "        # Voxel51 expects (x,y,w,h) relative (fractional) coordinates.\n",
    "        # So we need to do a conversion.\n",
    "        fo_box = [\n",
    "            box[0] / img.width,\n",
    "            box[1] / img.height,\n",
    "            (box[2] - box[0]) / img.width,\n",
    "            (box[3] - box[1]) / img.height,\n",
    "        ]\n",
    "        result.append({'label': label, 'bounding_box': fo_box, 'confidence': score})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f486933e-2101-431f-b9f2-9bd7303fc04b",
   "metadata": {},
   "source": [
    "We can test that our UDFs are working as expected with a `select()` statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dade4ca5-1fb5-4182-a0d6-b3b73a9b0f72",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "t.select(\n",
    "    t.image,\n",
    "    t.classifications,\n",
    "    vit_to_fo(t.classifications),\n",
    "    t.detections,\n",
    "    detr_to_fo(t.image, t.detections)\n",
    ").head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f09b35d-c102-462a-ac63-7da1d65053fd",
   "metadata": {},
   "source": [
    "Now we pass the modified structures to `export_images_as_fo_dataset`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e284c51f-a261-4979-a10a-5d7457e68fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "fo_dataset = pxt.io.export_images_as_fo_dataset(\n",
    "    t,\n",
    "    t.image,\n",
    "    classifications=vit_to_fo(t.classifications),\n",
    "    detections=detr_to_fo(t.image, t.detections)\n",
    ")\n",
    "session = fo.launch_app(fo_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95f050c-5fc5-4129-aa7f-47d14df79a62",
   "metadata": {},
   "source": [
    "## Adding Multiple Label Sets\n",
    "\n",
    "You can include multiple label sets of the same type in the same dataset by passing a `list` or `dict` of expressions to the `classifications` and/or `detections` parameters. If a `list` is specified, default names will be assigned to the label sets; if a `dict` is specified, the label sets will be named according to its keys.\n",
    "\n",
    "As an example, let's try recomputing our detections using the more powerful DETR model ResNet-101, and then load them into the same Voxel51 dataset as the earlier detections in order to compare them side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df5bd83-0393-4665-baa8-a97eea27c5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "t.add_computed_column(detections_101=detr_for_object_detection(\n",
    "    t.image, model_id='facebook/detr-resnet-101'\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f645048-ce82-4164-90ed-d96da8929dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "fo_dataset = pxt.io.export_images_as_fo_dataset(\n",
    "    t,\n",
    "    t.image,\n",
    "    classifications=vit_to_fo(t.classifications),\n",
    "    detections={\n",
    "        'detections_50': detr_to_fo(t.image, t.detections),\n",
    "        'detections_101': detr_to_fo(t.image, t.detections_101)\n",
    "    }\n",
    ")\n",
    "session = fo.launch_app(fo_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd6945c3-f4e5-48aa-8aae-21ed398dabae",
   "metadata": {},
   "source": [
    "Exploring the resulting images, we can see that the results are not much different between the two models, at least on our small sample dataset."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
