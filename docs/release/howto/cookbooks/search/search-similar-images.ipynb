{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Find similar images with CLIP\n",
    "\n",
    "Build visual similarity search to find images that look alike using OpenAI's CLIP model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem\n",
    "\n",
    "You have a collection of images and need to find visually similar onesâ€”for duplicate detection, content recommendations, or visual search.\n",
    "\n",
    "| Query | Expected matches |\n",
    "|-------|------------------|\n",
    "| sunset photo | Other sunset/beach images |\n",
    "| product image | Similar products |\n",
    "| user upload | Matching content in library |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solution\n",
    "\n",
    "**What's in this recipe:**\n",
    "- Create image embeddings with CLIP\n",
    "- Search by image similarity\n",
    "- Search by text description (cross-modal)\n",
    "\n",
    "You add an embedding index using CLIP, which understands both images and text. This enables finding similar images or searching images by text description."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU pixeltable sentence-transformers torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pixeltable as pxt\n",
    "from pixeltable.functions.huggingface import clip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a fresh directory\n",
    "pxt.drop_dir('image_search_demo', force=True)\n",
    "pxt.create_dir('image_search_demo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = pxt.create_table('image_search_demo.images', {'image': pxt.Image})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert sample images\n",
    "images.insert([\n",
    "    {'image': 'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images/000000000036.jpg'},\n",
    "    {'image': 'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images/000000000090.jpg'},\n",
    "    {'image': 'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images/000000000106.jpg'},\n",
    "    {'image': 'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images/000000000139.jpg'},\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create CLIP embedding index\n",
    "\n",
    "Add an embedding index using CLIP for cross-modal search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add CLIP embedding index (supports both image and text queries)\n",
    "images.add_embedding_index(\n",
    "    'image',\n",
    "    embedding=clip.using(model_id='openai/clip-vit-base-patch32')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Search by text description\n",
    "\n",
    "Find images matching a text query:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search by text description\n",
    "query = \"people eating food\"\n",
    "sim = images.image.similarity(query)\n",
    "\n",
    "results = (\n",
    "    images\n",
    "    .order_by(sim, asc=False)\n",
    "    .select(images.image, score=sim)\n",
    "    .limit(2)\n",
    ")\n",
    "results.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explanation\n",
    "\n",
    "**Why CLIP:**\n",
    "\n",
    "CLIP (Contrastive Language-Image Pre-training) understands both images and text in the same embedding space. This enables:\n",
    "- Image-to-image search (find similar photos)\n",
    "- Text-to-image search (find photos matching a description)\n",
    "\n",
    "**Index parameters:**\n",
    "\n",
    "| Parameter | Description |\n",
    "|-----------|-------------|\n",
    "| `image_embed` | Model for embedding images |\n",
    "| `string_embed` | Model for embedding text queries |\n",
    "\n",
    "**Both must use the same model** for cross-modal search to work.\n",
    "\n",
    "**New images are indexed automatically:**\n",
    "\n",
    "When you insert new images, embeddings are generated without extra code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## See also\n",
    "\n",
    "- [Semantic text search](https://docs.pixeltable.com/howto/cookbooks/search/search-semantic-text)\n",
    "- [Vector database documentation](https://docs.pixeltable.com/platform/embedding-indexes)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
