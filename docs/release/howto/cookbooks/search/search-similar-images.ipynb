{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Find similar images with CLIP\n",
        "\n",
        "Build visual similarity search to find images that look alike using OpenAI's CLIP model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem\n",
        "\n",
        "You have a collection of images and need to find visually similar onesâ€”for duplicate detection, content recommendations, or visual search.\n",
        "\n",
        "| Query | Expected matches |\n",
        "|-------|------------------|\n",
        "| sunset photo | Other sunset/beach images |\n",
        "| product image | Similar products |\n",
        "| user upload | Matching content in library |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solution\n",
        "\n",
        "**What's in this recipe:**\n",
        "- Create image embeddings with CLIP\n",
        "- Search by image similarity\n",
        "- Search by text description (cross-modal)\n",
        "\n",
        "You add an embedding index using CLIP, which understands both images and text. This enables finding similar images or searching images by text description.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -qU pixeltable sentence-transformers torch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pixeltable as pxt\n",
        "from pixeltable.functions.huggingface import clip\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created directory 'image_search_demo'.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<pixeltable.catalog.dir.Dir at 0x3b811eb10>"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a fresh directory\n",
        "pxt.drop_dir('image_search_demo', force=True)\n",
        "pxt.create_dir('image_search_demo')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created table 'images'.\n"
          ]
        }
      ],
      "source": [
        "images = pxt.create_table('image_search_demo.images', {'image': pxt.Image})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inserting rows into `images`: 4 rows [00:00, 1687.00 rows/s]\n",
            "Inserted 4 rows with 0 errors.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "4 rows inserted, 8 values computed."
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Insert sample images\n",
        "images.insert([\n",
        "    {'image': 'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images/000000000036.jpg'},\n",
        "    {'image': 'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images/000000000090.jpg'},\n",
        "    {'image': 'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images/000000000106.jpg'},\n",
        "    {'image': 'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images/000000000139.jpg'},\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create CLIP embedding index\n",
        "\n",
        "Add an embedding index using CLIP for cross-modal search:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[39], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Add CLIP embedding index (supports both image and text queries)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m images\u001b[38;5;241m.\u001b[39madd_embedding_index(\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m----> 4\u001b[0m     embedding\u001b[38;5;241m=\u001b[39m\u001b[43mclip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43musing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mopenai/clip-vit-base-patch32\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m )\n",
            "File \u001b[0;32m/Users/pierre/pixeltable/.venv/lib/python3.12/site-packages/pixeltable/func/function.py:341\u001b[0m, in \u001b[0;36mFunction.using\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignatures)):\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 341\u001b[0m         template \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_resolved_fns\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bind_and_create_template\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    342\u001b[0m         templates\u001b[38;5;241m.\u001b[39mappend(template)\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mTypeError\u001b[39;00m, excs\u001b[38;5;241m.\u001b[39mError):\n",
            "File \u001b[0;32m/Users/pierre/pixeltable/.venv/lib/python3.12/site-packages/pixeltable/func/function.py:393\u001b[0m, in \u001b[0;36mFunction._bind_and_create_template\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    390\u001b[0m             template_kwargs[name] \u001b[38;5;241m=\u001b[39m var\n\u001b[1;32m    391\u001b[0m             args_ok \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 393\u001b[0m return_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_return_type\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbindings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    394\u001b[0m call \u001b[38;5;241m=\u001b[39m exprs\u001b[38;5;241m.\u001b[39mFunctionCall(\u001b[38;5;28mself\u001b[39m, template_args, template_kwargs, return_type)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;66;03m# Construct the (n-k)-ary signature of the new function. We use `call.col_type` for this, rather than\u001b[39;00m\n\u001b[1;32m    397\u001b[0m \u001b[38;5;66;03m# `self.signature.return_type`, because the return type of the new function may be specialized via a\u001b[39;00m\n\u001b[1;32m    398\u001b[0m \u001b[38;5;66;03m# conditional return type.\u001b[39;00m\n",
            "File \u001b[0;32m/Users/pierre/pixeltable/.venv/lib/python3.12/site-packages/pixeltable/func/function.py:237\u001b[0m, in \u001b[0;36mFunction.call_return_type\u001b[0;34m(self, bound_args)\u001b[0m\n\u001b[1;32m    233\u001b[0m         return_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignature\u001b[38;5;241m.\u001b[39mreturn_type\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    235\u001b[0m         \u001b[38;5;66;03m# A conditional return type is specified and all its arguments are constants; use the specific\u001b[39;00m\n\u001b[1;32m    236\u001b[0m         \u001b[38;5;66;03m# call return type\u001b[39;00m\n\u001b[0;32m--> 237\u001b[0m         return_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conditional_return_type\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcrt_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m return_type\u001b[38;5;241m.\u001b[39mnullable:\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m return_type\n",
            "File \u001b[0;32m/Users/pierre/pixeltable/.venv/lib/python3.12/site-packages/pixeltable/functions/huggingface.py:208\u001b[0m, in \u001b[0;36m_\u001b[0;34m(model_id)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;129m@clip\u001b[39m\u001b[38;5;241m.\u001b[39mconditional_return_type\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_\u001b[39m(model_id: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ts\u001b[38;5;241m.\u001b[39mArrayType:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m CLIPModel\n\u001b[0;32m--> 208\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43m_lookup_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mCLIPModel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ts\u001b[38;5;241m.\u001b[39mArrayType((model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mprojection_dim,), dtype\u001b[38;5;241m=\u001b[39mts\u001b[38;5;241m.\u001b[39mFloatType(), nullable\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
            "File \u001b[0;32m/Users/pierre/pixeltable/.venv/lib/python3.12/site-packages/pixeltable/functions/huggingface.py:1497\u001b[0m, in \u001b[0;36m_lookup_model\u001b[0;34m(model_id, create, device, pass_device_to_create)\u001b[0m\n\u001b[1;32m   1495\u001b[0m     model \u001b[38;5;241m=\u001b[39m create(model_id, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1497\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, nn\u001b[38;5;241m.\u001b[39mModule):\n\u001b[1;32m   1499\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m pass_device_to_create \u001b[38;5;129;01mand\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m/Users/pierre/pixeltable/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:277\u001b[0m, in \u001b[0;36mrestore_default_dtype.<locals>._wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    275\u001b[0m old_dtype \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mget_default_dtype()\n\u001b[1;32m    276\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    278\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    279\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_default_dtype(old_dtype)\n",
            "File \u001b[0;32m/Users/pierre/pixeltable/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:5051\u001b[0m, in \u001b[0;36mPreTrainedModel.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   5041\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m dtype_orig \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5042\u001b[0m         torch\u001b[38;5;241m.\u001b[39mset_default_dtype(dtype_orig)\n\u001b[1;32m   5044\u001b[0m     (\n\u001b[1;32m   5045\u001b[0m         model,\n\u001b[1;32m   5046\u001b[0m         missing_keys,\n\u001b[1;32m   5047\u001b[0m         unexpected_keys,\n\u001b[1;32m   5048\u001b[0m         mismatched_keys,\n\u001b[1;32m   5049\u001b[0m         offload_index,\n\u001b[1;32m   5050\u001b[0m         error_msgs,\n\u001b[0;32m-> 5051\u001b[0m     ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_pretrained_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5052\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5053\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5054\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5055\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5056\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_mismatched_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5057\u001b[0m \u001b[43m        \u001b[49m\u001b[43msharded_metadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharded_metadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5058\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_map\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5059\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdisk_offload_folder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moffload_folder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5060\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5061\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhf_quantizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhf_quantizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5062\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_in_fp32_regex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5063\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice_mesh\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice_mesh\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5064\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5065\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5066\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   5067\u001b[0m \u001b[38;5;66;03m# make sure token embedding weights are still tied if needed\u001b[39;00m\n\u001b[1;32m   5068\u001b[0m model\u001b[38;5;241m.\u001b[39mtie_weights()\n",
            "File \u001b[0;32m/Users/pierre/pixeltable/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:5319\u001b[0m, in \u001b[0;36mPreTrainedModel._load_pretrained_model\u001b[0;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[0m\n\u001b[1;32m   5316\u001b[0m     original_checkpoint_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(state_dict\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m   5317\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   5318\u001b[0m     original_checkpoint_keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m-> 5319\u001b[0m         \u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_files\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmeta\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweights_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweights_only\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m   5320\u001b[0m     )\n\u001b[1;32m   5322\u001b[0m \u001b[38;5;66;03m# Check if we are in a special state, i.e. loading from a state dict coming from a different architecture\u001b[39;00m\n\u001b[1;32m   5323\u001b[0m prefix \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mbase_model_prefix\n",
            "File \u001b[0;32m/Users/pierre/pixeltable/.venv/lib/python3.12/site-packages/transformers/modeling_utils.py:508\u001b[0m, in \u001b[0;36mload_state_dict\u001b[0;34m(checkpoint_file, is_quantized, map_location, weights_only)\u001b[0m\n\u001b[1;32m    506\u001b[0m \u001b[38;5;66;03m# Fallback to torch.load (if weights_only was explicitly False, do not check safety as this is known to be unsafe)\u001b[39;00m\n\u001b[1;32m    507\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m weights_only:\n\u001b[0;32m--> 508\u001b[0m     \u001b[43mcheck_torch_load_is_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m map_location \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
            "File \u001b[0;32m/Users/pierre/pixeltable/.venv/lib/python3.12/site-packages/transformers/utils/import_utils.py:1647\u001b[0m, in \u001b[0;36mcheck_torch_load_is_safe\u001b[0;34m()\u001b[0m\n\u001b[1;32m   1645\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_torch_load_is_safe\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1646\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_greater_or_equal(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m2.6\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m-> 1647\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1648\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDue to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1649\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1650\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhen loading files with safetensors.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1651\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1652\u001b[0m         )\n",
            "\u001b[0;31mValueError\u001b[0m: Due to a serious vulnerability issue in `torch.load`, even with `weights_only=True`, we now require users to upgrade torch to at least v2.6 in order to use the function. This version restriction does not apply when loading files with safetensors.\nSee the vulnerability report here https://nvd.nist.gov/vuln/detail/CVE-2025-32434"
          ]
        }
      ],
      "source": [
        "# Add CLIP embedding index (supports both image and text queries)\n",
        "images.add_embedding_index(\n",
        "    'image',\n",
        "    embedding=clip.using(model_id='openai/clip-vit-base-patch32')\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Search by text description\n",
        "\n",
        "Find images matching a text query:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "Error",
          "evalue": "No embedding index found for column 'image'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[21], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Search by text description\u001b[39;00m\n\u001b[1;32m      2\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpeople eating food\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m sim \u001b[38;5;241m=\u001b[39m \u001b[43mimages\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m results \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m      6\u001b[0m     images\n\u001b[1;32m      7\u001b[0m     \u001b[38;5;241m.\u001b[39morder_by(sim, asc\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;241m.\u001b[39mselect(images\u001b[38;5;241m.\u001b[39mimage, score\u001b[38;5;241m=\u001b[39msim)\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;241m.\u001b[39mlimit(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     11\u001b[0m results\u001b[38;5;241m.\u001b[39mcollect()\n",
            "File \u001b[0;32m/Users/pierre/pixeltable/.venv/lib/python3.12/site-packages/pixeltable/exprs/column_ref.py:166\u001b[0m, in \u001b[0;36mColumnRef.similarity\u001b[0;34m(self, item, idx)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimilarity\u001b[39m(\u001b[38;5;28mself\u001b[39m, item: Any, \u001b[38;5;241m*\u001b[39m, idx: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Expr:\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msimilarity_expr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SimilarityExpr\n\u001b[0;32m--> 166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSimilarityExpr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m/Users/pierre/pixeltable/.venv/lib/python3.12/site-packages/pixeltable/exprs/similarity_expr.py:40\u001b[0m, in \u001b[0;36mSimilarityExpr.__init__\u001b[0;34m(self, col_ref, item, idx_name)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcomponents \u001b[38;5;241m=\u001b[39m [col_ref, item_expr]\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# determine index to use\u001b[39;00m\n\u001b[0;32m---> 40\u001b[0m idx_info \u001b[38;5;241m=\u001b[39m \u001b[43mcol_ref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtbl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_idx\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol_ref\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mEmbeddingIndex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midx_id \u001b[38;5;241m=\u001b[39m idx_info\u001b[38;5;241m.\u001b[39mid\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midx_name \u001b[38;5;241m=\u001b[39m idx_info\u001b[38;5;241m.\u001b[39mname\n",
            "File \u001b[0;32m/Users/pierre/pixeltable/.venv/lib/python3.12/site-packages/pixeltable/catalog/table_version.py:1696\u001b[0m, in \u001b[0;36mTableVersion.get_idx\u001b[0;34m(self, col, idx_name, idx_cls)\u001b[0m\n\u001b[1;32m   1694\u001b[0m candidates \u001b[38;5;241m=\u001b[39m [info \u001b[38;5;28;01mfor\u001b[39;00m info \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39midxs_by_col[col\u001b[38;5;241m.\u001b[39mqid] \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(info\u001b[38;5;241m.\u001b[39midx, idx_cls)]\n\u001b[1;32m   1695\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(candidates) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 1696\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m excs\u001b[38;5;241m.\u001b[39mError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx_cls\u001b[38;5;241m.\u001b[39mdisplay_name()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m index found for column \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1697\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(candidates) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m idx_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1698\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m excs\u001b[38;5;241m.\u001b[39mError(\n\u001b[1;32m   1699\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mColumn \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcol\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m has multiple \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx_cls\u001b[38;5;241m.\u001b[39mdisplay_name()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m indices; specify `idx_name` instead\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1700\u001b[0m     )\n",
            "\u001b[0;31mError\u001b[0m: No embedding index found for column 'image'"
          ]
        }
      ],
      "source": [
        "# Search by text description\n",
        "query = \"people eating food\"\n",
        "sim = images.image.similarity(query)\n",
        "\n",
        "results = (\n",
        "    images\n",
        "    .order_by(sim, asc=False)\n",
        "    .select(images.image, score=sim)\n",
        "    .limit(2)\n",
        ")\n",
        "results.collect()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explanation\n",
        "\n",
        "**Why CLIP:**\n",
        "\n",
        "CLIP (Contrastive Language-Image Pre-training) understands both images and text in the same embedding space. This enables:\n",
        "- Image-to-image search (find similar photos)\n",
        "- Text-to-image search (find photos matching a description)\n",
        "\n",
        "**Index parameters:**\n",
        "\n",
        "| Parameter | Description |\n",
        "|-----------|-------------|\n",
        "| `image_embed` | Model for embedding images |\n",
        "| `string_embed` | Model for embedding text queries |\n",
        "\n",
        "**Both must use the same model** for cross-modal search to work.\n",
        "\n",
        "**New images are indexed automatically:**\n",
        "\n",
        "When you insert new images, embeddings are generated without extra code.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## See also\n",
        "\n",
        "- [Semantic text search](./search-semantic-text.ipynb)\n",
        "- [Vector database documentation](https://docs.pixeltable.com/datastore/vector-database)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
