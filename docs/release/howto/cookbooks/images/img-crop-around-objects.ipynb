{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "title: \"Crop Images and Videos Around Detected Objects\"\n",
        "icon: \"notebook\"\n",
        "description: \"[Open in Kaggle](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/howto/cookbooks/images/img-crop-around-objects.ipynb) | [Open in Colab](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/howto/cookbooks/images/img-crop-around-objects.ipynb) | [View on GitHub](https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/howto/cookbooks/images/img-crop-around-objects.ipynb)\"\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Detect objects in images, expand bounding boxes with padding or a scale factor, crop to specific aspect ratios, and keep detection data aligned after transforms."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem\n",
        "\n",
        "Object detection gives you tight bounding boxes, but downstream tasks need more flexibility:\n",
        "\n",
        "| Task | Challenge |\n",
        "|------|-----------|\n",
        "| Product thumbnails | Bounding boxes clip the subject; need padding for context |\n",
        "| Social media repurposing | Source is 16:9, need 9:16 and 1:1 crops centred on the subject |\n",
        "| Resize + re-detect | After resizing or cropping, bounding box coordinates no longer align |\n",
        "| Video reframing | Need to spatially crop a video around a detected subject |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solution\n",
        "\n",
        "**What's in this recipe:**\n",
        "\n",
        "- **`expand_bbox`** — Add pixel padding or a scale factor around a bounding box\n",
        "- **`fit_bbox_to_aspect`** — Compute an aspect-ratio-matching crop region centred on a subject\n",
        "- **`rescale_bbox`** — Keep bounding boxes aligned after `resize()`\n",
        "- **`offset_bbox`** — Keep bounding boxes aligned after `crop()`\n",
        "- **`video.crop()`** — Spatially crop a video, optionally resizing to a target resolution\n",
        "\n",
        "All of these are computed columns — they run automatically when you insert new data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/opt/miniconda3/envs/pixeltable/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/opt/miniconda3/envs/pixeltable/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/opt/miniconda3/envs/pixeltable/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/opt/miniconda3/envs/pixeltable/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/opt/miniconda3/envs/pixeltable/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/opt/miniconda3/envs/pixeltable/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m25.3\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m26.0\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -qU pixeltable-yolox\n",
        "\n",
        "# Use local pixeltable from this branch (remove these two lines when the\n",
        "# UDFs ship in a released version of pixeltable).\n",
        "import sys, os\n",
        "sys.path.insert(0, os.path.abspath(os.path.join(os.getcwd(), '..', '..', '..', '..', '..')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "ename": "ImportError",
          "evalue": "cannot import name 'expand_bbox' from 'pixeltable.functions.image' (/opt/miniconda3/envs/pixeltable/lib/python3.11/site-packages/pixeltable/functions/image.py)",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpixeltable\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpxt\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpixeltable\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01myolox\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m yolox\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpixeltable\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mimage\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      4\u001b[0m     expand_bbox,\n\u001b[1;32m      5\u001b[0m     fit_bbox_to_aspect,\n\u001b[1;32m      6\u001b[0m     offset_bbox,\n\u001b[1;32m      7\u001b[0m     rescale_bbox,\n\u001b[1;32m      8\u001b[0m )\n",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'expand_bbox' from 'pixeltable.functions.image' (/opt/miniconda3/envs/pixeltable/lib/python3.11/site-packages/pixeltable/functions/image.py)"
          ]
        }
      ],
      "source": [
        "import pixeltable as pxt\n",
        "from pixeltable.functions.yolox import yolox\n",
        "from pixeltable.functions.image import (\n",
        "    expand_bbox,\n",
        "    fit_bbox_to_aspect,\n",
        "    offset_bbox,\n",
        "    rescale_bbox,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load images"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "pxt.drop_dir('crop_demo', force=True)\n",
        "pxt.create_dir('crop_demo')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "images = pxt.create_table('crop_demo/images', {'image': pxt.Image})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "image_urls = [\n",
        "    'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images/000000000036.jpg',\n",
        "    'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images/000000000090.jpg',\n",
        "    'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images/000000000106.jpg',\n",
        "]\n",
        "\n",
        "images.insert([{'image': url} for url in image_urls])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Detect objects\n",
        "\n",
        "Run YOLOX to get bounding boxes and class labels. Then extract the best (highest-confidence) detection per image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "images.add_computed_column(\n",
        "    detections=yolox(images.image, model_id='yolox_m', threshold=0.5)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@pxt.udf\n",
        "def best_bbox(detections: dict) -> tuple[int, int, int, int] | None:\n",
        "    \"\"\"Return the bounding box of the highest-confidence detection.\"\"\"\n",
        "    scores = detections.get('scores', [])\n",
        "    bboxes = detections.get('bboxes', [])\n",
        "    if not scores:\n",
        "        return None\n",
        "    idx = scores.index(max(scores))\n",
        "    b = bboxes[idx]\n",
        "    return (round(b[0]), round(b[1]), round(b[2]), round(b[3]))\n",
        "\n",
        "\n",
        "images.add_computed_column(bbox=best_bbox(images.detections))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "images.select(images.image, images.bbox).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Expand bounding boxes with padding\n",
        "\n",
        "Detection boxes are often tight to the subject. `expand_bbox` lets you add breathing room before cropping — either as a **scale factor** (e.g. 1.4 = 40% bigger) or **pixel padding** (e.g. 30px on every side)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Expand the bounding box by 40% around the centre\n",
        "images.add_computed_column(\n",
        "    bbox_expanded=expand_bbox(\n",
        "        images.bbox, images.image.width, images.image.height,\n",
        "        margin_factor=1.4,\n",
        "    )\n",
        ")\n",
        "\n",
        "# Crop: tight (original bbox) vs. expanded\n",
        "images.add_computed_column(crop_tight=images.image.crop(images.bbox))\n",
        "images.add_computed_column(crop_expanded=images.image.crop(images.bbox_expanded))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare tight crop vs expanded crop side by side\n",
        "images.select(images.crop_tight, images.crop_expanded).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "You can also use pixel padding instead of (or in addition to) a scale factor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add 30 pixels on every side\n",
        "images.add_computed_column(\n",
        "    bbox_padded=expand_bbox(\n",
        "        images.bbox, images.image.width, images.image.height,\n",
        "        padding=30,\n",
        "    )\n",
        ")\n",
        "images.add_computed_column(crop_padded=images.image.crop(images.bbox_padded))\n",
        "\n",
        "images.select(images.crop_tight, images.crop_padded).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Crop to a target aspect ratio\n",
        "\n",
        "`fit_bbox_to_aspect` computes a crop region that **contains the subject** and **matches a target aspect ratio**. The region is centred on the bounding box and clamped to image bounds.\n",
        "\n",
        "This is the key building block for social-media repurposing: generate 9:16, 1:1, and 4:5 crops from a single source image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compute crop regions for three different aspect ratios\n",
        "images.add_computed_column(\n",
        "    box_9x16=fit_bbox_to_aspect(\n",
        "        images.bbox, images.image.width, images.image.height,\n",
        "        aspect_ratio='9:16',\n",
        "    )\n",
        ")\n",
        "images.add_computed_column(\n",
        "    box_1x1=fit_bbox_to_aspect(\n",
        "        images.bbox, images.image.width, images.image.height,\n",
        "        aspect_ratio='1:1',\n",
        "    )\n",
        ")\n",
        "images.add_computed_column(\n",
        "    box_4x5=fit_bbox_to_aspect(\n",
        "        images.bbox, images.image.width, images.image.height,\n",
        "        aspect_ratio='4:5',\n",
        "    )\n",
        ")\n",
        "\n",
        "# Apply the crops\n",
        "images.add_computed_column(crop_9x16=images.image.crop(images.box_9x16))\n",
        "images.add_computed_column(crop_1x1=images.image.crop(images.box_1x1))\n",
        "images.add_computed_column(crop_4x5=images.image.crop(images.box_4x5))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "images.select(images.crop_9x16, images.crop_1x1, images.crop_4x5).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Keep bounding boxes aligned after transforms\n",
        "\n",
        "When you resize or crop an image, the original bounding box coordinates no longer match the new pixel grid. Pixeltable provides two utilities to fix this:\n",
        "\n",
        "- **`rescale_bbox`** — after `resize()`, scales coordinates proportionally\n",
        "- **`offset_bbox`** — after `crop()`, offsets coordinates into the cropped image's space"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### After resize"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Resize images to 320x240\n",
        "images.add_computed_column(resized=images.image.resize((320, 240)))\n",
        "\n",
        "# Rescale the bounding box to match the new dimensions\n",
        "images.add_computed_column(\n",
        "    bbox_resized=rescale_bbox(\n",
        "        images.bbox,\n",
        "        [images.image.width, images.image.height],\n",
        "        (320, 240),\n",
        "    )\n",
        ")\n",
        "\n",
        "# Crop the resized image using the rescaled bbox\n",
        "images.add_computed_column(\n",
        "    crop_from_resized=images.resized.crop(images.bbox_resized)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "images.select(images.resized, images.bbox_resized, images.crop_from_resized).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### After crop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# After cropping to 1:1, translate the original bbox into the cropped image's coordinate space\n",
        "images.add_computed_column(\n",
        "    bbox_in_crop=offset_bbox(images.bbox, images.box_1x1)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "images.select(images.crop_1x1, images.bbox_in_crop).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Crop videos around detected subjects\n",
        "\n",
        "The same workflow extends to videos. Detect on a representative frame, compute the crop region, then apply `video.crop()` which uses ffmpeg under the hood.\n",
        "\n",
        "> **Requires:** `ffmpeg` installed and in PATH"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "videos = pxt.create_table('crop_demo/videos', {'video': pxt.Video})"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract first frame for detection\n",
        "videos.add_computed_column(\n",
        "    first_frame=videos.video.extract_frame(timestamp=0.0)\n",
        ")\n",
        "\n",
        "# Run detection on the frame\n",
        "videos.add_computed_column(\n",
        "    detections=yolox(videos.first_frame, model_id='yolox_m', threshold=0.5)\n",
        ")\n",
        "\n",
        "# Get the best bounding box\n",
        "videos.add_computed_column(bbox=best_bbox(videos.detections))\n",
        "\n",
        "# Compute a 9:16 crop region centred on the subject\n",
        "videos.add_computed_column(\n",
        "    crop_box=fit_bbox_to_aspect(\n",
        "        videos.bbox, videos.first_frame.width, videos.first_frame.height,\n",
        "        aspect_ratio='9:16',\n",
        "    )\n",
        ")\n",
        "\n",
        "# Crop the video and resize to 1080x1920\n",
        "videos.add_computed_column(\n",
        "    cropped_video=videos.video.crop(videos.crop_box, target_size=(1080, 1920))\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Insert a sample video to see it in action.\n",
        "# Replace with your own file path or URL.\n",
        "# videos.insert(video='path/to/video.mp4')\n",
        "# videos.select(videos.first_frame, videos.bbox, videos.cropped_video).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explanation\n",
        "\n",
        "### Bounding box utilities\n",
        "\n",
        "| Function | Purpose | Key parameters |\n",
        "|----------|---------|----------------|\n",
        "| `expand_bbox` | Add breathing room around a detection | `margin_factor=1.3` (scale), `padding=20` (pixels) |\n",
        "| `fit_bbox_to_aspect` | Compute a crop region matching a target aspect ratio | `aspect_ratio='9:16'` |\n",
        "| `rescale_bbox` | Adjust bbox after `resize()` | `from_size`, `to_size` |\n",
        "| `offset_bbox` | Adjust bbox after `crop()` | `crop_box` — the region that was cropped |\n",
        "| `video.crop()` | Spatially crop a video via ffmpeg | `box`, optional `target_size` |\n",
        "\n",
        "### How `fit_bbox_to_aspect` works\n",
        "\n",
        "1. Centres on the bounding box\n",
        "2. Expands the smaller dimension to match the target aspect ratio\n",
        "3. Constrains to frame bounds (shifts if needed, never goes out of frame)\n",
        "4. Returns a crop box in `(left, upper, right, lower)` format — ready for `image.crop()` or `video.crop()`\n",
        "\n",
        "### All bounding boxes use PIL convention\n",
        "\n",
        "Every function uses `(left, upper, right, lower)` — the same format as `PIL.Image.crop()` and Pixeltable's `image.crop()`. This means you can chain detection → expand → fit → crop without coordinate conversion."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## See also\n",
        "\n",
        "- [Detect objects in images](https://docs.pixeltable.com/howto/cookbooks/images/img-detect-objects) — YOLOX object detection basics\n",
        "- [Extract frames from videos](https://docs.pixeltable.com/howto/cookbooks/video/video-extract-frames) — Frame extraction for video analysis\n",
        "- [PIL image transforms](https://docs.pixeltable.com/howto/cookbooks/images/img-pil-transforms) — Built-in image operations"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pixeltable",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
