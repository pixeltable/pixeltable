{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transform images with AI-powered editing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem\n",
        "\n",
        "You have a batch of images that need AI-powered transformations—like turning photos into paintings, adding stylistic effects, or modifying content based on text prompts.\n",
        "\n",
        "| Original | Prompt | Result |\n",
        "|----------|--------|--------|\n",
        "| photo.jpg | \"oil painting style\" | *Painterly version* |\n",
        "| landscape.jpg | \"add dramatic sunset colors\" | *Enhanced sky* |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solution\n",
        "\n",
        "**What's in this recipe:**\n",
        "\n",
        "- Transform images using text prompts with Hugging Face diffusion models (Stable Diffusion, FLUX, and more)\n",
        "- Control transformation strength and quality settings\n",
        "- Process batches of images automatically\n",
        "\n",
        "You apply AI-powered transformations to images using Pixeltable's `image_to_image` User-Defined Function (UDF). This function wraps Hugging Face's `AutoPipelineForImage2Image`, which automatically selects the right pipeline for models like Stable Diffusion, FLUX.1-dev, and others.\n",
        "\n",
        "You can iterate on transformations before adding them to your table. Use `.select()` with `.collect()` to preview results on sample images—nothing is stored in your table. If you want to collect only the first few rows, use `.head(n)` instead of `.collect()`. Once you're satisfied, use `.add_computed_column()` to apply the transformation to all images in your table.\n",
        "\n",
        "For more on this workflow, see [Get fast feedback on transformations](https://docs.pixeltable.com/howto/cookbooks/core/dev-iterative-workflow).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install local pixeltable in editable mode to use PR changes (FLUX support)\n",
        "# Remove this cell before merging - use standard install for production\n",
        "%pip install -qU -e /Users/pierre/pixeltable torch transformers diffusers accelerate\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ensure local pixeltable is used (for development/PR testing)\n",
        "import sys\n",
        "sys.path.insert(0, '/Users/pierre/pixeltable')\n",
        "\n",
        "import pixeltable as pxt\n",
        "from pixeltable.functions.huggingface import image_to_image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a fresh directory (drop existing if present)\n",
        "pxt.drop_dir('img2img_demo', force=True)\n",
        "pxt.create_dir('img2img_demo')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "t = pxt.create_table('img2img_demo.images', {\n",
        "    'image': pxt.Image,\n",
        "    'prompt': pxt.String\n",
        "})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "t.insert([\n",
        "    {\n",
        "        'image': 'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images/000000000285.jpg',\n",
        "        'prompt': 'oil painting style, vibrant colors, brushstrokes visible'\n",
        "    },\n",
        "    {\n",
        "        'image': 'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images/000000000776.jpg',\n",
        "        'prompt': 'watercolor painting, soft edges, artistic'\n",
        "    },\n",
        "])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View original images and prompts\n",
        "t.collect()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Iterate: test transformation on a single image\n",
        "\n",
        "Use `.select()` to define the transformation, then `.head(n)` to preview results on a subset of images. Nothing is stored in your table.\n",
        "\n",
        "The `image_to_image` function requires:\n",
        "- `image`: The source image to transform\n",
        "- `prompt`: Text describing the desired output\n",
        "- `model_id`: A Hugging Face model ID that supports image-to-image (e.g., `stable-diffusion-v1-5/stable-diffusion-v1-5`, `black-forest-labs/FLUX.1-dev`)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview transformation on first image using FLUX.1-dev\n",
        "# Note: FLUX.1-dev requires accepting the license on HuggingFace and a GPU with sufficient VRAM\n",
        "t.select(\n",
        "    t.image,\n",
        "    t.prompt,\n",
        "    image_to_image(\n",
        "        t.image,\n",
        "        t.prompt,\n",
        "        model_id='black-forest-labs/FLUX.1-dev'\n",
        "    )\n",
        ").head(1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Iterate: adjust transformation strength\n",
        "\n",
        "You control how much the model modifies the original image using `strength` (0.0-1.0):\n",
        "- **Lower values** (0.3-0.5): Subtle changes, preserves more of the original\n",
        "- **Higher values** (0.7-1.0): Dramatic changes, more creative freedom\n",
        "\n",
        "You pass additional parameters through `model_kwargs`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Preview with lower strength (more preservation of original)\n",
        "t.select(\n",
        "    t.image,\n",
        "    t.prompt,\n",
        "    image_to_image(\n",
        "        t.image,\n",
        "        t.prompt,\n",
        "        model_id='stable-diffusion-v1-5/stable-diffusion-v1-5',\n",
        "        model_kwargs={'strength': 0.5, 'num_inference_steps': 30}\n",
        "    )\n",
        ").head(1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Add: apply transformation to all images\n",
        "\n",
        "Once you're satisfied with the results, use `.add_computed_column()` with the same expression. This processes all rows and stores the results permanently in your table.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save as computed column\n",
        "t.add_computed_column(\n",
        "    transformed=image_to_image(\n",
        "        t.image,\n",
        "        t.prompt,\n",
        "        model_id='stable-diffusion-v1-5/stable-diffusion-v1-5',\n",
        "        model_kwargs={'strength': 0.6, 'num_inference_steps': 25}\n",
        "    )\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View original and transformed images side by side\n",
        "t.select(t.image, t.prompt, t.transformed).collect()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Use reproducible results with seeds\n",
        "\n",
        "You set a `seed` parameter to get the same output every time you run the transformation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add reproducible transformation\n",
        "t.add_computed_column(\n",
        "    transformed_seeded=image_to_image(\n",
        "        t.image,\n",
        "        t.prompt,\n",
        "        model_id='stable-diffusion-v1-5/stable-diffusion-v1-5',\n",
        "        seed=42,\n",
        "        model_kwargs={'strength': 0.6}\n",
        "    )\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View results\n",
        "t.select(t.image, t.transformed_seeded).collect()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explanation\n",
        "\n",
        "**How image-to-image works:**\n",
        "\n",
        "Image-to-image diffusion models take an existing image and a text prompt, then generate a new image that blends the structure of the original with the guidance from the prompt. The `strength` parameter controls the balance—lower values preserve more of the original, while higher values allow more dramatic transformations.\n",
        "\n",
        "**Model compatibility:**\n",
        "\n",
        "The `image_to_image` UDF uses `AutoPipelineForImage2Image` from the diffusers library, which automatically detects the model type and selects the appropriate pipeline. You use any compatible model:\n",
        "\n",
        "- `stable-diffusion-v1-5/stable-diffusion-v1-5` - General-purpose, runs on most hardware\n",
        "- `stabilityai/stable-diffusion-xl-base-1.0` - Higher quality, needs more VRAM\n",
        "- `black-forest-labs/FLUX.1-dev` - State-of-the-art quality, requires significant GPU resources\n",
        "\n",
        "**Key parameters:**\n",
        "\n",
        "- `strength` (0.0-1.0): How much to transform the image\n",
        "- `num_inference_steps`: Quality vs speed tradeoff (more steps = better quality)\n",
        "- `guidance_scale`: How closely to follow the prompt (7-8 is typical)\n",
        "- `seed`: For reproducible results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## See also\n",
        "\n",
        "- [Generate images from text](https://docs.pixeltable.com/howto/cookbooks/images/img-generate-images)\n",
        "- [Apply filters to images](https://docs.pixeltable.com/howto/cookbooks/images/img-apply-filters)\n",
        "- [Hugging Face image-to-image models](https://huggingface.co/models?pipeline_tag=image-to-image)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pixeltable",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
