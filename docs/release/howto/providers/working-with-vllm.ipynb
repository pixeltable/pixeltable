{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "title: \"Working with vLLM\"\n",
        "icon: \"notebook\"\n",
        "description: \"[Open in Kaggle](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/howto/providers/working-with-vllm.ipynb) | [Open in Colab](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/howto/providers/working-with-vllm.ipynb) | [View on GitHub](https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/howto/providers/working-with-vllm.ipynb)\"\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This tutorial demonstrates how to use Pixeltable's built-in `vLLM` integration to run local LLMs with high-throughput inference.\n",
        "\n",
        "<div class=\"alert alert-block alert-info\"><!-- mdx:none -->\n",
        "<b>If you are running this tutorial in Colab:</b>\n",
        "vLLM requires a GPU for efficient operation. Click on the <code>Runtime -> Change runtime type</code> menu item at the top, then select the <code>GPU</code> radio button and click on <code>Save</code>.\n",
        "</div>\n",
        "\n",
        "### Important notes\n",
        "\n",
        "- vLLM provides high-throughput inference with techniques like PagedAttention and continuous batching\n",
        "- Models are loaded from HuggingFace and cached in memory for reuse\n",
        "- vLLM currently requires a Linux environment with GPU support for best performance\n",
        "- Consider GPU memory when choosing model sizes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set up environment\n",
        "\n",
        "First, let's install Pixeltable with vLLM support:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -qU pixeltable vllm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a table for chat completions\n",
        "\n",
        "Now let's create a table that will contain our inputs and responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connected to Pixeltable database at: postgresql+psycopg://postgres:@/pixeltable?host=/Users/pjlb/.pixeltable/pgdata\n",
            "Created directory 'vllm_demo'.\n",
            "Created table 'chat'.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/pierre/pixeltable/pixeltable/env.py:494: UserWarning: Progress reporting is disabled because ipywidgets is not installed. To fix this, run: `pip install ipywidgets`\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import pixeltable as pxt\n",
        "from pixeltable.functions import vllm\n",
        "\n",
        "pxt.drop_dir('vllm_demo', force=True)\n",
        "pxt.create_dir('vllm_demo')\n",
        "\n",
        "t = pxt.create_table('vllm_demo/chat', {'input': pxt.String})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we add a computed column that calls the Pixeltable `chat_completions` UDF, which uses vLLM's high-throughput inference engine under the hood. We specify a HuggingFace model identifier, and vLLM will download and cache the model automatically.\n",
        "\n",
        "(If this is your first time using Pixeltable, the <a href=\"https://docs.pixeltable.com/tutorials/tables-and-data-operations\">Pixeltable Fundamentals</a> tutorial contains more details about table creation, computed columns, and UDFs.)\n",
        "\n",
        "For this demo we'll use `Qwen2.5-0.5B-Instruct`, a very small (0.5-billion parameter) model that still produces decent results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added 0 column values with 0 errors in 0.00 s\n",
            "Added 0 column values with 0 errors in 0.00 s\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "No rows affected."
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Add a computed column that uses vLLM for chat completion\n",
        "# against the input.\n",
        "\n",
        "messages = [\n",
        "    {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
        "    {'role': 'user', 'content': t.input},\n",
        "]\n",
        "\n",
        "t.add_computed_column(\n",
        "    result=vllm.chat_completions(\n",
        "        messages,\n",
        "        model='Qwen/Qwen2.5-0.5B-Instruct',\n",
        "    )\n",
        ")\n",
        "\n",
        "# Extract the output content from the JSON structure returned\n",
        "# by vLLM.\n",
        "\n",
        "t.add_computed_column(output=t.result.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test chat completion\n",
        "\n",
        "Let's try a few queries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with a few questions\n",
        "t.insert(\n",
        "    [\n",
        "        {'input': 'What is the capital of France?'},\n",
        "        {'input': 'What are some edible species of fish?'},\n",
        "        {'input': 'Who are the most prominent classical composers?'},\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>input</th>\n",
              "      <th>output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>What is the capital of France?</td>\n",
              "      <td>The capital of France is Paris.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>What are some edible species of fish?</td>\n",
              "      <td>There are many different types of fish that can be eaten, and the best way</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>Who are the most prominent classical composers?</td>\n",
              "      <td>There have been many great classical composers throughout history, and each of them has left</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "                                             input  \\\n",
              "0                   What is the capital of France?   \n",
              "1            What are some edible species of fish?   \n",
              "2  Who are the most prominent classical composers?   \n",
              "\n",
              "                                              output  \n",
              "0                    The capital of France is Paris.  \n",
              "1  There are many different types of fish that ca...  \n",
              "2  There have been many great classical composers...  "
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "t.select(t.input, t.output).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparing models\n",
        "\n",
        "vLLM makes it easy to compare the output of different models. Let's try comparing the output from `Qwen2.5-0.5B` against a somewhat larger model, `Qwen2.5-1.5B-Instruct`. As always, when we add a new computed column to our table, it's automatically evaluated against the existing table rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "t.add_computed_column(\n",
        "    result_qwen15=vllm.chat_completions(\n",
        "        messages,\n",
        "        model='Qwen/Qwen2.5-1.5B-Instruct',\n",
        "    )\n",
        ")\n",
        "\n",
        "t.add_computed_column(output_qwen15=t.result_qwen15.choices[0].message.content)\n",
        "\n",
        "t.select(t.input, t.output, t.output_qwen15).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using model_kwargs for sampling parameters\n",
        "\n",
        "vLLM supports fine-grained control over generation through `model_kwargs`. Sampling parameters like `max_tokens`, `temperature`, `top_p`, and `top_k` are passed alongside engine parameters — Pixeltable automatically routes each to the right place. Let's try running with a different system prompt and custom sampling settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added 3 column values with 0 errors in 18.49 s (0.16 rows/s)\n",
            "Added 3 column values with 0 errors in 0.01 s (280.87 rows/s)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>input</th>\n",
              "      <th>output_teacher</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>What is the capital of France?</td>\n",
              "      <td>The capital of France is Paris.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>What are some edible species of fish?</td>\n",
              "      <td>There are many edible fish species. Here are a few popular ones:\n",
              "\n",
              "1. **Salmon (Oncorhynchus sp.)**: A medium-sized fish that can be caught in streams, rivers, and lakes. They are very nutritious and are a good source of omega-3 fatty acids.\n",
              "\n",
              "2. **Salmon Trout (Salmo trutta)**: Also known as &quot;saltwater bass,&quot; these fish are smaller and more prevalent in saltwater environments. They are lean and high in protein.\n",
              "\n",
              "3. **Pike (Esox lucidus)**: A small, slow-moving fish that is commonly found in freshwater habitats. They are a good source of protein and omega-3s.\n",
              "\n",
              "4. **Pike Salmon (Esox lucius)**: Similar to the Pike, but they are larger and more aggressive. They are a good source of protein and omega-3s.\n",
              "\n",
              "5. **Herring (Salmo salar)**: A freshwater fish that is quite popular and has a mild flavor. They are low in fat but high in protein.\n",
              "\n",
              "6. **Cod (Gadus morhua)**: A small, cold-water fish that is often used in cooking due to its mild flavor. It is also an important part of the</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>Who are the most prominent classical composers?</td>\n",
              "      <td>The most prominent classical composers are composers from the classical period, which is roughly from the late 17th to the late 19th century. Some of the most well-known composers from this period include:\n",
              "\n",
              "1. Ludwig van Beethoven (1770-1827): He was a pivotal figure in the development of classical music and is known for his revolutionary innovations such as the symphony, the piano sonata, and the introduction of the concerto form.\n",
              "\n",
              "2. Wolfgang Amadeus Mozart (1756-1791): A prolific composer ...... eras, and choral works, as well as his keyboard music and piano sonatas.\n",
              "\n",
              "3. Richard Wagner (1813-1883): He was a German composer and music theorist who is often considered the father of the modern opera style. His works include the operas &quot;The Ring Cycle&quot; and &quot;The Green Hornet.&quot;\n",
              "\n",
              "4. Johann Sebastian Bach (1685-1750): He was a German composer who is one of the most important composers in history. He is known for his contrapuntal style, which involves the use of multiple voices and the use of</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "                                             input  \\\n",
              "0                   What is the capital of France?   \n",
              "1            What are some edible species of fish?   \n",
              "2  Who are the most prominent classical composers?   \n",
              "\n",
              "                                      output_teacher  \n",
              "0                    The capital of France is Paris.  \n",
              "1  There are many edible fish species. Here are a...  \n",
              "2  The most prominent classical composers are com...  "
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "messages_teacher = [\n",
        "    {\n",
        "        'role': 'system',\n",
        "        'content': 'You are a patient school teacher. '\n",
        "        'Explain concepts simply and clearly.',\n",
        "    },\n",
        "    {'role': 'user', 'content': t.input},\n",
        "]\n",
        "\n",
        "t.add_computed_column(\n",
        "    result_teacher=vllm.chat_completions(\n",
        "        messages_teacher,\n",
        "        model='Qwen/Qwen2.5-0.5B-Instruct',\n",
        "        model_kwargs={'max_tokens': 256, 'temperature': 0.7, 'top_p': 0.9},\n",
        "    )\n",
        ")\n",
        "\n",
        "t.add_computed_column(\n",
        "    output_teacher=t.result_teacher.choices[0].message.content\n",
        ")\n",
        "\n",
        "t.select(t.input, t.output_teacher).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text generation\n",
        "\n",
        "In addition to chat completions, vLLM also supports direct text generation with the `generate` UDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created table 'generation'.\n",
            "Added 0 column values with 0 errors in 0.00 s\n",
            "Added 0 column values with 0 errors in 0.00 s\n",
            "Inserted 2 rows with 0 errors in 5.42 s (0.37 rows/s)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>prompt</th>\n",
              "      <th>output</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>The capital of France is</td>\n",
              "      <td>:\n",
              "\n",
              "A: Paris\n",
              "\n",
              "B: Rome\n",
              "\n",
              "C: London\n",
              "\n",
              "D: Besançon\n",
              "\n",
              "To determine the capital of France, we need to identify the capital of the country known as France. France is a European country that shares a border with several other countries as well. The main cities containing capitals of different European countries can include Rome (Italy), London (United Kingdom), and Paris (France). However, a notable example is the capital of the United Kingdom, London.\n",
              "\n",
              "Here is the breakdown of</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>Once upon a time, there was a</td>\n",
              "      <td>group of ancient arthropods that started to express a desire for communication. This desire led them to discover the ability to use a form of chemical communication. While this was their first step toward cooperation with their new friends, they did not actually exchange friendly remarks. Instead, the ancient arthropods exchanged tens of thousands of pieces of metamorphic rock, called crystals, in a variety of sizes and colors.\n",
              "\n",
              "The diamond embodied mature metamorphic rock, and his researcher by the name of Charles told the group</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "                          prompt  \\\n",
              "0       The capital of France is   \n",
              "1  Once upon a time, there was a   \n",
              "\n",
              "                                              output  \n",
              "0  :\\n\\nA: Paris\\n\\nB: Rome\\n\\nC: London\\n\\nD: Be...  \n",
              "1   group of ancient arthropods that started to e...  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "gen_t = pxt.create_table('vllm_demo/generation', {'prompt': pxt.String})\n",
        "\n",
        "gen_t.add_computed_column(\n",
        "    result=vllm.generate(\n",
        "        gen_t.prompt,\n",
        "        model='Qwen/Qwen2.5-0.5B-Instruct',\n",
        "        model_kwargs={'max_tokens': 100},\n",
        "    )\n",
        ")\n",
        "\n",
        "gen_t.add_computed_column(output=gen_t.result.choices[0].text)\n",
        "\n",
        "gen_t.insert(\n",
        "    [\n",
        "        {'prompt': 'The capital of France is'},\n",
        "        {'prompt': 'Once upon a time, there was a'},\n",
        "    ]\n",
        ")\n",
        "\n",
        "gen_t.select(gen_t.prompt, gen_t.output).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Additional Resources\n",
        "\n",
        "- [Pixeltable Documentation](https://docs.pixeltable.com/)\n",
        "- [vLLM Documentation](https://docs.vllm.ai/)\n",
        "- [vLLM GitHub](https://github.com/vllm-project/vllm)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pixeltable",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
