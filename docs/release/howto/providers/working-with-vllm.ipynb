{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {
        "vscode": {
          "languageId": "raw"
        }
      },
      "source": [
        "---\n",
        "title: \"Working with vLLM\"\n",
        "icon: \"notebook\"\n",
        "description: \"[Open in Kaggle](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/howto/providers/working-with-vllm.ipynb) | [Open in Colab](https://colab.research.google.com/github/pixeltable/pixeltable/blob/release/docs/notebooks/howto/providers/working-with-vllm.ipynb) | [View on GitHub](https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/howto/providers/working-with-vllm.ipynb)\"\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This tutorial demonstrates how to use Pixeltable's built-in `vLLM` integration to run local LLMs with high-throughput inference.\n",
        "\n",
        "<div class=\"alert alert-block alert-info\"><!-- mdx:none -->\n",
        "<b>If you are running this tutorial in Colab:</b>\n",
        "vLLM requires a GPU for efficient operation. Click on the <code>Runtime -> Change runtime type</code> menu item at the top, then select the <code>GPU</code> radio button and click on <code>Save</code>.\n",
        "</div>\n",
        "\n",
        "### Important notes\n",
        "\n",
        "- vLLM provides high-throughput inference with techniques like PagedAttention and continuous batching\n",
        "- Models are loaded from HuggingFace and cached in memory for reuse\n",
        "- vLLM currently requires a Linux environment with GPU support for best performance\n",
        "- Consider GPU memory when choosing model sizes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Set up environment\n",
        "\n",
        "First, let's install Pixeltable with vLLM support:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/opt/miniconda3/envs/pixeltable/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/opt/miniconda3/envs/pixeltable/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "%pip install -qU pixeltable vllm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create a table for chat completions\n",
        "\n",
        "Now let's create a table that will contain our inputs and responses."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pixeltable as pxt\n",
        "from pixeltable.functions import vllm\n",
        "\n",
        "pxt.drop_dir('vllm_demo', force=True)\n",
        "pxt.create_dir('vllm_demo')\n",
        "\n",
        "t = pxt.create_table('vllm_demo/chat', {'input': pxt.String})"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, we add a computed column that calls the Pixeltable `chat_completions` UDF, which uses vLLM's high-throughput inference engine under the hood. We specify a HuggingFace model identifier, and vLLM will download and cache the model automatically.\n",
        "\n",
        "(If this is your first time using Pixeltable, the <a href=\"https://docs.pixeltable.com/tutorials/tables-and-data-operations\">Pixeltable Fundamentals</a> tutorial contains more details about table creation, computed columns, and UDFs.)\n",
        "\n",
        "For this demo we'll use `Qwen2.5-0.5B-Instruct`, a very small (0.5-billion parameter) model that still produces decent results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add a computed column that uses vLLM for chat completion\n",
        "# against the input.\n",
        "\n",
        "messages = [\n",
        "    {'role': 'system', 'content': 'You are a helpful assistant.'},\n",
        "    {'role': 'user', 'content': t.input},\n",
        "]\n",
        "\n",
        "t.add_computed_column(\n",
        "    result=vllm.chat_completions(\n",
        "        messages,\n",
        "        model='Qwen/Qwen2.5-0.5B-Instruct',\n",
        "    )\n",
        ")\n",
        "\n",
        "# Extract the output content from the JSON structure returned\n",
        "# by vLLM.\n",
        "\n",
        "t.add_computed_column(output=t.result.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Test chat completion\n",
        "\n",
        "Let's try a few queries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test with a few questions\n",
        "t.insert(\n",
        "    [\n",
        "        {'input': 'What is the capital of France?'},\n",
        "        {'input': 'What are some edible species of fish?'},\n",
        "        {'input': 'Who are the most prominent classical composers?'},\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "t.select(t.input, t.output).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparing models\n",
        "\n",
        "vLLM makes it easy to compare the output of different models. Let's try comparing the output from `Qwen2.5-0.5B` against a somewhat larger model, `Llama-3.2-1B-Instruct`. As always, when we add a new computed column to our table, it's automatically evaluated against the existing table rows."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "t.add_computed_column(\n",
        "    result_l3=vllm.chat_completions(\n",
        "        messages,\n",
        "        model='meta-llama/Llama-3.2-1B-Instruct',\n",
        "    )\n",
        ")\n",
        "\n",
        "t.add_computed_column(output_l3=t.result_l3.choices[0].message.content)\n",
        "\n",
        "t.select(t.input, t.output, t.output_l3).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using sampling parameters\n",
        "\n",
        "vLLM supports fine-grained control over generation through sampling parameters. Let's try running with a different system prompt and custom sampling settings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "messages_teacher = [\n",
        "    {\n",
        "        'role': 'system',\n",
        "        'content': 'You are a patient school teacher. '\n",
        "        'Explain concepts simply and clearly.',\n",
        "    },\n",
        "    {'role': 'user', 'content': t.input},\n",
        "]\n",
        "\n",
        "t.add_computed_column(\n",
        "    result_teacher=vllm.chat_completions(\n",
        "        messages_teacher,\n",
        "        model='Qwen/Qwen2.5-0.5B-Instruct',\n",
        "        sampling_kwargs={'max_tokens': 256, 'temperature': 0.7, 'top_p': 0.9},\n",
        "    )\n",
        ")\n",
        "\n",
        "t.add_computed_column(\n",
        "    output_teacher=t.result_teacher.choices[0].message.content\n",
        ")\n",
        "\n",
        "t.select(t.input, t.output_teacher).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text generation\n",
        "\n",
        "In addition to chat completions, vLLM also supports direct text generation with the `generate` UDF."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "gen_t = pxt.create_table('vllm_demo/generation', {'prompt': pxt.String})\n",
        "\n",
        "gen_t.add_computed_column(\n",
        "    result=vllm.generate(\n",
        "        gen_t.prompt,\n",
        "        model='Qwen/Qwen2.5-0.5B-Instruct',\n",
        "        sampling_kwargs={'max_tokens': 100},\n",
        "    )\n",
        ")\n",
        "\n",
        "gen_t.add_computed_column(output=gen_t.result.choices[0].text)\n",
        "\n",
        "gen_t.insert(\n",
        "    [\n",
        "        {'prompt': 'The capital of France is'},\n",
        "        {'prompt': 'Once upon a time, there was a'},\n",
        "    ]\n",
        ")\n",
        "\n",
        "gen_t.select(gen_t.prompt, gen_t.output).collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Additional Resources\n",
        "\n",
        "- [Pixeltable Documentation](https://docs.pixeltable.com/)\n",
        "- [vLLM Documentation](https://docs.vllm.ai/)\n",
        "- [vLLM GitHub](https://github.com/vllm-project/vllm)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "pixeltable",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
