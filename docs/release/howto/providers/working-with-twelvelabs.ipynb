{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Working with Twelve Labs in Pixeltable\n",
        "\n",
        "Pixeltable's Twelve Labs integration enables you to create powerful multimodal embeddings for text, images, audio, and video using the Twelve Labs Embed API.\n",
        "\n",
        "### Prerequisites\n",
        "- A Twelve Labs account with an API key (https://playground.twelvelabs.io/)\n",
        "\n",
        "### Important Notes\n",
        "\n",
        "- Twelve Labs usage may incur costs based on your plan.\n",
        "- Audio and video embeddings require a minimum duration of 4 seconds.\n",
        "- The `marengo3.0` model produces 512-dimensional embeddings.\n",
        "- Similarity search supports: `string=` for text queries, `image=` for image queries (PIL Image object)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Setup\n",
        "\n",
        "First, install the required libraries and configure your API key."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "%pip install -qU pixeltable twelvelabs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "if 'TWELVELABS_API_KEY' not in os.environ:\n",
        "    os.environ['TWELVELABS_API_KEY'] = getpass.getpass('Enter your Twelve Labs API key: ')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pixeltable as pxt\n",
        "from pixeltable.functions.twelvelabs import embed\n",
        "\n",
        "# Create a fresh directory for our demo\n",
        "pxt.drop_dir('twelvelabs_demo', force=True)\n",
        "pxt.create_dir('twelvelabs_demo')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Embeddings\n",
        "\n",
        "Create text embeddings using the Twelve Labs embed function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a table with text and add an embedding index\n",
        "text_t = pxt.create_table('twelvelabs_demo.text_search', {'text': pxt.String})\n",
        "\n",
        "# Add embedding index for text similarity search\n",
        "text_t.add_embedding_index(\n",
        "    'text',\n",
        "    embedding=embed.using(model_name='marengo3.0')\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Insert sample documents\n",
        "documents = [\n",
        "    \"Artificial intelligence is transforming video understanding and analysis.\",\n",
        "    \"Machine learning models can detect objects and actions in video streams.\",\n",
        "    \"Natural language processing enables understanding of spoken words in audio.\",\n",
        "    \"Computer vision techniques analyze visual patterns in images and videos.\",\n",
        "    \"Deep learning models generate embeddings that capture semantic meaning.\",\n",
        "    \"Multimodal AI systems combine understanding of text, images, and audio.\",\n",
        "]\n",
        "\n",
        "text_t.insert({'text': doc} for doc in documents)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Perform semantic similarity search\n",
        "query = \"How do AI systems understand video content?\"\n",
        "sim = text_t.text.similarity(string=query)\n",
        "\n",
        "text_t.order_by(sim, asc=False).limit(3).select(text_t.text, score=sim).collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Document Embeddings (PDF, HTML, Markdown)\n",
        "\n",
        "Create embeddings from documents like PDFs, HTML, and Markdown files. Use the `document_splitter` iterator to chunk documents into searchable text segments."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pixeltable.functions.document import document_splitter\n",
        "\n",
        "# Create a table with documents\n",
        "doc_t = pxt.create_table('twelvelabs_demo.documents', {'document': pxt.Document})\n",
        "\n",
        "# Create a view that chunks documents into text segments\n",
        "doc_chunks_v = pxt.create_view(\n",
        "    'twelvelabs_demo.doc_chunks',\n",
        "    doc_t,\n",
        "    iterator=document_splitter(\n",
        "        document=doc_t.document,\n",
        "        separators='sentence'  # Split by sentence for fine-grained search\n",
        "    )\n",
        ")\n",
        "\n",
        "# Add embedding index on the text chunks\n",
        "doc_chunks_v.add_embedding_index(\n",
        "    'text',\n",
        "    embedding=embed.using(model_name='marengo3.0')\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Insert a PDF document\n",
        "pdf_url = 'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/rag-demo/Argus-Market-Digest-June-2024.pdf'\n",
        "doc_t.insert([{'document': pdf_url}])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Search document chunks using text query\n",
        "sim = doc_chunks_v.text.similarity(string=\"market performance and stock trends\")\n",
        "\n",
        "doc_chunks_v.order_by(sim, asc=False).limit(3).select(\n",
        "    doc_chunks_v.text,\n",
        "    score=sim\n",
        ").collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Image Embeddings\n",
        "\n",
        "Create image embeddings and search using both **text queries** (cross-modal search) and **image queries** (image-to-image search)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a table with images and add an embedding index\n",
        "image_t = pxt.create_table('twelvelabs_demo.image_search', {'image': pxt.Image})\n",
        "\n",
        "# Add embedding index - supports both image indexing and text-based queries\n",
        "image_t.add_embedding_index(\n",
        "    'image',\n",
        "    embedding=embed.using(model_name='marengo3.0')\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Insert sample images\n",
        "image_urls = [\n",
        "    'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images/000000000025.jpg',\n",
        "    'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images/000000000139.jpg',\n",
        "    'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images/000000000042.jpg',\n",
        "    'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images/000000000061.jpg',\n",
        "]\n",
        "\n",
        "image_t.insert({'image': url} for url in image_urls)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Search images using text (cross-modal search)\n",
        "sim = image_t.image.similarity(string=\"animals in nature\")\n",
        "\n",
        "image_t.order_by(sim, asc=False).limit(2).select(image_t.image, score=sim).collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Image-based image search (image-to-image similarity)\n",
        "# Load a query image and find similar images in the table\n",
        "from PIL import Image\n",
        "import urllib.request\n",
        "\n",
        "# Download and load a query image\n",
        "query_image_url = 'https://upload.wikimedia.org/wikipedia/commons/thumb/7/7a/Huskiesatrest.jpg/640px-Huskiesatrest.jpg'\n",
        "with urllib.request.urlopen(query_image_url) as response:\n",
        "    query_image = Image.open(response)\n",
        "\n",
        "# Search for similar images using the image query\n",
        "sim_image = image_t.image.similarity(image=query_image)\n",
        "\n",
        "image_t.order_by(sim_image, asc=False).limit(2).select(\n",
        "    image=image_t.image, \n",
        "    similarity=sim_image\n",
        ").collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text + Image Combined Embeddings\n",
        "\n",
        "A unique feature of Twelve Labs is the ability to create embeddings from **both text and image together**. This captures the joint semantic representation of multimodal content and is useful for image captioning, visual question answering, and other multimodal applications."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a table for text+image combined embeddings\n",
        "multimodal_t = pxt.create_table(\n",
        "    'twelvelabs_demo.text_image_combined',\n",
        "    {'image': pxt.Image, 'caption': pxt.String}\n",
        ")\n",
        "\n",
        "# Add computed column that creates embeddings from BOTH text and image together\n",
        "# This uses the text_image embedding type in the Twelve Labs API\n",
        "multimodal_t.add_computed_column(\n",
        "    combined_embedding=embed(\n",
        "        multimodal_t.caption,  # text parameter\n",
        "        multimodal_t.image,    # image parameter (optional)\n",
        "        model_name='marengo3.0'\n",
        "    )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Insert images with captions\n",
        "multimodal_t.insert([\n",
        "    {\n",
        "        'image': 'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images/000000000139.jpg',\n",
        "        'caption': 'A person standing next to an elephant'\n",
        "    },\n",
        "    {\n",
        "        'image': 'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images/000000000025.jpg',\n",
        "        'caption': 'A giraffe in a natural habitat'\n",
        "    }\n",
        "])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# View the combined embeddings\n",
        "multimodal_t.select(\n",
        "    multimodal_t.image,\n",
        "    multimodal_t.caption,\n",
        "    multimodal_t.combined_embedding\n",
        ").collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Audio Embeddings with Embedding Index\n",
        "\n",
        "Create audio embeddings and search using text queries. Audio segments must be at least 4 seconds long.\n",
        "\n",
        "Twelve Labs audio embeddings support **embedding options** to focus on different aspects:\n",
        "- `'audio'`: Focus on the raw audio signal (sounds, music, ambient noise)\n",
        "- `'transcription'`: Focus on the spoken content (what is said)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pixeltable.functions.audio import audio_splitter\n",
        "\n",
        "# Create a base table for audio files\n",
        "audio_t = pxt.create_table('twelvelabs_demo.audio_files', {'audio': pxt.Audio})\n",
        "\n",
        "# Insert a sample audio file (JFK speech excerpt)\n",
        "audio_url = 'https://github.com/pixeltable/pixeltable/raw/release/tests/data/audio/jfk_1961_0109_cityuponahill-excerpt.flac'\n",
        "audio_t.insert([{'audio': audio_url}])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a view that chunks the audio into searchable segments\n",
        "# Twelve Labs requires minimum 4 second duration\n",
        "audio_chunks_v = pxt.create_view(\n",
        "    'twelvelabs_demo.audio_chunks',\n",
        "    audio_t,\n",
        "    iterator=audio_splitter(\n",
        "        audio_t.audio,\n",
        "        chunk_duration_sec=5.0,\n",
        "        min_chunk_duration_sec=4.0\n",
        "    )\n",
        ")\n",
        "\n",
        "# Add embedding index for similarity search\n",
        "audio_chunks_v.add_embedding_index(\n",
        "    'audio_chunk',\n",
        "    embedding=embed.using(model_name='marengo3.0')\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Search audio chunks using text query\n",
        "sim = audio_chunks_v.audio_chunk.similarity(string=\"speech about government and politics\")\n",
        "\n",
        "audio_chunks_v.order_by(sim, asc=False).limit(3).select(\n",
        "    audio_chunks_v.audio_chunk,\n",
        "    score=sim\n",
        ").collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Audio Embedding Options\n",
        "\n",
        "Use `embedding_option` to focus on specific aspects of the audio content."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create computed column with transcription-focused embedding\n",
        "audio_chunks_v.add_computed_column(\n",
        "    transcription_embedding=embed(\n",
        "        audio_chunks_v.audio_chunk,\n",
        "        model_name='marengo3.0',\n",
        "        embedding_option=['transcription']  # Focus on spoken content\n",
        "    )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# View the transcription-focused embeddings\n",
        "audio_chunks_v.select(\n",
        "    audio_chunks_v.audio_chunk,\n",
        "    audio_chunks_v.transcription_embedding\n",
        ").limit(2).collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Video Embeddings with Embedding Index\n",
        "\n",
        "Create video embeddings and search using text queries. Video segments must be at least 4 seconds long.\n",
        "\n",
        "Twelve Labs video embeddings support **embedding options** to focus on different aspects:\n",
        "- `'visual'`: Focus on visual content (what you see)\n",
        "- `'audio'`: Focus on audio content (what you hear)\n",
        "- `'transcription'`: Focus on spoken content (what is said)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "from pixeltable.functions.video import video_splitter\n",
        "\n",
        "# Create a base table for video files\n",
        "video_t = pxt.create_table('twelvelabs_demo.video_files', {'video': pxt.Video})\n",
        "\n",
        "# Insert a sample video file\n",
        "video_url = 'https://github.com/pixeltable/pixeltable/raw/release/tests/data/videos/bangkok_half_res.mp4'\n",
        "video_t.insert([{'video': video_url}])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create a view that segments the video\n",
        "# Twelve Labs requires minimum 4 second duration\n",
        "video_segments_v = pxt.create_view(\n",
        "    'twelvelabs_demo.video_segments',\n",
        "    video_t,\n",
        "    iterator=video_splitter(\n",
        "        video=video_t.video,\n",
        "        duration=5.0,\n",
        "        min_segment_duration=4.0\n",
        "    )\n",
        ")\n",
        "\n",
        "# Add embedding index for similarity search\n",
        "video_segments_v.add_embedding_index(\n",
        "    'video_segment',\n",
        "    embedding=embed.using(model_name='marengo3.0')\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Search video segments using text query\n",
        "sim = video_segments_v.video_segment.similarity(string=\"city traffic and urban scenery\")\n",
        "\n",
        "video_segments_v.order_by(sim, asc=False).limit(3).select(\n",
        "    video_segments_v.video_segment,\n",
        "    score=sim\n",
        ").collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Video Embedding Options\n",
        "\n",
        "Use `embedding_option` to focus on specific aspects of the video content."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Create computed column with visual-focused embedding\n",
        "video_segments_v.add_computed_column(\n",
        "    visual_embedding=embed(\n",
        "        video_segments_v.video_segment,\n",
        "        model_name='marengo3.0',\n",
        "        embedding_option=['visual']  # Focus on visual content only\n",
        "    )\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# View the visual-focused embeddings\n",
        "video_segments_v.select(\n",
        "    video_segments_v.video_segment,\n",
        "    video_segments_v.visual_embedding\n",
        ").limit(2).collect()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Available Models\n",
        "\n",
        "Twelve Labs provides several embedding models:\n",
        "\n",
        "| Model | Embedding Dimension | Description |\n",
        "|-------|---------------------|-------------|\n",
        "| `marengo3.0` | 512 | Latest multimodal embedding model |\n",
        "| `Marengo-retrieval-2.7` | 1024 | Retrieval-optimized model |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary of Twelve Labs Features\n",
        "\n",
        "| Feature | Description |\n",
        "|---------|-------------|\n",
        "| Text embeddings | `embed(text, model_name=...)` |\n",
        "| Image embeddings | `embed(image, model_name=...)` |\n",
        "| Text + Image combined | `embed(text, image, model_name=...)` - unique joint embedding |\n",
        "| Audio embeddings | `embed(audio, model_name=..., embedding_option=[...])` |\n",
        "| Video embeddings | `embed(video, model_name=..., embedding_option=[...])` |\n",
        "| Document search | `document_splitter` + text embedding on chunks |\n",
        "| Embedding indices | `add_embedding_index(col, embedding=embed.using(...))` |\n",
        "| Text similarity search | `col.similarity(string=\"query\")` |\n",
        "| Image similarity search | `col.similarity(image=pil_image)` |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Learn More\n",
        "\n",
        "- [Twelve Labs Documentation](https://docs.twelvelabs.io/)\n",
        "- [Embed API Guide](https://docs.twelvelabs.io/v1.3/docs/guides/create-embeddings)\n",
        "- [Pixeltable Embedding Indexes](https://docs.pixeltable.com/platform/embedding-indexes)\n",
        "\n",
        "If you have any questions, don't hesitate to reach out."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}