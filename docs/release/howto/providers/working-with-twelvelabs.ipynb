{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Twelve Labs in Pixeltable\n",
    "\n",
    "Pixeltable's Twelve Labs integration enables you to create powerful multimodal embeddings for text, images, audio, and video using the Twelve Labs Embed API. These embeddings allow you to build semantic search across all modalities.\n",
    "\n",
    "### Prerequisites\n",
    "- A Twelve Labs account with an API key (https://playground.twelvelabs.io/)\n",
    "\n",
    "### Important Notes\n",
    "\n",
    "- Twelve Labs usage may incur costs based on your plan.\n",
    "- Audio and video embeddings require a minimum duration of 4 seconds.\n",
    "- The `marengo3.0` model produces 512-dimensional embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, install the required libraries and configure your API key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Ignoring invalid distribution ~orch (/opt/miniconda3/envs/pixeltable/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/opt/miniconda3/envs/pixeltable/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/opt/miniconda3/envs/pixeltable/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/opt/miniconda3/envs/pixeltable/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/opt/miniconda3/envs/pixeltable/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution ~orch (/opt/miniconda3/envs/pixeltable/lib/python3.11/site-packages)\u001b[0m\u001b[33m\n",
      "\u001b[0mNote: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -qU pixeltable twelvelabs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "if 'TWELVELABS_API_KEY' not in os.environ:\n",
    "    os.environ['TWELVELABS_API_KEY'] = getpass.getpass('Enter your Twelve Labs API key: ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory 'twelvelabs_demo'.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<pixeltable.catalog.dir.Dir at 0x36102f350>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pixeltable as pxt\n",
    "from pixeltable.functions.twelvelabs import embed\n",
    "\n",
    "# Create a fresh directory for our demo\n",
    "pxt.drop_dir('twelvelabs_demo', force=True)\n",
    "pxt.create_dir('twelvelabs_demo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Embeddings with Embedding Index\n",
    "\n",
    "Create text embeddings and enable semantic search using an embedding index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table 'text_search'.\n"
     ]
    }
   ],
   "source": [
    "# Create a table with text and add an embedding index\n",
    "text_t = pxt.create_table('twelvelabs_demo.text_search', {'text': pxt.String})\n",
    "\n",
    "# Add embedding index for text similarity search\n",
    "text_t.add_embedding_index(\n",
    "    'text',\n",
    "    string_embed=embed.using(model_name='marengo3.0')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting rows into `text_search`: 6 rows [00:00, 979.94 rows/s]\n",
      "Inserted 6 rows with 0 errors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "6 rows inserted, 12 values computed."
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert sample documents\n",
    "documents = [\n",
    "    \"Artificial intelligence is transforming video understanding and analysis.\",\n",
    "    \"Machine learning models can detect objects and actions in video streams.\",\n",
    "    \"Natural language processing enables understanding of spoken words in audio.\",\n",
    "    \"Computer vision techniques analyze visual patterns in images and videos.\",\n",
    "    \"Deep learning models generate embeddings that capture semantic meaning.\",\n",
    "    \"Multimodal AI systems combine understanding of text, images, and audio.\",\n",
    "]\n",
    "\n",
    "text_t.insert({'text': doc} for doc in documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>text</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Artificial intelligence is transforming video understanding and analysis.</td>\n",
       "      <td>0.867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Computer vision techniques analyze visual patterns in images and videos.</td>\n",
       "      <td>0.669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Machine learning models can detect objects and actions in video streams.</td>\n",
       "      <td>0.656</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "                                                text     score\n",
       "0  Artificial intelligence is transforming video ...  0.867369\n",
       "1  Computer vision techniques analyze visual patt...  0.668609\n",
       "2  Machine learning models can detect objects and...  0.655579"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform semantic similarity search\n",
    "query = \"How do AI systems understand video content?\"\n",
    "sim = text_t.text.similarity(string=query)\n",
    "\n",
    "text_t.order_by(sim, asc=False).limit(3).select(text_t.text, score=sim).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Embeddings with Embedding Index\n",
    "\n",
    "Create image embeddings and search for similar images using text queries (cross-modal search)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table 'image_search'.\n"
     ]
    }
   ],
   "source": [
    "# Create a table with images and add an embedding index\n",
    "image_t = pxt.create_table('twelvelabs_demo.image_search', {'image': pxt.Image})\n",
    "\n",
    "# Add embedding index using the generic 'embedding' parameter\n",
    "# The Twelve Labs embed function supports multiple modalities (text, image, etc.)\n",
    "# so it will automatically support both image indexing and text-based queries\n",
    "image_t.add_embedding_index(\n",
    "    'image',\n",
    "    embedding=embed.using(model_name='marengo3.0')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting rows into `image_search`: 4 rows [00:00, 1270.62 rows/s]\n",
      "Inserted 4 rows with 0 errors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4 rows inserted, 12 values computed."
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert sample images\n",
    "image_urls = [\n",
    "    'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images/000000000025.jpg',\n",
    "    'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images/000000000139.jpg',\n",
    "    'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images/000000000042.jpg',\n",
    "    'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images/000000000061.jpg',\n",
    "]\n",
    "\n",
    "image_t.insert({'image': url} for url in image_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search images using text (cross-modal search)\n",
    "sim = image_t.image.similarity(string=\"animals in nature\")\n",
    "\n",
    "image_t.order_by(sim, asc=False).limit(2).select(image_t.image, score=sim).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Audio Embeddings with Embedding Index\n",
    "\n",
    "Create audio embeddings and enable semantic search over audio content. Audio segments must be at least 4 seconds long, so we use the audio splitter to create appropriately-sized chunks.\n",
    "\n",
    "**Note:** This feature was added in PR #990. You can now index audio columns and search them using text queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixeltable.functions.audio import audio_splitter\n",
    "\n",
    "# Create a base table for audio files\n",
    "audio_t = pxt.create_table('twelvelabs_demo.audio_files', {'audio': pxt.Audio})\n",
    "\n",
    "# Insert a sample audio file (JFK speech excerpt)\n",
    "audio_url = 'https://github.com/pixeltable/pixeltable/raw/release/tests/data/audio/jfk_1961_0109_cityuponahill-excerpt.flac'\n",
    "audio_t.insert([{'audio': audio_url}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a view that chunks the audio into searchable segments\n",
    "# Twelve Labs requires minimum 4 second duration\n",
    "audio_chunks_v = pxt.create_view(\n",
    "    'twelvelabs_demo.audio_chunks',\n",
    "    audio_t,\n",
    "    iterator=audio_splitter(\n",
    "        audio_t.audio,\n",
    "        chunk_duration_sec=5.0,\n",
    "        min_chunk_duration_sec=4.0\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add embedding index using the generic 'embedding' parameter\n",
    "# The Twelve Labs embed function supports multiple modalities, so it will\n",
    "# automatically resolve to both audio and text embeddings\n",
    "audio_chunks_v.add_embedding_index(\n",
    "    'audio_chunk',\n",
    "    embedding=embed.using(model_name='marengo3.0')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search audio chunks using text\n",
    "sim = audio_chunks_v.audio_chunk.similarity(string=\"speech about government and politics\")\n",
    "\n",
    "audio_chunks_v.order_by(sim, asc=False).limit(3).select(\n",
    "    audio_chunks_v.audio_chunk,\n",
    "    score=sim\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also retrieve the embedding directly\n",
    "audio_chunks_v.select(\n",
    "    audio_chunks_v.audio_chunk,\n",
    "    embedding=audio_chunks_v.audio_chunk.embedding()\n",
    ").limit(2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Embeddings with Embedding Index\n",
    "\n",
    "Create video embeddings and enable semantic search over video content. Video segments must be at least 4 seconds long.\n",
    "\n",
    "**Note:** This feature was added in PR #990. You can now index video columns and search them using text queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixeltable.functions.video import video_splitter\n",
    "\n",
    "# Create a base table for video files\n",
    "video_t = pxt.create_table('twelvelabs_demo.video_files', {'video': pxt.Video})\n",
    "\n",
    "# Insert a sample video file\n",
    "video_url = 'https://github.com/pixeltable/pixeltable/raw/release/tests/data/videos/bangkok_half_res.mp4'\n",
    "video_t.insert([{'video': video_url}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a view that segments the video into searchable chunks\n",
    "# Twelve Labs requires minimum 4 second duration\n",
    "video_segments_v = pxt.create_view(\n",
    "    'twelvelabs_demo.video_segments',\n",
    "    video_t,\n",
    "    iterator=video_splitter(\n",
    "        video=video_t.video,\n",
    "        duration=5.0,\n",
    "        min_segment_duration=4.0\n",
    "    )\n",
    ")\n",
    "\n",
    "# Add embedding index using the generic 'embedding' parameter\n",
    "# The Twelve Labs embed function supports multiple modalities, so it will\n",
    "# automatically resolve to both video and text embeddings\n",
    "video_segments_v.add_embedding_index(\n",
    "    'video_segment',\n",
    "    embedding=embed.using(model_name='marengo3.0')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search video segments using text\n",
    "sim = video_segments_v.video_segment.similarity(string=\"city traffic and urban scenery\")\n",
    "\n",
    "video_segments_v.order_by(sim, asc=False).limit(3).select(\n",
    "    video_segments_v.video_segment,\n",
    "    score=sim\n",
    ").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve the embedding directly\n",
    "video_segments_v.select(\n",
    "    video_segments_v.video_segment,\n",
    "    embedding=video_segments_v.video_segment.embedding()\n",
    ").limit(2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computed Columns with Embeddings\n",
    "\n",
    "You can also create computed columns that store embeddings directly, which is useful when you want to use the embeddings for other purposes beyond similarity search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a table with a computed embedding column\n",
    "docs_t = pxt.create_table('twelvelabs_demo.docs_with_embeddings', {'text': pxt.String})\n",
    "\n",
    "# Add computed column for embeddings\n",
    "docs_t.add_computed_column(\n",
    "    embedding=embed(docs_t.text, model_name='marengo3.0')\n",
    ")\n",
    "\n",
    "# Insert data\n",
    "docs_t.insert([\n",
    "    {'text': 'Video understanding is a key area of AI research.'},\n",
    "    {'text': 'Audio analysis helps in transcription and content moderation.'},\n",
    "])\n",
    "\n",
    "# View the embeddings\n",
    "docs_t.select(docs_t.text, docs_t.embedding).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Available Models\n",
    "\n",
    "Twelve Labs provides several embedding models:\n",
    "\n",
    "| Model | Embedding Dimension | Description |\n",
    "|-------|-------------------|-------------|\n",
    "| `marengo3.0` | 512 | Latest multimodal embedding model |\n",
    "| `Marengo-retrieval-2.7` | 1024 | Retrieval-optimized model |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn More\n",
    "\n",
    "- [Twelve Labs Documentation](https://docs.twelvelabs.io/)\n",
    "- [Embed API Guide](https://docs.twelvelabs.io/v1.3/docs/guides/create-embeddings)\n",
    "- [Pixeltable Embedding Indexes](https://docs.pixeltable.com/platform/embedding-indexes)\n",
    "\n",
    "If you have any questions, don't hesitate to reach out."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
