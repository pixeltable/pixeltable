{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "983yjns496tx"
   },
   "source": [
    "# Working with Ollama in Pixeltable\n",
    "\n",
    "Ollama is a popular platform for local serving of LLMs. In this tutorial, we'll show how to integrate Ollama models into a Pixeltable workflow.\n",
    "\n",
    "## Install Ollama\n",
    "\n",
    "You'll need to have an Ollama server instance to query. There are several ways to do this.\n",
    "\n",
    "### Running on a local machine\n",
    "\n",
    "If you're running this notebook on your own machine, running Windows, Mac OS, or Linux, you can install Ollama at: https://ollama.com/download\n",
    "\n",
    "### Running on Google Colab\n",
    "\n",
    "- OR, if you're running on Colab, you can install Ollama by uncommenting and running the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To install Ollama on colab, uncomment and run the following\n",
    "# three lines (this will also work on a local Linux machine\n",
    "# if you don't already have Ollama installed).\n",
    "\n",
    "# !curl -fsSL https://ollama.com/install.sh | sh\n",
    "# import subprocess\n",
    "# ollama_process = subprocess.Popen(['ollama', 'serve'], stderr=subprocess.PIPE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running on a remote Ollama server\n",
    "\n",
    "- OR, if you have access to an Ollama server running remotely, you can uncomment and run the following line, replacing the default URL with the URL of your remote Ollama instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To run the notebook against an instance of Ollama running on a\n",
    "# remote server, uncomment the following line and specify the URL.\n",
    "\n",
    "# os.environs['OLLAMA_HOST'] = 'https://127.0.0.1:11434'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've completed the installation, run the following commands to verify that it's been successfully installed. This may result in an LLM being downloaded, so it may take some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "ollama.pull('qwen2.5:0.5b')\n",
    "ollama.generate('qwen2.5:0.5b', 'What is the capital of Missouri?')['response']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Pixeltable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's install Pixeltable and create a table for the demo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9pckrD01ik-e",
    "outputId": "060b8b32-48a6-48a0-e720-4eacf94d83ef"
   },
   "outputs": [],
   "source": [
    "%pip install -qU pixeltable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5ti10tXu5m3X",
    "outputId": "30848066-1e9b-4efd-aad7-b2271a031ec3"
   },
   "outputs": [],
   "source": [
    "import pixeltable as pxt\n",
    "from pixeltable.functions.ollama import chat\n",
    "\n",
    "pxt.drop_dir('ollama_demo', force=True)\n",
    "pxt.create_dir('ollama_demo')\n",
    "t = pxt.create_table('ollama_demo.chat', {'input': pxt.String})\n",
    "\n",
    "messages = [{'role': 'user', 'content': t.input}]\n",
    "\n",
    "t.add_computed_column(output=chat(\n",
    "    messages=messages,\n",
    "    model='qwen2.5:0.5b',\n",
    "    # These parameters are optional and can be used to tune model behavior:\n",
    "    options={'max_tokens': 300, 'top_p': 0.9, 'temperature': 0.5},\n",
    "))\n",
    "\n",
    "# Extract the response content into a separate column\n",
    "\n",
    "t.add_computed_column(response=t.output.message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can insert our input prompts into the table now. As always, Pixeltable automatically updates the computed columns by calling the relevant Ollama endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 599
    },
    "id": "IkMM7OYb5rQ_",
    "outputId": "8e94af3e-485c-49f2-d7ba-b5490ec83af9"
   },
   "outputs": [],
   "source": [
    "# Start a conversation\n",
    "t.insert(input='What are the most popular services for LLM inference?')\n",
    "t.select(t.input, t.response).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lTtQcjKQAlis"
   },
   "source": [
    "### Learn More\n",
    "\n",
    "To learn more about advanced techniques like RAG operations in Pixeltable, check out the [RAG Operations in Pixeltable](https://docs.pixeltable.com/howto/use-cases/rag-operations) tutorial.\n",
    "\n",
    "If you have any questions, don't hesitate to reach out."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
