{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transcribing and Indexing Audio and Video in Pixeltable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we'll build an end-to-end workflow for creating and indexing audio transcriptions of video data. We'll demonstrate how Pixeltable can be used to:\n",
    "\n",
    "1. Extract audio data from video files;\n",
    "1. Transcribe the audio using OpenAI Whisper;\n",
    "1. Build a semantic index of the transcriptions, using the Huggingface sentence_transformers models;\n",
    "1. Search this index.\n",
    "\n",
    "The tutorial assumes you're already somewhat familiar with Pixeltable. If this is your first time using Pixeltable, the [10-Minute Tour](https://docs.pixeltable.com/overview/ten-minute-tour) tutorial is a great place to start.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\"><!-- mdx:none -->\n",
    "<b>If you are running this tutorial in Colab:</b>\n",
    "In order to make the tutorial run a bit snappier, let's switch to a GPU-equipped instance for this Colab session. To do that, click on the <code>Runtime -> Change runtime type</code> menu item at the top, then select the <code>GPU</code> radio button and click on <code>Save</code>.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Table for Video Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first install the Python packages we'll need for the demo. We're going to use the popular Whisper library, running locally. Later in the demo, we'll see how to use the OpenAI API endpoints as an alternative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q pixeltable openai openai-whisper sentence-transformers spacy\n",
    "!python -m spacy download en_core_web_sm -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create a Pixeltable table to hold our videos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pixeltable as pxt\n",
    "\n",
    "pxt.drop_dir('transcription_demo', force=True)  # Ensure a clean slate for the demo\n",
    "pxt.create_dir('transcription_demo')\n",
    "\n",
    "# Create a table to store our videos and workflow\n",
    "video_table = pxt.create_table(\n",
    "    'transcription_demo.video_table',\n",
    "    {'video': pxt.Video}\n",
    ")\n",
    "\n",
    "video_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's insert some video files into the table. In this demo, we'll be using one-minute excerpts from a Lex Fridman podcast. We'll begin by inserting two of them into our new table. In this demo, our videos are given as `https` links, but Pixeltable also accepts local files and S3 URLs as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "videos = [\n",
    "    'https://github.com/pixeltable/pixeltable/raw/release/docs/resources/audio-transcription-demo/'\n",
    "    f'Lex-Fridman-Podcast-430-Excerpt-{n}.mp4'\n",
    "    for n in range(3)\n",
    "]\n",
    "\n",
    "video_table.insert({'video': video} for video in videos[:2])\n",
    "video_table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll add another column to hold extracted audio from our videos. The new column is an example of a _computed column_: it's updated automatically based on the contents of another column (or columns). In this case, the value of the `audio` column is defined to be the audio track extracted from whatever's in the `video` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixeltable.functions.video import extract_audio\n",
    "\n",
    "video_table.add_computed_column(\n",
    "    audio=extract_audio(video_table.video, format='mp3')\n",
    ")\n",
    "video_table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we look at the structure of the video table, we see that the new column is a computed column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add another computed column to extract metadata from the audio streams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixeltable.functions.audio import get_metadata\n",
    "\n",
    "video_table.add_computed_column(\n",
    "    metadata=get_metadata(video_table.audio)\n",
    ")\n",
    "video_table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Transcriptions\n",
    "\n",
    "Now we'll add a step to create transcriptions of our videos. As mentioned above, we're going to use the Whisper library for this, running locally. Pixeltable has a built-in function, `whisper.transcribe`, that serves as an adapter for the Whisper library's transcription capability. All we have to do is add a computed column that calls this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixeltable.functions import whisper\n",
    "\n",
    "video_table.add_computed_column(\n",
    "    transcription=whisper.transcribe(\n",
    "        audio=video_table.audio,\n",
    "        model='base.en'\n",
    "    )\n",
    ")\n",
    "\n",
    "video_table.select(\n",
    "    video_table.video,\n",
    "    video_table.transcription.text\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to index the transcriptions, we'll first need to split them into sentences. We can do this using Pixeltable's built-in `string_splitter` iterator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixeltable.functions.string import string_splitter\n",
    "\n",
    "sentences_view = pxt.create_view(\n",
    "    'transcription_demo.sentences_view',\n",
    "    video_table,\n",
    "    iterator=string_splitter(\n",
    "        video_table.transcription.text,\n",
    "        separators='sentence'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `string_splitter` creates a new view, with the audio transcriptions broken into individual, one-sentence chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sentences_view.select(\n",
    "    sentences_view.pos,\n",
    "    sentences_view.text\n",
    ").show(8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add an Embedding Index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's use the Huggingface `sentence_transformers` library to create an embedding index of our sentences, attaching it to the `text` column of our `sentences_view`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixeltable.functions.huggingface import sentence_transformer\n",
    "\n",
    "sentences_view.add_embedding_index(\n",
    "    'text',\n",
    "    embedding=sentence_transformer.using(model_id='intfloat/e5-large-v2')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can do a simple lookup to test our new index. The following snippet returns the results of a nearest-neighbor search on the input \"What is happiness?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sim = sentences_view.text.similarity(string='What is happiness?')\n",
    "\n",
    "(\n",
    "    sentences_view\n",
    "    .order_by(sim, asc=False)\n",
    "    .limit(10)\n",
    "    .select(sentences_view.text,similarity=sim)\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental Updates\n",
    "\n",
    "_Incremental updates_ are a key feature of Pixeltable. Whenever a new video is added to the original table, all of its downstream computed columns are updated automatically. Let's demonstrate this by adding a third video to the table and seeing how the updates propagate through to the index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_table.insert([{'video': videos[2]}])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "video_table.select(\n",
    "    video_table.video,\n",
    "    video_table.metadata,\n",
    "    video_table.transcription.text\n",
    ").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sim = sentences_view.text.similarity(string='What is happiness?')\n",
    "\n",
    "(\n",
    "    sentences_view\n",
    "    .order_by(sim, asc=False)\n",
    "    .limit(20)\n",
    "    .select(sentences_view.text, similarity=sim)\n",
    "    .collect()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the new results showing up in `sentences_view`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the OpenAI API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes our tutorial using the locally installed Whisper library. Sometimes, it may be preferable to use the OpenAI API rather than a locally installed library. In this section we'll show how this can be done in Pixeltable, simply by using a different function to construct our computed columns.\n",
    "\n",
    "Since this section relies on calling out to the OpenAI API, you'll need to have an API key, which you can enter below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "if 'OPENAI_API_KEY' not in os.environ:\n",
    "    os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pixeltable.functions import openai\n",
    "\n",
    "video_table.add_computed_column(\n",
    "    transcription_from_api=openai.transcriptions(\n",
    "        video_table.audio,\n",
    "        model='whisper-1'\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's compare the results from the local model and the API side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "video_table.select(\n",
    "    video_table.video,\n",
    "    video_table.transcription.text,\n",
    "    video_table.transcription_from_api.text\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "They look pretty similar, which isn't surprising, since the OpenAI transcriptions endpoint runs on Whisper.\n",
    "\n",
    "One difference is that the local library spits out a lot more information about the internal behavior of the model. Note that we've been selecting `video_table.transcription.text` in the preceding queries, which pulls out just the `text` field of the transcription results. The actual results are a sizable JSON structure that includes a lot of metadata. To see the full output, we can select `video_table.transcription` instead, to get the full JSON struct. Here's what it looks like (we'll select just one row, since it's a lot of output):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_table.select(\n",
    "    video_table.transcription,\n",
    "    video_table.transcription_from_api\n",
    ").show(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
