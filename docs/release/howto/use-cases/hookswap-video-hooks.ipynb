{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Hookswap: AI-Generated Video Hooks\n",
        "\n",
        "Generate scroll-stopping video hook introductions using AI. This workflow creates multiple hook variants tailored to specific industries and states, complete with AI-generated visuals, voice narration, and captions."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem\n",
        "\n",
        "You need to create engaging video hooks that capture attention in the first few seconds. Each hook should be tailored to specific audiences.\n",
        "\n",
        "| Use case | Input | Output |\n",
        "|----------|-------|--------|\n",
        "| Marketing videos | Original video + industry | Multiple hook variants |\n",
        "| Regional targeting | Video + target states | State-specific hooks |\n",
        "| Social media | Long-form content | Scroll-stopping intros |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solution\n",
        "\n",
        "**What's in this recipe:**\n",
        "\n",
        "1. **Transcribe** the original video for context\n",
        "2. **Generate hooks** tailored to industry and location\n",
        "3. **Create visuals** matching each hook's psychology\n",
        "4. **Add narration** with AI text-to-speech\n",
        "5. **Assemble** the final videos with captions\n",
        "6. **Export** in multiple aspect ratios (9:16, 1:1, 16:9)\n",
        "\n",
        "All steps are declarative computed columns—add new videos and hooks generate automatically."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -qU pixeltable openai fal-client httpx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "# OpenAI for GPT-4o, DALL-E, Whisper, and TTS\n",
        "if 'OPENAI_API_KEY' not in os.environ:\n",
        "    os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key: ')\n",
        "\n",
        "# fal.ai for LumaAI image-to-video (optional but recommended)\n",
        "if 'FAL_API_KEY' not in os.environ:\n",
        "    os.environ['FAL_API_KEY'] = getpass.getpass('fal.ai API Key (for LumaAI): ')\n",
        "\n",
        "# ElevenLabs for high-quality TTS (optional, falls back to OpenAI)\n",
        "if 'ELEVENLABS_API_KEY' not in os.environ:\n",
        "    key = getpass.getpass('ElevenLabs API Key (press Enter to skip): ')\n",
        "    if key:\n",
        "        os.environ['ELEVENLABS_API_KEY'] = key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pixeltable as pxt\n",
        "from pixeltable.functions import openai, fal\n",
        "from pixeltable.functions.video import with_audio, concat_videos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create fresh directory for our hookswap demo\n",
        "pxt.drop_dir('hookswap', force=True)\n",
        "pxt.create_dir('hookswap')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Custom UDFs\n",
        "\n",
        "We'll create a few custom functions for ElevenLabs TTS, LumaAI video generation, and aspect ratio transformations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### ElevenLabs Text-to-Speech"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import httpx\n",
        "from pixeltable.utils.local_store import TempStore\n",
        "\n",
        "@pxt.udf\n",
        "async def elevenlabs_tts(\n",
        "    text: str,\n",
        "    voice_id: str = 'EXAVITQu4vr4xnSDxMaL',  # \"Sarah\" - default warm female voice\n",
        "    model_id: str = 'eleven_multilingual_v2'\n",
        ") -> pxt.Audio:\n",
        "    \"\"\"\n",
        "    Generate speech using ElevenLabs API.\n",
        "    \n",
        "    Args:\n",
        "        text: Text to synthesize\n",
        "        voice_id: ElevenLabs voice ID (default: Sarah)\n",
        "        model_id: Model to use for synthesis\n",
        "    \n",
        "    Returns:\n",
        "        Audio file path\n",
        "    \"\"\"\n",
        "    api_key = os.environ.get('ELEVENLABS_API_KEY')\n",
        "    if not api_key:\n",
        "        raise ValueError('ELEVENLABS_API_KEY not set')\n",
        "    \n",
        "    url = f'https://api.elevenlabs.io/v1/text-to-speech/{voice_id}'\n",
        "    headers = {\n",
        "        'Accept': 'audio/mpeg',\n",
        "        'Content-Type': 'application/json',\n",
        "        'xi-api-key': api_key\n",
        "    }\n",
        "    data = {\n",
        "        'text': text,\n",
        "        'model_id': model_id,\n",
        "        'voice_settings': {\n",
        "            'stability': 0.5,\n",
        "            'similarity_boost': 0.75\n",
        "        }\n",
        "    }\n",
        "    \n",
        "    async with httpx.AsyncClient() as client:\n",
        "        response = await client.post(url, json=data, headers=headers, timeout=60.0)\n",
        "        response.raise_for_status()\n",
        "        \n",
        "        output_path = TempStore.create_path(extension='.mp3')\n",
        "        with open(output_path, 'wb') as f:\n",
        "            f.write(response.content)\n",
        "        \n",
        "        return str(output_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Voice Selection Based on Hook Tone"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ElevenLabs voice mapping by tone\n",
        "VOICE_MAP = {\n",
        "    'authoritative_male': '29vD33N1CtxCmqQRPOHJ',    # Drew\n",
        "    'authoritative_female': 'EXAVITQu4vr4xnSDxMaL',  # Sarah\n",
        "    'warm_male': 'ErXwobaYiN019PkySvjV',             # Antoni\n",
        "    'warm_female': '21m00Tcm4TlvDq8ikWAM',           # Rachel\n",
        "    'urgent_male': 'VR6AewLTigWG4xSOukaG',           # Arnold\n",
        "    'urgent_female': 'ThT5KcBeYPX3keUQqHPh',         # Dorothy\n",
        "    'friendly_male': 'TxGEqnHWrfWFTfGW9XjX',         # Josh\n",
        "    'friendly_female': 'MF3mGyEYCl7XYWbV9V6O',       # Emily\n",
        "}\n",
        "\n",
        "@pxt.udf\n",
        "def get_voice_id(tone: str, gender: str = 'female') -> str:\n",
        "    \"\"\"\n",
        "    Get ElevenLabs voice ID based on tone and gender.\n",
        "    \n",
        "    Args:\n",
        "        tone: One of 'authoritative', 'warm', 'urgent', 'friendly'\n",
        "        gender: 'male' or 'female'\n",
        "    \n",
        "    Returns:\n",
        "        ElevenLabs voice ID\n",
        "    \"\"\"\n",
        "    key = f\"{tone}_{gender}\"\n",
        "    return VOICE_MAP.get(key, VOICE_MAP['warm_female'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### LumaAI Image-to-Video via fal.ai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import PIL.Image\n",
        "import base64\n",
        "import io\n",
        "\n",
        "@pxt.udf\n",
        "def image_to_data_url(image: PIL.Image.Image) -> str:\n",
        "    \"\"\"Convert PIL Image to base64 data URL for API calls.\"\"\"\n",
        "    buffer = io.BytesIO()\n",
        "    image.save(buffer, format='PNG')\n",
        "    b64 = base64.b64encode(buffer.getvalue()).decode('utf-8')\n",
        "    return f'data:image/png;base64,{b64}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@pxt.udf\n",
        "def extract_video_url(fal_response: dict) -> str:\n",
        "    \"\"\"Extract video URL from fal.ai LumaAI response.\"\"\"\n",
        "    if fal_response and 'video' in fal_response:\n",
        "        return fal_response['video']['url']\n",
        "    return None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Aspect Ratio Transformations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import subprocess\n",
        "import pathlib\n",
        "from pixeltable.utils.local_store import TempStore\n",
        "\n",
        "@pxt.udf\n",
        "def add_blur_padding(\n",
        "    video: pxt.Video,\n",
        "    target_ratio: str = '1:1'\n",
        ") -> pxt.Video:\n",
        "    \"\"\"\n",
        "    Add blurred padding to video for different aspect ratios.\n",
        "    \n",
        "    Args:\n",
        "        video: Input video (assumed 9:16 vertical)\n",
        "        target_ratio: '1:1' for square or '16:9' for landscape\n",
        "    \n",
        "    Returns:\n",
        "        Video with blur-padded background\n",
        "    \"\"\"\n",
        "    output_path = str(TempStore.create_path(extension='.mp4'))\n",
        "    \n",
        "    if target_ratio == '1:1':\n",
        "        # Square: 1080x1080\n",
        "        filter_complex = (\n",
        "            \"[0:v]split=2[blur][fg];\"\n",
        "            \"[blur]scale=1080:1080:force_original_aspect_ratio=increase,\"\n",
        "            \"crop=1080:1080,boxblur=20:20[bg];\"\n",
        "            \"[fg]scale=-1:1080:force_original_aspect_ratio=decrease[scaled];\"\n",
        "            \"[bg][scaled]overlay=(W-w)/2:(H-h)/2\"\n",
        "        )\n",
        "    elif target_ratio == '16:9':\n",
        "        # Landscape: 1920x1080\n",
        "        filter_complex = (\n",
        "            \"[0:v]split=2[blur][fg];\"\n",
        "            \"[blur]scale=1920:1080:force_original_aspect_ratio=increase,\"\n",
        "            \"crop=1920:1080,boxblur=20:20[bg];\"\n",
        "            \"[fg]scale=-1:1080:force_original_aspect_ratio=decrease[scaled];\"\n",
        "            \"[bg][scaled]overlay=(W-w)/2:(H-h)/2\"\n",
        "        )\n",
        "    else:\n",
        "        raise ValueError(f\"Unsupported ratio: {target_ratio}\")\n",
        "    \n",
        "    cmd = [\n",
        "        'ffmpeg', '-y',\n",
        "        '-i', str(video),\n",
        "        '-filter_complex', filter_complex,\n",
        "        '-c:v', 'libx264',\n",
        "        '-crf', '23',\n",
        "        '-c:a', 'aac',\n",
        "        '-loglevel', 'error',\n",
        "        output_path\n",
        "    ]\n",
        "    \n",
        "    subprocess.run(cmd, check=True, capture_output=True)\n",
        "    return output_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Create the Video Pipeline\n",
        "\n",
        "### Step 1: Main Videos Table"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create the main videos table\n",
        "videos = pxt.create_table(\n",
        "    'hookswap.videos',\n",
        "    {\n",
        "        'video': pxt.Video,\n",
        "        'industry': pxt.String,      # e.g., 'insurance', 'fitness', 'real_estate'\n",
        "        'target_state': pxt.String,  # e.g., 'California', 'Texas', 'Florida'\n",
        "    }\n",
        ")\n",
        "\n",
        "print(\"Created videos table\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 2: Extract Audio and Transcribe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract audio from video\n",
        "videos.add_computed_column(\n",
        "    audio=videos.video.extract_audio(format='mp3')\n",
        ")\n",
        "\n",
        "# Transcribe with word-level timestamps\n",
        "videos.add_computed_column(\n",
        "    transcription=openai.transcriptions(\n",
        "        videos.audio,\n",
        "        model='whisper-1',\n",
        "        model_kwargs={\n",
        "            'response_format': 'verbose_json',\n",
        "            'timestamp_granularities': ['word']\n",
        "        }\n",
        "    )\n",
        ")\n",
        "\n",
        "# Extract just the text for context\n",
        "videos.add_computed_column(\n",
        "    transcript_text=videos.transcription['text']\n",
        ")\n",
        "\n",
        "print(\"Added transcription columns\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 3: Generate Hook Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Build the hook generation prompt\n",
        "@pxt.udf\n",
        "def build_hook_prompt(industry: str, state: str, transcript: str) -> list:\n",
        "    \"\"\"\n",
        "    Build the prompt for GPT-4o to generate a hook.\n",
        "    \"\"\"\n",
        "    # State-specific context\n",
        "    state_context = {\n",
        "        'California': 'wildfires, Prop 13 property tax, earthquake risk, high cost of living',\n",
        "        'Texas': 'hailstorms, homestead exemption, no state income tax, flooding',\n",
        "        'Florida': 'hurricanes, flood zones, rising insurance costs, sinkholes',\n",
        "        'New York': 'high property taxes, rent control, winter storms, aging infrastructure',\n",
        "        'Arizona': 'extreme heat, dust storms, water scarcity, rapid growth',\n",
        "    }\n",
        "    \n",
        "    context = state_context.get(state, 'local regulations and regional challenges')\n",
        "    \n",
        "    system_prompt = f\"\"\"You are an expert at writing scroll-stopping video hooks for {industry} content.\n",
        "Your hooks should:\n",
        "- Be 2-3 sentences max (under 15 seconds when spoken)\n",
        "- Create immediate urgency or curiosity\n",
        "- Reference specific local context when relevant\n",
        "- Use psychology: fear of missing out, social proof, or problem-agitation\n",
        "\n",
        "For {state}, consider: {context}\"\"\"\n",
        "\n",
        "    user_prompt = f\"\"\"Write a compelling video hook for this {industry} video targeting {state} audiences.\n",
        "\n",
        "Original video transcript (for context):\n",
        "{transcript[:500]}...\n",
        "\n",
        "Return ONLY the hook text, nothing else.\"\"\"\n",
        "\n",
        "    return [\n",
        "        {'role': 'system', 'content': system_prompt},\n",
        "        {'role': 'user', 'content': user_prompt}\n",
        "    ]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate the hook text\n",
        "videos.add_computed_column(\n",
        "    hook_messages=build_hook_prompt(videos.industry, videos.target_state, videos.transcript_text)\n",
        ")\n",
        "\n",
        "videos.add_computed_column(\n",
        "    hook_response=openai.chat_completions(\n",
        "        videos.hook_messages,\n",
        "        model='gpt-4o'\n",
        "    )\n",
        ")\n",
        "\n",
        "videos.add_computed_column(\n",
        "    hook_text=videos.hook_response['choices'][0]['message']['content']\n",
        ")\n",
        "\n",
        "print(\"Added hook generation columns\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 4: Analyze Hook Tone and Select Voice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@pxt.udf\n",
        "def build_tone_prompt(hook_text: str) -> list:\n",
        "    \"\"\"Build prompt to analyze hook tone.\"\"\"\n",
        "    return [\n",
        "        {\n",
        "            'role': 'system',\n",
        "            'content': 'Analyze the emotional tone of the given text. Return ONLY one word: authoritative, warm, urgent, or friendly.'\n",
        "        },\n",
        "        {\n",
        "            'role': 'user',\n",
        "            'content': hook_text\n",
        "        }\n",
        "    ]\n",
        "\n",
        "videos.add_computed_column(\n",
        "    tone_messages=build_tone_prompt(videos.hook_text)\n",
        ")\n",
        "\n",
        "videos.add_computed_column(\n",
        "    tone_response=openai.chat_completions(\n",
        "        videos.tone_messages,\n",
        "        model='gpt-4o-mini'\n",
        "    )\n",
        ")\n",
        "\n",
        "videos.add_computed_column(\n",
        "    hook_tone=videos.tone_response['choices'][0]['message']['content'].apply(str.lower).apply(str.strip)\n",
        ")\n",
        "\n",
        "videos.add_computed_column(\n",
        "    voice_id=get_voice_id(videos.hook_tone, 'female')\n",
        ")\n",
        "\n",
        "print(\"Added tone analysis and voice selection\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 5: Generate Visual Concept and Image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "@pxt.udf\n",
        "def build_visual_prompt(hook_text: str, industry: str, state: str) -> list:\n",
        "    \"\"\"Build prompt for visual concept generation.\"\"\"\n",
        "    return [\n",
        "        {\n",
        "            'role': 'system',\n",
        "            'content': f\"\"\"You are a visual director creating imagery for {industry} video content.\n",
        "Generate a concise 30-50 word visual description for a vertical video frame.\n",
        "Include: main subject, environment, lighting, mood, any state-specific elements for {state}.\n",
        "The image should evoke the emotion of the hook without containing text.\"\"\"\n",
        "        },\n",
        "        {\n",
        "            'role': 'user',\n",
        "            'content': f'Create a visual concept for this hook: \"{hook_text}\"'\n",
        "        }\n",
        "    ]\n",
        "\n",
        "videos.add_computed_column(\n",
        "    visual_messages=build_visual_prompt(videos.hook_text, videos.industry, videos.target_state)\n",
        ")\n",
        "\n",
        "videos.add_computed_column(\n",
        "    visual_response=openai.chat_completions(\n",
        "        videos.visual_messages,\n",
        "        model='gpt-4o'\n",
        "    )\n",
        ")\n",
        "\n",
        "videos.add_computed_column(\n",
        "    visual_concept=videos.visual_response['choices'][0]['message']['content']\n",
        ")\n",
        "\n",
        "print(\"Added visual concept generation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate the hook image with DALL-E\n",
        "videos.add_computed_column(\n",
        "    hook_image=openai.image_generations(\n",
        "        videos.visual_concept,\n",
        "        model='dall-e-3',\n",
        "        model_kwargs={\n",
        "            'size': '1024x1792',  # Vertical format\n",
        "            'quality': 'standard'\n",
        "        }\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"Added image generation\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 6: Convert Image to Video with LumaAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Convert image to data URL for fal.ai\n",
        "videos.add_computed_column(\n",
        "    hook_image_url=image_to_data_url(videos.hook_image)\n",
        ")\n",
        "\n",
        "# Generate video from image using LumaAI via fal.ai\n",
        "videos.add_computed_column(\n",
        "    luma_response=fal.run(\n",
        "        input={\n",
        "            'prompt': 'Subtle cinematic motion, gentle camera movement, professional video quality',\n",
        "            'image_url': videos.hook_image_url,\n",
        "            'aspect_ratio': '9:16',\n",
        "            'loop': False\n",
        "        },\n",
        "        app='fal-ai/luma-dream-machine'\n",
        "    )\n",
        ")\n",
        "\n",
        "# Extract the video URL and convert to Video type\n",
        "videos.add_computed_column(\n",
        "    hook_video_url=extract_video_url(videos.luma_response)\n",
        ")\n",
        "\n",
        "videos.add_computed_column(\n",
        "    hook_video=videos.hook_video_url.astype(pxt.Video)\n",
        ")\n",
        "\n",
        "print(\"Added image-to-video conversion\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 7: Generate Text-to-Speech"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Use ElevenLabs if available, otherwise fall back to OpenAI\n",
        "use_elevenlabs = 'ELEVENLABS_API_KEY' in os.environ and os.environ['ELEVENLABS_API_KEY']\n",
        "\n",
        "if use_elevenlabs:\n",
        "    videos.add_computed_column(\n",
        "        hook_audio=elevenlabs_tts(videos.hook_text, videos.voice_id)\n",
        "    )\n",
        "    print(\"Using ElevenLabs for TTS\")\n",
        "else:\n",
        "    videos.add_computed_column(\n",
        "        hook_audio=openai.speech(\n",
        "            videos.hook_text,\n",
        "            model='tts-1',\n",
        "            voice='nova'  # Warm, engaging voice\n",
        "        )\n",
        "    )\n",
        "    print(\"Using OpenAI for TTS (ElevenLabs not configured)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 8: Assemble Hook Video with Audio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Combine hook video with TTS audio\n",
        "videos.add_computed_column(\n",
        "    assembled_hook=with_audio(\n",
        "        videos.hook_video,\n",
        "        videos.hook_audio\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"Added audio assembly\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 9: Add Captions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add text overlay as captions (simplified - full SRT would need more processing)\n",
        "videos.add_computed_column(\n",
        "    captioned_hook=videos.assembled_hook.overlay_text(\n",
        "        videos.hook_text,\n",
        "        font_size=36,\n",
        "        color='white',\n",
        "        box=True,\n",
        "        box_color='black',\n",
        "        box_opacity=0.7,\n",
        "        box_border=[8, 16],\n",
        "        vertical_align='bottom',\n",
        "        vertical_margin=100\n",
        "    )\n",
        ")\n",
        "\n",
        "print(\"Added captions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 10: Stitch Hook to Original Video"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Concatenate hook with original video\n",
        "videos.add_computed_column(\n",
        "    final_video=concat_videos([videos.captioned_hook, videos.video])\n",
        ")\n",
        "\n",
        "print(\"Added final video stitching\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Step 11: Generate Aspect Ratio Variants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Square version (1:1) for Instagram feed\n",
        "videos.add_computed_column(\n",
        "    final_video_square=add_blur_padding(videos.final_video, '1:1')\n",
        ")\n",
        "\n",
        "# Landscape version (16:9) for YouTube\n",
        "videos.add_computed_column(\n",
        "    final_video_landscape=add_blur_padding(videos.final_video, '16:9')\n",
        ")\n",
        "\n",
        "print(\"Added aspect ratio variants\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Demo: Process a Sample Video\n",
        "\n",
        "Let's insert a sample video and watch the pipeline in action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Insert a sample video\n",
        "# Using a short sample video URL - replace with your own video\n",
        "sample_video_url = 'https://github.com/pixeltable/pixeltable/raw/release/docs/resources/bangkok.mp4'\n",
        "\n",
        "videos.insert([\n",
        "    {\n",
        "        'video': sample_video_url,\n",
        "        'industry': 'travel',\n",
        "        'target_state': 'California'\n",
        "    }\n",
        "])\n",
        "\n",
        "print(\"Inserted sample video - pipeline is now processing...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check the generated hook text\n",
        "videos.select(\n",
        "    videos.industry,\n",
        "    videos.target_state,\n",
        "    videos.hook_text,\n",
        "    videos.hook_tone,\n",
        "    videos.visual_concept\n",
        ").head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View the generated hook image\n",
        "videos.select(\n",
        "    videos.hook_text,\n",
        "    videos.hook_image\n",
        ").head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View the final videos\n",
        "videos.select(\n",
        "    videos.industry,\n",
        "    videos.target_state,\n",
        "    videos.final_video,\n",
        "    videos.final_video_square,\n",
        "    videos.final_video_landscape\n",
        ").head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Multiple Hooks\n",
        "\n",
        "Add the same video with different target states to generate multiple hook variants."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate hooks for multiple states\n",
        "videos.insert([\n",
        "    {'video': sample_video_url, 'industry': 'travel', 'target_state': 'Texas'},\n",
        "    {'video': sample_video_url, 'industry': 'travel', 'target_state': 'Florida'},\n",
        "])\n",
        "\n",
        "print(\"Added 2 more state variants - processing...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare all hook variants\n",
        "videos.select(\n",
        "    videos.target_state,\n",
        "    videos.hook_text,\n",
        "    videos.hook_tone\n",
        ").collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook demonstrated a complete AI video hook generation pipeline:\n",
        "\n",
        "1. **Transcription**: Extract audio and transcribe with word-level timestamps\n",
        "2. **Hook Generation**: GPT-4o creates state/industry-specific hooks\n",
        "3. **Voice Selection**: Analyze tone and select appropriate voice\n",
        "4. **Visual Creation**: Generate concept and image with DALL-E\n",
        "5. **Video Synthesis**: Convert image to video with LumaAI\n",
        "6. **Audio Synthesis**: Generate narration with ElevenLabs/OpenAI TTS\n",
        "7. **Assembly**: Combine video, audio, and captions\n",
        "8. **Final Output**: Stitch to original and create aspect ratio variants\n",
        "\n",
        "All steps are **declarative computed columns**—add new videos and the entire pipeline runs automatically!\n",
        "\n",
        "### Next Steps\n",
        "\n",
        "- Add more industries and state contexts\n",
        "- Implement A/B testing by generating multiple hook styles\n",
        "- Add embedding-based similarity search to find best-performing hooks\n",
        "- Connect to a frontend for batch processing"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
