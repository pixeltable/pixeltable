---
title: "Production Operations"
description: "Concurrency, error handling, schema evolution, and deployment patterns"
---

## Concurrent Access & Scaling

| Aspect | Details |
|--------|---------|
| **Thread Safety** | Each thread gets its own database connection and transaction context automatically |
| **Locking** | Automatic table-level locking for schema changes |
| **Isolation** | PostgreSQL `SERIALIZABLE` isolation prevents data race conditions |
| **Retries** | Built-in retry logic handles transient serialization failures |

| Scaling Dimension | Current Approach | Limitation |
|-------------------|------------------|------------|
| **Metadata Storage** | Single embedded PostgreSQL instance | Vertical scaling (larger EC2/VM) |
| **Compute** | Multiple API workers connected to same instance | Shared access to storage volume required |
| **High Availability** | Single attached storage volume | Failover requires volume detach/reattach |

<Info>
Multi-node HA and horizontal scaling planned for Pixeltable Cloud (2026).
</Info>

## Web Framework Concurrency

Pixeltable is thread-safe and works with FastAPI, Flask, Django, and other web frameworks out of the box. The key rule: **use sync (`def`) endpoint handlers**, not `async def`.

### Why Sync Endpoints

FastAPI (and Starlette) dispatches sync (`def`) handlers to a thread pool. Each concurrent request gets its own thread, and Pixeltable automatically creates an isolated database connection per thread. This gives you true parallel request handling with no extra configuration.

```python
from pydantic import BaseModel
from fastapi import FastAPI
import pixeltable as pxt

app = FastAPI()

class SearchResult(BaseModel):
    text: str
    score: float

@app.post("/ingest")
def ingest(text: str):
    t = pxt.get_table('myapp/documents')
    status = t.insert([{'text': text}])
    return {'inserted': status.num_rows}

@app.get("/search")
def search(query: str, limit: int = 10) -> list[SearchResult]:
    t = pxt.get_table('myapp/documents')
    sim = t.text.similarity(string=query)
    results = (
        t.order_by(sim, asc=False)
        .limit(limit)
        .select(t.text, score=sim)
        .collect()
    )
    return list(results.to_pydantic(SearchResult))
```

<Warning>
**Do not use `async def` for endpoints that call Pixeltable.** Pixeltable's API is synchronous. Inside an `async def` handler, Pixeltable calls block the event loop, serializing all requests and starving other coroutines. With `def` handlers, FastAPI's thread pool handles concurrency for you.
</Warning>

### Returning Query Results

`table.select(...).collect()` returns a `ResultSet` object, which Pydantic cannot serialize directly. You have two options:

**Option 1: `to_pydantic()` (recommended for FastAPI)**

Define a Pydantic model and let Pixeltable validate and convert each row. FastAPI serializes these natively.

```python
class Item(BaseModel):
    name: str
    score: float

@app.get("/rows")
def get_rows() -> list[Item]:
    t = pxt.get_table('myapp/items')
    return list(t.select(t.name, t.score).collect().to_pydantic(Item))
```

**Option 2: `to_pandas()` + `to_dict()`**

Convert via pandas when you don't need a Pydantic model.

```python
@app.get("/rows")
def get_rows():
    t = pxt.get_table('myapp/items')
    df = t.select(t.name, t.score).collect().to_pandas()
    return {'rows': df.to_dict(orient='records')}
```

### uvloop Compatibility

Pixeltable is compatible with [uvloop](https://github.com/MagicStack/uvloop), the high-performance event loop used by default in many production deployments. No special configuration is needed ‚Äî sync endpoints work identically whether the server uses the default asyncio loop or uvloop.

```bash
# uvicorn with uvloop (the default when uvloop is installed)
uvicorn app:app --host 0.0.0.0 --port 8000 --workers 1
```

## GPU Acceleration

- **Automatic GPU Detection:** Pixeltable uses CUDA GPUs for local models (Hugging Face, Ollama) when available.
- **CPU Fallback:** Models run on CPU if no GPU detected (functional but slower).
- **Configuration:** Control via `CUDA_VISIBLE_DEVICES` environment variable.

## Error Handling

| Error Type | Mode | Behavior |
|------------|------|----------|
| **Computed Column Errors** | `on_error='abort'` (default) | Fails entire operation if any row errors |
| | `on_error='ignore'` | Continues processing; stores `None` with error metadata |
| **Media Validation** | `media_validation='on_write'` (default) | Validates media during insert (catches errors early) |
| | `media_validation='on_read'` | Defers validation until media accessed (faster inserts) |

Access error details via `table.column.errortype` and `table.column.errormsg`.

```python
# Example: Graceful error handling in production
table.add_computed_column(
    analysis=llm_analyze(table.document),
    on_error='ignore'  # Continue processing despite individual failures
)

# Query for errors
errors = table.where(table.analysis.errortype != None).collect()
```

## Testing Transformations Before Deployment

When you add a computed column, Pixeltable executes it immediately for all existing rows. For expensive operations (LLM calls, model inference), validate your logic on a sample first using `select()`; nothing is stored until you commit with `add_computed_column()`.

```python
# 1. Test transformation on sample rows (nothing stored)
table.select(
    table.text,
    summary=summarize_with_llm(table.text)
).head(3)  # Only processes 3 rows

# 2. Once satisfied, persist to table (processes all rows)
table.add_computed_column(summary=summarize_with_llm(table.text))
```

This "iterate-then-add" workflow lets you catch errors early without wasting API calls or compute on your full dataset.

<Tip>
**Pro tip:** Save expressions as variables to guarantee identical logic in both steps:
```python
summary_expr = summarize_with_llm(table.text)
table.select(table.text, summary=summary_expr).head(3)  # Test
table.add_computed_column(summary=summary_expr)          # Commit
```
</Tip>

<Card title="Full Tutorial" icon="flask" href="/howto/cookbooks/core/dev-iterative-workflow">
  Step-by-step guide with examples for built-in functions, expressions, and custom UDFs
</Card>

## Schema Evolution

| Operation Type | Examples | Impact |
|----------------|----------|--------|
| **Safe** | Add columns, Add computed columns, Add indexes | Incremental computation only |
| **Destructive** | Modify computed columns (`if_exists='replace'`), Drop columns/tables/views | Full recomputation or data loss |

**Production Safety:**
```python
# Use if_exists='ignore' for idempotent schema migrations
import pixeltable as pxt
import config

docs_table = pxt.get_table(f'{config.APP_NAMESPACE}/documents')
docs_table.add_computed_column(
    embedding=embed_model(docs_table.document),
    if_exists='ignore'  # No-op if column exists
)
```
- Version control `setup_pixeltable.py` like database migration scripts.
- Rollback via `table.revert()` (single operation) or Git revert (complex changes).

## Deployment Patterns

**Web Applications:**
- Execute `setup_pixeltable.py` during deployment initialization
- Web server processes connect to Pixeltable instance
- Pixeltable uses connection pooling internally
- Use sync (`def`) endpoint handlers for concurrent request support

<Card title="App Template" icon="github" href="https://github.com/pixeltable/pixeltable-app-template">
  Clone a working FastAPI + React app with multimodal upload, search, and agent endpoints already configured.
</Card>

**Batch Processing:**
- Schedule via `cron`, Airflow, AWS EventBridge, GCP Cloud Scheduler
- Isolate batch workloads from real-time serving (separate containers/instances)
- Use Pixeltable's incremental computation to process only new data

**Containers:**
- Docker provides reproducible builds across environments
- **Full Backend:** Mount persistent volume at `~/.pixeltable`
- **Kubernetes:** Use `ReadWriteOnce` PVC (single-pod write access)
- Docker Compose or Kubernetes for multi-container deployments

```dockerfile
# Dockerfile for Pixeltable application
FROM python:3.11-slim

WORKDIR /app
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

COPY . .

# Initialize schema and start application
CMD python setup_pixeltable.py && uvicorn app:app --host 0.0.0.0
```

## Environment Management

### Multi-Tenancy and Isolation

| Isolation Type | Implementation | Use Case | Overhead |
|----------------|----------------|----------|----------|
| **Logical** | Single Pixeltable instance with directory namespaces (`pxt.create_dir(f"user_{user_id}")`) | Dev/staging environments, simple multi-user apps | Low |
| **Physical** | Separate container instances per tenant | SaaS with strict data isolation | High |

**Logical Isolation Example:**
```python
# Per-user isolation via namespaces
pxt.create_dir(f"user_{user_id}", if_exists='ignore')
user_table = pxt.create_table(f"user_{user_id}/chat_history", schema={...})
```

### High Availability Constraints

| Configuration | Status | Details |
|---------------|--------|---------|
| **Single Pod + ReadWriteOnce PVC** | ‚úÖ Supported | One active pod writes to dedicated volume. Failover requires volume detach/reattach. |
| **Multiple Pods + Shared Volume (NFS/EFS)** | ‚ùå Not Supported | **Will cause database corruption.** Do not mount same `pgdata` to multiple pods. |
| **Multi-Node HA** | üîú Coming 2026 | Available in Pixeltable Cloud (serverless scaling, API endpoints). [Join waitlist](https://www.pixeltable.com/waitlist) |

<Warning>
**Single-Writer Limitation:** Pixeltable's storage layer uses an embedded PostgreSQL instance. **Only one process can write to `~/.pixeltable/pgdata` at a time**.
</Warning>

## Troubleshooting

### Reset Database (Development Only)

To completely reset Pixeltable's local state during development:

```bash
# Stop all Pixeltable processes first, then:
rm -rf ~/.pixeltable/pgdata ~/.pixeltable/media ~/.pixeltable/file_cache
```

<Warning>
**This deletes all data.** Only use in development. For production, use backups and `table.revert()` or snapshots instead.
</Warning>

### Common Issues

| Symptom | Cause | Solution |
|---------|-------|----------|
| "Cannot connect to database" | Stale lock file | Remove `~/.pixeltable/pgdata/postmaster.pid` if no process is running |
| Slow first query | File cache miss | Files download on first access; subsequent queries are fast |
| "Table not found" | Wrong namespace | Check `pxt.list_tables()` and verify `config.APP_NAMESPACE` |
| OOM on large media | Full file loaded to memory | Use iterators (`FrameIterator`, `DocumentSplitter`) to process incrementally |

### Environment Separation

Use environment-specific namespaces to manage dev/staging/prod configurations:

```python
# config.py
import os

ENV = os.getenv('ENVIRONMENT', 'dev')
APP_NAMESPACE = f'{ENV}_myapp'  # Creates: dev_myapp, staging_myapp, prod_myapp

# Model and API configuration
EMBEDDING_MODEL = os.getenv('EMBEDDING_MODEL', 'intfloat/e5-large-v2')
OPENAI_MODEL = os.getenv('OPENAI_MODEL', 'gpt-4o-mini')

# Optional: Cloud storage for generated media
MEDIA_STORAGE_BUCKET = os.getenv('MEDIA_STORAGE_BUCKET')
```

## Testing

**Staging Environment:**
- Mirror production configuration.
- Test schema changes, UDF updates, application code changes.
- Use representative data (anonymized or synthetic).

```python
# Test environment with isolated namespace
import pixeltable as pxt

TEST_NS = 'test_myapp'
pxt.create_dir(TEST_NS, if_exists='replace')
# Run setup targeting test namespace
# Execute tests
# pxt.drop_dir(TEST_NS, force=True)  # Cleanup
```

