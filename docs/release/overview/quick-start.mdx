---
title: 'Quick Start'
description: 'The fastest way to get started using Pixeltable'
icon: "blank"
---

## System Requirements

Before installing, ensure your system meets these requirements:

- Python 3.10 or higher
- Linux, MacOS, or Windows

## Installation

It is recommended to install Pixeltable in a virtual environment.

<Tabs>
  <Tab title="venv">
    <Steps>
      <Step title="Create virtual environment">
        ```bash
        python -m venv .venv
        ```
      </Step>
      <Step title="Activate environment">
        <CodeGroup>
          ```bash Linux/MacOS
          source .venv/bin/activate
          ```
          ```bash Windows
          .venv\Scripts\activate
          ```
        </CodeGroup>
      </Step>
      <Step title="Install Pixeltable">
        ```bash
        pip install pixeltable
        ```
      </Step>
    </Steps>
  </Tab>
  <Tab title="uv">
    <Steps>
      <Step title="Install uv">
        Install uv from the [Installing uv](https://docs.astral.sh/uv/getting-started/installation/) guide.
      </Step>
      <Step title="Create environment">
        ```bash
        uv venv --python 3.12
        ```
      </Step>
      <Step title="Activate environment">
        <CodeGroup>
          ```bash Linux/MacOS
          source .venv/bin/activate
          ```
          ```bash Windows
          .venv\Scripts\activate
          ```
        </CodeGroup>
      </Step>
      <Step title="Install Pixeltable">
        ```bash
        uv pip install pixeltable
        ```
      </Step>
    </Steps>
  </Tab>
  <Tab title="conda">
    <Steps>
      <Step title="Install Miniconda">
        Download and install from the [Miniconda Installation](https://www.anaconda.com/docs/getting-started/miniconda/main) guide.
      </Step>
      <Step title="Create and activate environment">
        ```bash
        conda create --name pxt python=3.12
        conda activate pxt
        ```
      </Step>
      <Step title="Install Pixeltable">
        ```bash
        pip install pixeltable
        ```
      </Step>
    </Steps>
  </Tab>
</Tabs>

## Getting Help

- Join our [Discord Community](https://discord.gg/pixeltable)
- Report issues on [GitHub](https://github.com/pixeltable/pixeltable/issues)
- Contact [support@pixeltable.com](mailto:support@pixeltable.com)

## Build an Image Analysis App

<Note>
This guide will help you spin up a functioning AI workload in 5 minutes.
</Note>

<Steps>
  <Step title="Install Required Packages">
    Pixeltable requires only a minimal set of Python packages by default. To use AI models, you'll need to install
    additional dependencies.
    ```bash
    pip install torch transformers openai
    ```
  </Step>

  <Step title="Create a Table">
    ```python
    import pixeltable as pxt

    # Create a namespace and table
    pxt.create_dir('quickstart', if_exists='replace_force')
    t = pxt.create_table('quickstart.images', {'image': pxt.Image})
    ```

    Tables are persistent: your data survives restarts and can be queried anytime.
  </Step>

  <Step title="Add AI Object Detection">
    ```python
    from pixeltable.functions import huggingface

    # Add DETR object detection as a computed column
    t.add_computed_column(
        detections=huggingface.detr_for_object_detection(
            t.image,
            model_id='facebook/detr-resnet-50'
        )
    )

    # Extract labels from detections
    t.add_computed_column(labels=t.detections.label_text)
    ```

    Computed columns run automatically whenever new data is inserted.
  </Step>

  <Step title="Insert Data">
    ```python
    # Insert a few images
    t.insert([
      {'image': 'https://raw.githubusercontent.com/pixeltable/pixeltable/release/docs/resources/images/000000000001.jpg'},
      {'image': 'https://raw.githubusercontent.com/pixeltable/pixeltable/release/docs/resources/images/000000000025.jpg'}
    ])
    ```

    You can insert images from URLs and/or local paths in any combination.
  </Step>

  <Step title="Query Results">
    ```python
    # Query results
    t.select(t.image, t.labels).collect()
    ```

    **Expected output:**

    | image      | labels              |
    |------------|---------------------|
    | [Image]    | [car, parking meter, truck, car, car, truck]   |
    | [Image]    | [giraffe, giraffe] |

  </Step>

  <Step title="(Optional) Add LLM Vision">
    You'll need an OpenAI API key to use this step. If you don't have one, you can
    safely skip this step.

    ```python
    import os
    from pixeltable.functions import openai

    # Set your API key
    os.environ['OPENAI_API_KEY'] = 'your-key-here'

    t.add_computed_column(
        description=openai.vision(
            prompt="Describe this image in one sentence.",
            image=t.image,
            model='gpt-4o-mini'
        )
    )

    t.select(t.image, t.labels, t.description).collect()
    ```

    ```python
    # See the full text of the description in row 0
    t.select(t.description).collect()[0]
    ```

    Pixeltable orchestrates LLM calls for optimized throughput, handling
    rate limiting, retries, and caching automatically.
  </Step>

  <Step>
    Insert a few more images.

    ```python
    t.insert([
      {'image': 'https://raw.githubusercontent.com/pixeltable/pixeltable/release/docs/resources/images/000000000034.jpg'},
      {'image': 'https://raw.githubusercontent.com/pixeltable/pixeltable/release/docs/resources/images/000000000057.jpg'}
    ])

    t.select(t.image, t.labels, t.description).collect()
    ```

    When new data is insterted into tables, Pixeltable incrementally runs all
    computed columns against the new data, ensuring the table is up to date.
  </Step>
</Steps>

<Accordion title="What happened behind the scenes?">
Pixeltable automatically:

1. Created a persistent multimodal table
2. Downloaded and cached the DETR model
3. Ran inference on your image
4. Stored all results (including computed columns) for instant retrieval
5. Will incrementally process any new images you insert
</Accordion>

## Additional Tutorial - Seems Redundant with the Above?

<Steps>
  <Step title="Start Building (Step 1)" icon="code">
    ```bash
    pip install pixeltable
    ```
    Get up and running with basic [tables](/platform/tables), [queries](/platform/filtering-and-selecting), and [computed columns](/platform/computed-columns).

    <CodeGroup>
    ```python table
    # Create a table to hold data
    t = pxt.create_table('films_table', {
        'name': pxt.String,
        'revenue': pxt.Float,
        'budget': pxt.Float
    })
    ```
    ```python query
    # Insert data into a table
    t.insert([
      {'name': 'Inside Out', 'revenue': 800.5, 'budget': 200.0},
      {'name': 'Toy Story', 'revenue': 1073.4, 'budget': 200.0}
    ])

  # Retrieves all the rows in the table.
  t.collect()
    ```
    ```python transform
    # Add a new column for the profit calculation
    t.add_computed_column(profit=(t.revenue - t.budget))

    # It will automatically compute its value for all rows
    t.select(t.profit).head()
    ```
    </CodeGroup>

  <Info>
  All data and computed results are automatically stored and versioned.
  </Info>
  </Step>

  <Step title="Add Processing (Step 2)" icon="sparkles">
    Add [LLMs](/integrations/frameworks#cloud-llm-providers), [computer vision](/integrations/frameworks#computer-vision), and [embeddings indices](/platform/vector-database).
      <CodeGroup>
    ```python embedding index
    from pixeltable.functions.huggingface import clip
    import PIL.Image

    # create embedding index on the 'img' column of table 't'
    t.add_embedding_index(
        'img',
        embedding=clip.using(model_id='openai/clip-vit-base-patch32')
    )

    # index is kept up-to-date enabling relevant searches
    sim = t.img.similarity(sample_img)

    res = (
        t.order_by(sim, asc=False)  # Order by similarity
        .where(t.id != 6)  # Metadata filtering
        .limit(2)  # Limit number of results to 2
        .select(t.id, t.img, sim)
        .collect()  # Retrieve results now
    )
    ```
    ```python llms
    from pixeltable.functions import openai

    # Assemble the prompt and instructions
    messages = [
        {
            'role': 'system',
            'content': 'Please read the following passages.'
        },
        {
            'role': 'user',
            'content': t.prompt # generated from the 'prompt' column
        }
    ]

    # Add a computed column that calls OpenAI
    t.add_computed_column(
        response=openai.chat_completions(
            model='gpt-4o-mini',
            messages=messages,
            model_kwargs={'temperature': 0.7}
        )
    )
        ```
        ```python computer vision
    from pixeltable.functions.yolox import yolox

    # compute object detections using the `yolox_tiny` model
    frames_view.add_computed_column(detect_yolox_tiny=yolox(
        frames_view.frame, model_id='yolox_tiny', threshold=0.25
    ))

    # The inference in the computed column is now stored
    frames_view.select(
        frames_view.frame,
        frames_view.detect_yolox_tiny
    ).show(3)
    ```
    </CodeGroup>
  <Info>
  Pixeltable orchestrates model execution, ensuring results are stored, indexed, and accessible through the same query interface.
  </Info>
  </Step>

  <Step title="Scale Up (Step 3)" icon="chart-mixed">
    Handle [production data](/platform/bringing-data) volumes, and deploy your application.
      <CodeGroup>
    ```python bring cloud data
    # Import media data (videos, images, audio...)
    v = pxt.create_table('videos', {'video': pxt.Video})

    prefix = 's3://multimedia-commons/'
    paths = [
        'data/videos/mp4/ffe/ffb/ffeffbef41bbc269810b2a1a888de.mp4',
        'data/videos/mp4/ffe/feb/ffefebb41485539f964760e6115fbc44.mp4',
        'data/videos/mp4/ffe/f73/ffef7384d698b5f70d411c696247169.mp4'
    ]
    v.insert({'video': prefix + p} for p in paths)
    ```
    ```python chunking with views
    # Optimize large-scale data processing
    from pixeltable.iterators import DocumentSplitter

    # Create chunked views for efficient processing
    doc_chunks = pxt.create_view(
        'chunks',
        analysis,
        iterator=DocumentSplitter.create(
            document=analysis.document,
            separators='sentence',
            limit=500  # Control chunk size
        )
    )
    ```
    ```python serving
    from fastapi import FastAPI, HTTPException
    from pydantic import BaseModel

    app = FastAPI()

    class AnalysisRequest(BaseModel):
        document: str
        metadata: dict = {}

    @app.post("/analyze")
    async def analyze_document(request: AnalysisRequest):
        try:
            # Insert document for processing
            analysis.insert([{
                'document': request.document,
                'metadata': request.metadata,
                'timestamp': datetime.now()
            }])

            # Get analysis results using computed columns
            result = analysis.select(
                analysis.embeddings,
                analysis.summary,
                analysis.sentiment
            ).tail(1)

            return {
                "status": "success",
                "results": result.to_dict('records')[0]
            }
        except Exception as e:
            raise HTTPException(status_code=0, detail=str(e))
    ```
    </CodeGroup>
  <Info>
Handle images, video, audio, numbers, array and text seamlessly in one interface.
</Info>
  </Step>
</Steps>
