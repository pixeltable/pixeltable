---
title: 'What is Pixeltable?'
description: 'Pixeltable unifies storage and orchestration for multimodal AI applications, providing a declarative interface for incremental transforms, indexing, and orchestration.'
---

## Key Capabilities

<CardGroup cols={2}>
  <Card title="Persistent Storage" icon="database" iconType="duotone">
    All data and computed results are automatically stored and versioned.
  </Card>
  <Card title="Incremental Updates" icon="arrows-rotate" iconType="duotone">
    Data transformations run automatically on new data—no orchestration code needed.
  </Card>
  <Card title="Multimodal-Native" icon="photo-film" iconType="duotone">
    Images, video, audio, and documents integrate seamlessly with structured data.
  </Card>
  <Card title="AI Integration" icon="microchip-ai" iconType="duotone">
    Built-in support for OpenAI, Anthropic, Gemini, Hugging Face, and dozens more.
  </Card>
</CardGroup>

## Get started

<CardGroup cols={2}>
  <Card title="Quick Start" icon="bolt" iconType="duotone" href="/overview/quick-start">
    Install Pixeltable and run your first pipeline in 5 minutes.
  </Card>
  <Card title="10-Minute Tour" icon="play" iconType="duotone" href="/overview/ten-minute-tour">
    See Pixeltable in action with a hands-on image workflow.
  </Card>
  <Card title="Core Concepts" icon="cubes" iconType="duotone" href="/platform/type-system">
    Learn about tables, computed columns, views, and the type system.
  </Card>
  <Card title="SDK Reference" icon="code" iconType="duotone" href="/sdk/latest">
    Complete API reference for the Pixeltable Python SDK.
  </Card>
</CardGroup>

<Tip>
Many documentation pages are interactive notebooks (marked with <Icon icon="notebook" /> in the sidebar). Open them in Colab, Kaggle, or locally to follow along.
</Tip>

## Core Primitives

Pixeltable provides a small set of primitives that compose into any multimodal AI workflow:

<AccordionGroup>
  <Accordion title="Store" icon="database">
    **Create tables with native multimodal types**
    
    ```python
    t = pxt.create_table('myapp.media', {
        'video': pxt.Video,
        'image': pxt.Image,
        'audio': pxt.Audio,
        'document': pxt.Document,
        'metadata': pxt.Json
    })
    ```
    
    <CardGroup cols={2}>
      <Card title="Tables & Data" icon="table" href="/tutorials/tables-and-data-operations">
        Create, insert, update, delete
      </Card>
      <Card title="Type System" icon="shapes" href="/platform/type-system">
        All supported types
      </Card>
    </CardGroup>
  </Accordion>
  
  <Accordion title="Orchestrate" icon="wand-magic-sparkles">
    **Declarative computed columns: API calls, LLM inference, local models, vision**
    
    ```python
    # LLM API call
    t.add_computed_column(summary=openai.chat_completions(
        messages=[{'role': 'user', 'content': 'Summarize: ' + t.text}]
    ))
    
    # Local model inference
    t.add_computed_column(objects=yolox(t.image, model_id='yolox_s'))
    
    # Vision analysis
    t.add_computed_column(desc=openai.vision(prompt="Describe", image=t.image))
    ```
    
    <CardGroup cols={2}>
      <Card title="Computed Columns" icon="bolt" href="/tutorials/computed-columns">
        Incremental transforms
      </Card>
      <Card title="AI Integrations" icon="microchip-ai" href="/integrations/frameworks">
        OpenAI, Anthropic, Gemini, HuggingFace...
      </Card>
    </CardGroup>
  </Accordion>
  
  <Accordion title="Split" icon="scissors">
    **Create views with iterators to expand rows**
    
    ```python
    # Extract frames from video at 1 fps
    frames = pxt.create_view('myapp.frames', t, iterator=FrameIterator(t.video, fps=1))
    
    # Chunk documents for RAG
    chunks = pxt.create_view('myapp.chunks', t, iterator=DocumentSplitter(t.document))
    ```
    
    <CardGroup cols={2}>
      <Card title="Views" icon="layer-group" href="/platform/views">
        Virtual tables
      </Card>
      <Card title="Iterators" icon="arrows-split-up-and-left" href="/platform/iterators">
        Frame, Document, Audio splitters
      </Card>
    </CardGroup>
  </Accordion>
  
  <Accordion title="Index" icon="magnifying-glass">
    **Add embedding indexes for semantic search**
    
    ```python
    t.add_embedding_index('text', embedding=openai.embeddings())
    
    # Search by similarity
    results = t.order_by(t.text.similarity('find relevant docs'), asc=False).limit(10)
    ```
    
    <Card title="Embedding Indexes" icon="vector-square" href="/platform/embedding-indexes">
      Vector search with automatic maintenance
    </Card>
  </Accordion>
  
  <Accordion title="Extend" icon="puzzle-piece">
    **Write custom functions with `@pxt.udf` and `@pxt.query`**
    
    ```python
    @pxt.udf
    def extract_entities(text: str) -> list[str]:
        # Your custom logic
        return entities
    
    @pxt.query
    def search_by_topic(topic: str):
        return t.where(t.category == topic).select(t.title, t.summary)
    ```
    
    <Card title="UDFs & Queries" icon="code" href="/platform/udfs-in-pixeltable">
      Custom Python functions
    </Card>
  </Accordion>
  
  <Accordion title="Agents & Tools" icon="robot">
    **Tool calling for AI agents and MCP integration**
    
    ```python
    # Load tools from MCP server, UDFs, and queries
    mcp_tools = pxt.mcp_udfs('http://localhost:8000/mcp')
    tools = pxt.tools(search_by_topic, extract_entities, *mcp_tools)
    
    # LLM decides which tool to call; Pixeltable executes it
    t.add_computed_column(response=openai.chat_completions(
        messages=[{'role': 'user', 'content': t.question}],
        tools=tools
    ))
    t.add_computed_column(result=openai.invoke_tools(tools, t.response))
    ```
    
    <CardGroup cols={2}>
      <Card title="Tool Calling" icon="wrench" href="/howto/cookbooks/agents/llm-tool-calling">
        Build agents with tools
      </Card>
      <Card title="Agents & MCP" icon="plug" href="/use-cases/agents-mcp">
        MCP servers, memory, Pixelbot
      </Card>
    </CardGroup>
  </Accordion>
  
  <Accordion title="Query & Experiment" icon="flask">
    **SQL-like queries + test transformations before committing**
    
    ```python
    # Query data with familiar syntax
    results = t.where(t.score > 0.8).order_by(t.timestamp).limit(10).collect()
    
    # Test transformations on sample rows BEFORE adding to table
    t.select(t.text, summary=summarize(t.text)).head(3)  # Nothing stored yet
    t.add_computed_column(summary=summarize(t.text))      # Now commit to all rows
    ```
    
    <CardGroup cols={2}>
      <Card title="Queries & Expressions" icon="terminal" href="/tutorials/queries-and-expressions">
        Select, filter, aggregate
      </Card>
      <Card title="Iterative Development" icon="rotate" href="/howto/deployment/operations#testing-transformations-before-deployment">
        Test before commit
      </Card>
    </CardGroup>
  </Accordion>
  
  <Accordion title="Version" icon="clock-rotate-left">
    **Time travel and automatic versioning**
    
    ```python
    t.history()                    # View all versions
    t.revert(version=5)            # Rollback changes
    old_data = pxt.get_table('myapp.media:3')  # Query past version
    ```
    
    <Card title="Version Control" icon="code-branch" href="/platform/version-control">
      History, snapshots, lineage
    </Card>
  </Accordion>
  
  <Accordion title="Import/Export" icon="arrow-right-arrow-left">
    **Load from any source, export to ML formats**
    
    ```python
    # Import from files, URLs, S3, Hugging Face
    t.insert(pxt.io.import_csv('data.csv'))
    t.insert(pxt.io.import_huggingface_dataset(dataset))
    
    # Export to ML/analytics formats
    pxt.io.export_parquet(t, 'output.parquet')
    loader = DataLoader(t.to_pytorch_dataset(), batch_size=32)
    coco_path = t.to_coco_dataset()
    ```
    
    <CardGroup cols={2}>
      <Card title="Data Import" icon="download" href="/howto/cookbooks/data/data-import-csv">
        CSV, JSON, Parquet, S3, HF
      </Card>
      <Card title="Data Export" icon="upload" href="/howto/cookbooks/data/data-export-pytorch">
        PyTorch, Parquet, COCO, LanceDB
      </Card>
    </CardGroup>
  </Accordion>
  
  <Accordion title="Share" icon="cloud-arrow-up">
    **Publish and replicate datasets via Pixeltable Cloud**
    
    ```python
    pxt.publish(t, 'my-dataset')              # Share publicly
    pxt.replicate('user/dataset', 'local')   # Pull to local
    ```
    
    <Card title="Data Sharing" icon="share-nodes" href="/platform/data-sharing">
      Publish, replicate, collaborate
    </Card>
  </Accordion>
</AccordionGroup>

<Note>
These primitives are **use-case agnostic by design**. We don't build vertical solutions—we build the infrastructure that makes vertical solutions trivial to build.
</Note>

## Use Cases

Pixeltable's primitives compose into any multimodal AI workflow:

<CardGroup cols={3}>
  <Card title="Data Wrangling for ML" icon="flask" href="/use-cases/ml-data-wrangling">
    Curate, augment, and export training datasets. Pre-annotate with models, integrate with Label Studio, export to PyTorch.
  </Card>
  <Card title="Backend for AI Apps" icon="microchip-ai" href="/use-cases/ai-applications">
    Build RAG systems, semantic search, and multimodal APIs. Pixeltable handles storage, retrieval, and orchestration.
  </Card>
  <Card title="Agents & MCP" icon="robot" href="/use-cases/agents-mcp">
    Tool-calling agents with persistent memory, MCP server integration, and automatic conversation history.
  </Card>
</CardGroup>

<Tip>
Explore the **[Cheat Sheet](/howto/deployment/cheatsheet)** for a quick reference, or dive into the **How-To** section for hands-on cookbooks covering RAG, video analysis, audio transcription, and more.
</Tip>

## Next steps

<CardGroup cols={2}>
  <Card
    title="Join the Community"
    icon="discord"
    iconType="duotone"
    href="https://discord.com/invite/QPyqFYx2UN"
  >
    Get help, share projects, and connect with other developers
  </Card>
  <Card
    title="GitHub"
    icon="github"
    iconType="duotone"
    href="https://github.com/pixeltable/pixeltable"
  >
    Star the repo, report issues, and contribute
  </Card>
</CardGroup>
