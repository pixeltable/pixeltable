---
title: 'Ecosystem'
description: 'Explore Pixeltable ecosystem of built-in integrations for AI/ML workflows'
icon: 'toolbox'
---

From language models to computer vision frameworks, Pixeltable integrates with the entire ecosystem. All integrations are available out-of-the-box with Pixeltable installation. No additional setup required unless specified.

<Note>
If you have a framework that you want us to integrate with, please reach out and you can also leverage Pixeltable's [UDFs](/notebooks/feature-guides/udfs-in-pixeltable) to build your own.
</Note>

## Cloud LLM Providers

<CardGroup cols={3}>
  <Card
    title="Anthropic Claude"
    icon="brain"
    href="/notebooks/integrations/working-with-anthropic"
  >
    Integrate Claude models for advanced language understanding and generation with multimodal capabilities
  </Card>

  <Card
    title="Google Gemini"
    icon="sparkles"
    href="/notebooks/integrations/working-with-gemini"
  >
    Access Google's Gemini models for state-of-the-art multimodal AI capabilities
  </Card>

  <Card
    title="OpenAI"
    icon="square-code"
    href="/notebooks/integrations/working-with-openai"
  >
    Leverage GPT models for text generation, embeddings, and image analysis
  </Card>

  <Card
    title="Mistral AI"
    icon="wind"
    href="/notebooks/integrations/working-with-mistralai"
  >
    Use Mistral's efficient language models for various NLP tasks
  </Card>

  <Card
    title="Together AI"
    icon="users"
    href="/notebooks/integrations/working-with-together"
  >
    Access a variety of open-source models through Together AI's platform
  </Card>

  <Card
    title="Fireworks"
    icon="rocket"
    href="/notebooks/integrations/working-with-fireworks"
  >
    Use Fireworks.ai's optimized model inference infrastructure
  </Card>

   <Card
    title="DeepSeek"
    icon="robot"
    href="/notebooks/integrations/working-with-deepseek"
  >
    Leverage DeepSeek's powerful language and code models for text and code generation
  </Card>

   <Card
    title="AWS Bedrock"
    icon="aws"
    href="/notebooks/integrations/working-with-bedrock"
  >
    Access a variety of AI models through AWS Bedrock's unified API
  </Card>

   <Card
    title="Groq"
    icon="microchip"
    href="/notebooks/integrations/working-with-groq"
  >
    Access Groq's models for text generation
  </Card>

  <Card
    title="OpenRouter"
    icon="route"
    href="/notebooks/integrations/working-with-openrouter"
  >
    Unified access to 100+ LLMs from various providers through a single API
  </Card>

</CardGroup>

## Local LLM Runtimes

<CardGroup cols={2}>
  <Card
    title="Llama.cpp"
    icon="microchip"
    href="/notebooks/integrations/working-with-llama-cpp"
  >
    High-performance C++ implementation for running LLMs on CPU and GPU
  </Card>

  <Card
    title="Ollama"
    icon="box"
    href="/notebooks/integrations/working-with-ollama"
  >
    Easy-to-use toolkit for running and managing open-source models locally
  </Card>
</CardGroup>

## Computer Vision

<CardGroup cols={2}>
  <Card
    title="YOLOX"
    icon="camera"
    href="/notebooks/use-cases/object-detection-in-videos"
  >
    State-of-the-art object detection with YOLOX models
  </Card>

  <Card
    title="Voxel51"
    icon="cube"
    href="/notebooks/integrations/working-with-fiftyone"
  >
    Advanced video and image dataset management with Voxel51
  </Card>
</CardGroup>

## Annotation Tools

<CardGroup cols={1}>
  <Card
    title="Label Studio"
    icon="tags"
    href="/notebooks/integrations/using-label-studio-with-pixeltable"
  >
    Comprehensive platform for data annotation and labeling workflows
  </Card>

</CardGroup>

## Audio Processing

<Card
  title="Whisper/WhisperX"
  icon="waveform"
  href="/notebooks/use-cases/audio-transcriptions"
>
  High-quality speech recognition and transcription using OpenAI's Whisper models
</Card>

## Data Wrangling

<Card
  title="Pandas"
  icon="table"
  href="/notebooks/fundamentals/tables-and-data-operations"
>
  Import and Export from and to Pandas DataFrames if needed
</Card>

## Usage Examples

<AccordionGroup>
  <Accordion title="LLM Integration">
    ```python
    import pixeltable as pxt
    from pixeltable.functions import openai

    # Create a table with computed column for OpenAI completion
    table = pxt.create_table('responses', {'prompt': pxt.String})

    table.add_computed_column(
        response=openai.chat_completions(
            messages=[{'role': 'user', 'content': table.prompt}],
            model='gpt-4'
        )
    )
    ```
  </Accordion>

  <Accordion title="Computer Vision">
    ```python
    from pixeltable.functions.yolox import yolox

    # Add object detection to video frames
    frames_view.add_computed_column(
        detections=yolox(
            frames_view.frame,
            model_id='yolox_l'
        )
    )
    ```
  </Accordion>

  <Accordion title="Audio Processing">
    ```python
    from pixeltable.functions import openai

    # Transcribe audio files
    audio_table.add_computed_column(
        transcription=openai.transcriptions(
            audio=audio_table.file,
            model='whisper-1'
        )
    )
    ```
  </Accordion>
</AccordionGroup>

## Integration Features

<Steps>
  <Step title="Easy Setup">
    Most integrations work out-of-the-box with simple API configuration
  </Step>

  <Step title="Computed Columns">
    Use integrations directly in computed columns for automated processing
  </Step>

  <Step title="Batch Processing">
    Efficient handling of batch operations with automatic optimization
  </Step>
</Steps>

<Tip>
  Check our [Github](https://github.com/pixeltable/pixeltable/tree/main/docs/notebooks/integrations) for detailed usage instructions for each integration.
</Tip>

<Note>
Need help setting up integrations? Join our [Discord community](https://discord.com/invite/QPyqFYx2UN) for support.
</Note>