{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Kaggle](https://kaggle.com/static/images/open-in-kaggle.svg)](https://kaggle.com/kernels/welcome?src=https://github.com/pixeltable/pixeltable/blob/master/docs/release/tutorials/image-operations.ipynb)&nbsp;&nbsp;\n",
    "[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/pixeltable/pixeltable/blob/master/docs/release/tutorials/image-operations.ipynb)\n",
    "\n",
    "# Working with Images, Videos, and Documents\n",
    "\n",
    "In this tutorial, we'll be working with a [...]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T20:58:42.863380Z",
     "start_time": "2024-04-11T20:58:42.859783Z"
    }
   },
   "outputs": [],
   "source": [
    "import pixeltable as pxt\n",
    "from pixeltable.iterators.document import DocumentSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T20:58:42.867255Z",
     "start_time": "2024-04-11T20:58:42.865054Z"
    }
   },
   "outputs": [],
   "source": [
    "pdf_path = f'{pxt.__path__[0]}/tests/data/documents/layout-parser-paper.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_tuples = []\n",
    "for i, tup in enumerate(DocumentSplitter(pdf_path, separators='paragraph', metadata='page, bounding_box')):\n",
    "    example_tuples.append(tup)\n",
    "    if i == 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'LayoutParser: A Unified Toolkit for Deep\\nLearning Based Document Image Analysis\\n',\n",
       "  'page': 0,\n",
       "  'bounding_box': {'x1': 157.6219940185547,\n",
       "   'y1': 115.03832244873047,\n",
       "   'x2': 457.8017883300781,\n",
       "   'y2': 147.33184814453125}},\n",
       " {'text': 'Zejiang Shen1 (), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\\nLee4, Jacob Carlson3, and Weining Li5\\n',\n",
       "  'page': 0,\n",
       "  'bounding_box': {'x1': 134.8090057373047,\n",
       "   'y1': 167.27603149414062,\n",
       "   'x2': 480.54638671875,\n",
       "   'y2': 192.8096160888672}},\n",
       " {'text': '1 Allen Institute for AI\\nshannons@allenai.org\\n2 Brown University\\nruochen zhang@brown.edu\\n3 Harvard University\\n{melissadell,jacob carlson}@fas.harvard.edu\\n4 University of Washington\\nbcgl@cs.washington.edu\\n5 University of Waterloo\\nw422li@uwaterloo.ca\\n',\n",
       "  'page': 0,\n",
       "  'bounding_box': {'x1': 207.23001098632812,\n",
       "   'y1': 200.6651611328125,\n",
       "   'x2': 408.1268615722656,\n",
       "   'y2': 312.072998046875}},\n",
       " {'text': 'Abstract. Recent advances in document image analysis (DIA) have been\\nprimarily driven by the application of neural networks. Ideally, research\\noutcomes could be easily deployed in production and extended for further\\ninvestigation. However, various factors like loosely organized codebases\\nand sophisticated model configurations complicate the easy reuse of im-\\nportant innovations by a wide audience. Though there have been on-going\\nefforts to improve reusability and simplify deep learning (DL) model\\ndevelopment in disciplines like natural language processing and computer\\nvision, none of them are optimized for challenges in the domain of DIA.\\nThis represents a major gap in the existing toolkit, as DIA is central to\\nacademic research across a wide range of disciplines in the social sciences\\nand humanities. This paper introduces LayoutParser, an open-source\\nlibrary for streamlining the usage of DL in DIA research and applica-\\ntions. The core LayoutParser library comes with a set of simple and\\nintuitive interfaces for applying and customizing DL models for layout de-\\ntection, character recognition, and many other document processing tasks.\\nTo promote extensibility, LayoutParser also incorporates a community\\nplatform for sharing both pre-trained models and full document digiti-\\nzation pipelines. We demonstrate that LayoutParser is helpful for both\\nlightweight and large-scale digitization pipelines in real-word use cases.\\nThe library is publicly available at https://layout-parser.github.io.\\n',\n",
       "  'page': 0,\n",
       "  'bounding_box': {'x1': 162.7790069580078,\n",
       "   'y1': 338.9521789550781,\n",
       "   'x2': 454.092529296875,\n",
       "   'y2': 567.0989990234375}},\n",
       " {'text': 'Keywords: Document Image Analysis · Deep Learning · Layout Analysis\\n· Character Recognition · Open Source library · Toolkit.\\n',\n",
       "  'page': 0,\n",
       "  'bounding_box': {'x1': 162.3459930419922,\n",
       "   'y1': 577.9205322265625,\n",
       "   'x2': 452.2435302734375,\n",
       "   'y2': 600.168212890625}}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T20:58:43.144688Z",
     "start_time": "2024-04-11T20:58:42.912418Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table `pdf_table`.\n",
      "Inserting rows into `pdf_table`: 1 rows [00:00, 1069.70 rows/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inserted 1 row with 0 errors.\n",
      "Inserting rows into `pdf_paragraphs`: 153 rows [00:00, 15016.46 rows/s]\n",
      "Created view `pdf_paragraphs` with 153 rows, 0 exceptions.\n"
     ]
    }
   ],
   "source": [
    "pxt.drop_table('pdf_paragraphs', ignore_errors=True)\n",
    "pxt.drop_table('pdf_table', ignore_errors=True)\n",
    "\n",
    "pdf_table = pxt.create_table('pdf_table', { 'document' : pxt.DocumentType() })\n",
    "\n",
    "pdf_table.insert([{'document':pdf_path}])\n",
    "paragraph_table = cl.create_view(\n",
    "    'pdf_paragraphs',\n",
    "    pdf_table,\n",
    "    iterator=DocumentSplitter(document=pdf_table.document, separators='paragraph', metadata='page,bounding_box')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-11T20:58:43.165541Z",
     "start_time": "2024-04-11T20:58:43.145501Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>pos</th>\n",
       "      <th>text</th>\n",
       "      <th>page</th>\n",
       "      <th>bounding_box</th>\n",
       "      <th>document</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>LayoutParser: A Unified Toolkit for Deep\\nLearning Based Document Image Analysis\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>{'x1': 157.6219940185547, 'x2': 457.8017883300781, 'y1': 115.03832244873047, 'y2': 147.33184814453125}</td>\n",
       "      <td>/Users/orm/repos/pixeltable/pixeltable/tests/data/documents/layout-parser-paper.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Zejiang Shen1 (), Ruochen Zhang2, Melissa Dell3, Benjamin Charles Germain\\nLee4, Jacob Carlson3, and Weining Li5\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>{'x1': 134.8090057373047, 'x2': 480.54638671875, 'y1': 167.27603149414062, 'y2': 192.8096160888672}</td>\n",
       "      <td>/Users/orm/repos/pixeltable/pixeltable/tests/data/documents/layout-parser-paper.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>1 Allen Institute for AI\\nshannons@allenai.org\\n2 Brown University\\nruochen zhang@brown.edu\\n3 Harvard University\\n{melissadell,jacob carlson}@fas.harvard.edu\\n4 University of Washington\\nbcgl@cs.washington.edu\\n5 University of Waterloo\\nw422li@uwaterloo.ca\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>{'x1': 207.23001098632812, 'x2': 408.1268615722656, 'y1': 200.6651611328125, 'y2': 312.072998046875}</td>\n",
       "      <td>/Users/orm/repos/pixeltable/pixeltable/tests/data/documents/layout-parser-paper.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Abstract. Recent advances in document image analysis (DIA) have been\\nprimarily driven by the application of neural networks. Ideally, research\\noutcomes could be easily deployed in production and extended for further\\ninvestigation. However, various factors like loosely organized codebases\\nand sophisticated model configurations complicate the easy reuse of im-\\nportant innovations by a wide audience. Though there have been on-going\\nefforts to improve reusability and simplify deep learning (DL) model\\ndevelopment in disciplines like natural language processing and computer\\nvision, none of them are optimized for challenges in the domain of DIA.\\nThis represents a major gap in the existing toolkit, as DIA is central to\\nacademic research across a wide range of disciplines in the social sciences\\nand humanities. This paper introduces LayoutParser, an open-source\\nlibrary for streamlining the usage of DL in DIA research and applica-\\ntions. The core LayoutParser library comes with a set of simple and\\nintuitive interfaces for applying and customizing DL models for layout de-\\ntection, character recognition, and many other document processing tasks.\\nTo promote extensibility, LayoutParser also incorporates a community\\nplatform for sharing both pre-trained models and full document digiti-\\nzation pipelines. We demonstrate that LayoutParser is helpful for both\\nlightweight and large-scale digitization pipelines in real-word use cases.\\nThe library is publicly available at https://layout-parser.github.io.\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>{'x1': 162.7790069580078, 'x2': 454.092529296875, 'y1': 338.9521789550781, 'y2': 567.0989990234375}</td>\n",
       "      <td>/Users/orm/repos/pixeltable/pixeltable/tests/data/documents/layout-parser-paper.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Keywords: Document Image Analysis · Deep Learning · Layout Analysis\\n· Character Recognition · Open Source library · Toolkit.\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>{'x1': 162.3459930419922, 'x2': 452.2435302734375, 'y1': 577.9205322265625, 'y2': 600.168212890625}</td>\n",
       "      <td>/Users/orm/repos/pixeltable/pixeltable/tests/data/documents/layout-parser-paper.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1\\nIntroduction\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>{'x1': 134.7650146484375, 'x2': 228.99588012695312, 'y1': 619.8935546875, 'y2': 631.8607177734375}</td>\n",
       "      <td>/Users/orm/repos/pixeltable/pixeltable/tests/data/documents/layout-parser-paper.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>Deep Learning(DL)-based approaches are the state-of-the-art for a wide range of\\ndocument image analysis (DIA) tasks including document image classification [11,\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>{'x1': 134.76499938964844, 'x2': 481.96844482421875, 'y1': 643.4950561523438, 'y2': 665.4126586914062}</td>\n",
       "      <td>/Users/orm/repos/pixeltable/pixeltable/tests/data/documents/layout-parser-paper.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>arXiv:2103.15348v2  [cs.CV]  21 Jun 2021\\n</td>\n",
       "      <td>0</td>\n",
       "      <td>{'x1': 10.940000534057617, 'x2': 37.619998931884766, 'y1': 213.3599853515625, 'y2': 560.0}</td>\n",
       "      <td>/Users/orm/repos/pixeltable/pixeltable/tests/data/documents/layout-parser-paper.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>2\\nZ. Shen et al.\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>{'x1': 134.76499938964844, 'x2': 222.0977325439453, 'y1': 93.17021942138672, 'y2': 102.1366195678711}</td>\n",
       "      <td>/Users/orm/repos/pixeltable/pixeltable/tests/data/documents/layout-parser-paper.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>37], layout detection [38, 22], table detection [26], and scene text detection [4].\\nA generalized learning-based framework dramatically reduces the need for the\\nmanual specification of complicated rules, which is the status quo with traditional\\nmethods. DL has the potential to transform DIA pipelines and benefit a broad\\nspectrum of large-scale document digitization projects.\\nHowever, there are several practical difficulties for taking advantages of re-\\ncent advances in DL-based methods: 1) DL models are notoriously convoluted\\nfor reuse and extension. Existing models are developed using distinct frame-\\nworks like TensorFlow [1] or PyTorch [24], and the high-level parameters can\\nbe obfuscated by implementation details [8]. It can be a time-consuming and\\nfrustrating experience to debug, reproduce, and adapt existing models for DIA,\\nand many researchers who would benefit the most from using these methods lack\\nthe technical background to implement them from scratch. 2) Document images\\ncontain diverse and disparate patterns across domains, and customized training\\nis often required to achieve a desirable detection accuracy. Currently there is no\\nfull-fledged infrastructure for easily curating the target document image datasets\\nand fine-tuning or re-training the models. 3) DIA usually requires a sequence of\\nmodels and other processing to obtain the final outputs. Often research teams use\\nDL models and then perform further document analyses in separate processes,\\nand these pipelines are not documented in any central location (and often not\\ndocumented at all). This makes it difficult for research teams to learn about how\\nfull pipelines are implemented and leads them to invest significant resources in\\nreinventing the DIA wheel.\\nLayoutParser provides a unified toolkit to support DL-based document image\\nanalysis and processing. To address the aforementioned challenges, LayoutParser\\nis built with the following components:\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>{'x1': 134.00100708007812, 'x2': 482.518798828125, 'y1': 118.3260269165039, 'y2': 427.1676330566406}</td>\n",
       "      <td>/Users/orm/repos/pixeltable/pixeltable/tests/data/documents/layout-parser-paper.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>1. An off-the-shelf toolkit for applying DL models for layout detection, character\\nrecognition, and other DIA tasks (Section 3)\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>{'x1': 138.97097778320312, 'x2': 480.8236999511719, 'y1': 435.1130065917969, 'y2': 457.0306701660156}</td>\n",
       "      <td>/Users/orm/repos/pixeltable/pixeltable/tests/data/documents/layout-parser-paper.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>2. A rich repository of pre-trained neural network models (Model Zoo) that\\nunderlies the off-the-shelf usage\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>{'x1': 138.97100830078125, 'x2': 480.89447021484375, 'y1': 458.6190490722656, 'y2': 480.5366516113281}</td>\n",
       "      <td>/Users/orm/repos/pixeltable/pixeltable/tests/data/documents/layout-parser-paper.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3. Comprehensive tools for efficient document image data annotation and model\\ntuning to support different levels of customization\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>{'x1': 138.97100830078125, 'x2': 480.5992126464844, 'y1': 482.12603759765625, 'y2': 504.0436706542969}</td>\n",
       "      <td>/Users/orm/repos/pixeltable/pixeltable/tests/data/documents/layout-parser-paper.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>4. A DL model hub and community platform for the easy sharing, distribu-\\ntion, and discussion of DIA models and pipelines, to promote reusability,\\nreproducibility, and extensibility (Section 4)\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>{'x1': 138.97100830078125, 'x2': 482.3170471191406, 'y1': 505.6330261230469, 'y2': 539.5056762695312}</td>\n",
       "      <td>/Users/orm/repos/pixeltable/pixeltable/tests/data/documents/layout-parser-paper.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>The library implements simple and intuitive Python APIs without sacrificing\\ngeneralizability and versatility, and can be easily installed via pip. Its convenient\\nfunctions for handling document image data can be seamlessly integrated with\\nexisting DIA pipelines. With detailed documentations and carefully curated\\ntutorials, we hope this tool will benefit a variety of end-users, and will lead to\\nadvances in applications in both industry and academic research.\\nLayoutParser is well aligned with recent efforts for improving DL model\\nreusability in other disciplines like natural language processing [8, 34] and com-\\nputer vision [35], but with a focus on unique challenges in DIA. We show\\nLayoutParser can be applied in sophisticated and large-scale digitization projects\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>{'x1': 134.406005859375, 'x2': 482.25592041015625, 'y1': 547.8540649414062, 'y2': 665.4207153320312}</td>\n",
       "      <td>/Users/orm/repos/pixeltable/pixeltable/tests/data/documents/layout-parser-paper.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>LayoutParser: A Unified Toolkit for DL-Based DIA\\n3\\n</td>\n",
       "      <td>2</td>\n",
       "      <td>{'x1': 237.13600158691406, 'x2': 480.6082763671875, 'y1': 93.17021942138672, 'y2': 102.13903045654297}</td>\n",
       "      <td>/Users/orm/repos/pixeltable/pixeltable/tests/data/documents/layout-parser-paper.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>that require precision, efficiency, and robustness, as well as simple and light-\\nweight document processing tasks focusing on efficacy and flexibility (Section 5).\\nLayoutParser is being actively maintained, and support for more deep learning\\nmodels and novel methods in text-based layout analysis methods [37, 34] is\\nplanned.\\nThe rest of the paper is organized as follows. Section 2 provides an overview\\nof related work. The core LayoutParser library, DL Model Zoo, and customized\\nmodel training are described in Section 3, and the DL model hub and commu-\\nnity platform are detailed in Section 4. Section 5 shows two examples of how\\nLayoutParser can be used in practical DIA projects, and Section 6 concludes.\\n</td>\n",
       "      <td>2</td>\n",
       "      <td>{'x1': 134.406005859375, 'x2': 482.5365295410156, 'y1': 118.3260269165039, 'y2': 236.00668334960938}</td>\n",
       "      <td>/Users/orm/repos/pixeltable/pixeltable/tests/data/documents/layout-parser-paper.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>2\\nRelated Work\\n</td>\n",
       "      <td>2</td>\n",
       "      <td>{'x1': 134.7650146484375, 'x2': 236.80259704589844, 'y1': 256.9905700683594, 'y2': 268.9577331542969}</td>\n",
       "      <td>/Users/orm/repos/pixeltable/pixeltable/tests/data/documents/layout-parser-paper.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>LayoutParser: A Unified Toolkit for DL-Based DIA\\n5\\n</td>\n",
       "      <td>4</td>\n",
       "      <td>{'x1': 237.13600158691406, 'x2': 480.6082763671875, 'y1': 93.17021942138672, 'y2': 102.13903045654297}</td>\n",
       "      <td>/Users/orm/repos/pixeltable/pixeltable/tests/data/documents/layout-parser-paper.pdf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>Recently, various DL models and datasets have been developed for layout analysis\\ntasks. The dhSegment [22] utilizes fully convolutional networks [20] for segmen-\\ntation tasks on historical documents. Object detection-based methods like Faster\\nR-CNN [28] and Mask R-CNN [12] are used for identifying document elements [38]\\nand detecting tables [30, 26]. Most recently, Graph Neural Networks [29] have also\\nbeen used in table detection [27]. However, these models are usually implemented\\nindividually and there is no unified framework to load and use such models.\\nThere has been a surge of interest in creating open-source tools for document\\nimage processing: a search of document image analysis in Github leads to 5M\\nrelevant code pieces 6; yet most of them rely on traditional rule-based methods\\nor provide limited functionalities. The closest prior research to our work is the\\nOCR-D project7, which also tries to build a complete toolkit for DIA. However,\\nsimilar to the platform developed by Neudecker et al. [21], it is designed for\\nanalyzing historical documents, and provides no supports for recent DL models.\\nThe DocumentLayoutAnalysis project8 focuses on processing born-digital PDF\\ndocuments via analyzing the stored PDF data. Repositories like DeepLayout9\\n</td>\n",
       "      <td>2</td>\n",
       "      <td>{'x1': 134.406005859375, 'x2': 482.5574951171875, 'y1': 282.9630432128906, 'y2': 472.37469482421875}</td>\n",
       "      <td>/Users/orm/repos/pixeltable/pixeltable/tests/data/documents/layout-parser-paper.pdf</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "    pos                                               text  page  \\\n",
       "0     0  LayoutParser: A Unified Toolkit for Deep\\nLear...     0   \n",
       "1     1  Zejiang Shen1 (), Ruochen Zhang2, Melissa Dell...     0   \n",
       "2     2  1 Allen Institute for AI\\nshannons@allenai.org...     0   \n",
       "3     3  Abstract. Recent advances in document image an...     0   \n",
       "4     4  Keywords: Document Image Analysis · Deep Learn...     0   \n",
       "5     5                                  1\\nIntroduction\\n     0   \n",
       "6     6  Deep Learning(DL)-based approaches are the sta...     0   \n",
       "7     7         arXiv:2103.15348v2  [cs.CV]  21 Jun 2021\\n     0   \n",
       "8     8                                2\\nZ. Shen et al.\\n     1   \n",
       "9     9  37], layout detection [38, 22], table detectio...     1   \n",
       "10   10  1. An off-the-shelf toolkit for applying DL mo...     1   \n",
       "11   11  2. A rich repository of pre-trained neural net...     1   \n",
       "12   12  3. Comprehensive tools for efficient document ...     1   \n",
       "13   13  4. A DL model hub and community platform for t...     1   \n",
       "14   14  The library implements simple and intuitive Py...     1   \n",
       "15   15  LayoutParser: A Unified Toolkit for DL-Based D...     2   \n",
       "16   16  that require precision, efficiency, and robust...     2   \n",
       "17   17                                  2\\nRelated Work\\n     2   \n",
       "18   36  LayoutParser: A Unified Toolkit for DL-Based D...     4   \n",
       "19   18  Recently, various DL models and datasets have ...     2   \n",
       "\n",
       "                                         bounding_box  \\\n",
       "0   {'x1': 157.6219940185547, 'x2': 457.8017883300...   \n",
       "1   {'x1': 134.8090057373047, 'x2': 480.5463867187...   \n",
       "2   {'x1': 207.23001098632812, 'x2': 408.126861572...   \n",
       "3   {'x1': 162.7790069580078, 'x2': 454.0925292968...   \n",
       "4   {'x1': 162.3459930419922, 'x2': 452.2435302734...   \n",
       "5   {'x1': 134.7650146484375, 'x2': 228.9958801269...   \n",
       "6   {'x1': 134.76499938964844, 'x2': 481.968444824...   \n",
       "7   {'x1': 10.940000534057617, 'x2': 37.6199989318...   \n",
       "8   {'x1': 134.76499938964844, 'x2': 222.097732543...   \n",
       "9   {'x1': 134.00100708007812, 'x2': 482.518798828...   \n",
       "10  {'x1': 138.97097778320312, 'x2': 480.823699951...   \n",
       "11  {'x1': 138.97100830078125, 'x2': 480.894470214...   \n",
       "12  {'x1': 138.97100830078125, 'x2': 480.599212646...   \n",
       "13  {'x1': 138.97100830078125, 'x2': 482.317047119...   \n",
       "14  {'x1': 134.406005859375, 'x2': 482.25592041015...   \n",
       "15  {'x1': 237.13600158691406, 'x2': 480.608276367...   \n",
       "16  {'x1': 134.406005859375, 'x2': 482.53652954101...   \n",
       "17  {'x1': 134.7650146484375, 'x2': 236.8025970458...   \n",
       "18  {'x1': 237.13600158691406, 'x2': 480.608276367...   \n",
       "19  {'x1': 134.406005859375, 'x2': 482.55749511718...   \n",
       "\n",
       "                                             document  \n",
       "0   /Users/orm/repos/pixeltable/pixeltable/tests/d...  \n",
       "1   /Users/orm/repos/pixeltable/pixeltable/tests/d...  \n",
       "2   /Users/orm/repos/pixeltable/pixeltable/tests/d...  \n",
       "3   /Users/orm/repos/pixeltable/pixeltable/tests/d...  \n",
       "4   /Users/orm/repos/pixeltable/pixeltable/tests/d...  \n",
       "5   /Users/orm/repos/pixeltable/pixeltable/tests/d...  \n",
       "6   /Users/orm/repos/pixeltable/pixeltable/tests/d...  \n",
       "7   /Users/orm/repos/pixeltable/pixeltable/tests/d...  \n",
       "8   /Users/orm/repos/pixeltable/pixeltable/tests/d...  \n",
       "9   /Users/orm/repos/pixeltable/pixeltable/tests/d...  \n",
       "10  /Users/orm/repos/pixeltable/pixeltable/tests/d...  \n",
       "11  /Users/orm/repos/pixeltable/pixeltable/tests/d...  \n",
       "12  /Users/orm/repos/pixeltable/pixeltable/tests/d...  \n",
       "13  /Users/orm/repos/pixeltable/pixeltable/tests/d...  \n",
       "14  /Users/orm/repos/pixeltable/pixeltable/tests/d...  \n",
       "15  /Users/orm/repos/pixeltable/pixeltable/tests/d...  \n",
       "16  /Users/orm/repos/pixeltable/pixeltable/tests/d...  \n",
       "17  /Users/orm/repos/pixeltable/pixeltable/tests/d...  \n",
       "18  /Users/orm/repos/pixeltable/pixeltable/tests/d...  \n",
       "19  /Users/orm/repos/pixeltable/pixeltable/tests/d...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paragraph_table.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Operations in Pixeltable\n",
    "\n",
    "In this tutorial, we'll be working with a subset of the COCO dataset (10 samples each for the train, test, and validation splits). To avoid further installs, the tutorial comes pre-packaged with a data file (of JSON records) and a set of images, which we're going to download into a temp directory now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import sys\n",
    "import tempfile\n",
    "import tqdm\n",
    "import urllib.request\n",
    "\n",
    "download_prefix = 'https://raw.github.com/pixeltable/pixeltable/master/docs/source/data'\n",
    "json_data_url = f'{download_prefix}/coco-records.json'\n",
    "\n",
    "records = json.loads(urllib.request.urlopen(json_data_url).read().decode('utf-8'))\n",
    "\n",
    "image_dir = tempfile.mkdtemp()\n",
    "for r in tqdm.tqdm(records, ncols=100, file=sys.stdout):\n",
    "    filename = r['filepath'].split('/')[1]\n",
    "    out_filepath = f'{image_dir}/{filename}'\n",
    "    url = f'{download_prefix}/{r[\"filepath\"]}'\n",
    "    r['img'] = out_filepath\n",
    "    del r['filepath']\n",
    "    img_data = urllib.request.urlopen(url).read()\n",
    "    with open(out_filepath, 'wb') as img_file:\n",
    "        img_file.write(img_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each data record is a dictionary with top-level fields img, tag, metadata, and ground_truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'metadata': {'width': 640, 'height': 480},\n",
       " 'ground_truth': {'detections': [{'attributes': {},\n",
       "    'tags': [],\n",
       "    'label': 'bowl',\n",
       "    'bounding_box': [0.0016875000000000002,\n",
       "     0.3910208333333333,\n",
       "     0.9556093750000001,\n",
       "     0.5954999999999999],\n",
       "    'supercategory': 'kitchen',\n",
       "    'iscrowd': 0},\n",
       "   {'attributes': {},\n",
       "    'tags': [],\n",
       "    'label': 'bowl',\n",
       "    'bounding_box': [0.48707812500000003,\n",
       "     0.008979166666666667,\n",
       "     0.49887499999999996,\n",
       "     0.47641666666666665],\n",
       "    'supercategory': 'kitchen',\n",
       "    'iscrowd': 0},\n",
       "   {'attributes': {},\n",
       "    'tags': [],\n",
       "    'label': 'broccoli',\n",
       "    'bounding_box': [0.39,\n",
       "     0.4776458333333334,\n",
       "     0.49412500000000004,\n",
       "     0.5105833333333334],\n",
       "    'supercategory': 'food',\n",
       "    'iscrowd': 0},\n",
       "   {'attributes': {},\n",
       "    'tags': [],\n",
       "    'label': 'bowl',\n",
       "    'bounding_box': [0.0, 0.02814583333333333, 0.678875, 0.7815],\n",
       "    'supercategory': 'kitchen',\n",
       "    'iscrowd': 0},\n",
       "   {'attributes': {},\n",
       "    'tags': [],\n",
       "    'label': 'orange',\n",
       "    'bounding_box': [0.5878125, 0.08408333333333333, 0.118046875, 0.0969375],\n",
       "    'supercategory': 'food',\n",
       "    'iscrowd': 0},\n",
       "   {'attributes': {},\n",
       "    'tags': [],\n",
       "    'label': 'orange',\n",
       "    'bounding_box': [0.7277812499999999,\n",
       "     0.0811875,\n",
       "     0.090734375,\n",
       "     0.09722916666666667],\n",
       "    'supercategory': 'food',\n",
       "    'iscrowd': 0},\n",
       "   {'attributes': {},\n",
       "    'tags': [],\n",
       "    'label': 'orange',\n",
       "    'bounding_box': [0.60265625,\n",
       "     0.15345833333333334,\n",
       "     0.13128125,\n",
       "     0.14689583333333334],\n",
       "    'supercategory': 'food',\n",
       "    'iscrowd': 0},\n",
       "   {'attributes': {},\n",
       "    'tags': [],\n",
       "    'label': 'orange',\n",
       "    'bounding_box': [0.568828125, 0.0051875, 0.1480625, 0.14806249999999999],\n",
       "    'supercategory': 'food',\n",
       "    'iscrowd': 0}]},\n",
       " 'tag': 'train',\n",
       " 'img': '/var/folders/hb/qd0dztsj43j_mdb6hbl1gzyc0000gn/T/tmpn3ofhfwr/000000000009.jpg'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "records[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Ingest\n",
    "\n",
    "In Pixeltable, all data resides in tables, which themselves can be organized into a directory structure.\n",
    "\n",
    "Let's start by creating an `demo` directory (the `ignore_errors` argument tells it not to raise an exception if the directory already exists):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Pixeltable database at: postgresql://postgres:@/pixeltable?host=/Users/asiegel/.pixeltable/pgdata\n"
     ]
    }
   ],
   "source": [
    "import pixeltable as pxt\n",
    "\n",
    "pxt.create_dir('demo', ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a table\n",
    "\n",
    "A table for our sample data requires a column for each top-level field: `filepath`, `tag`, `metadata`, `ground_truth`.\n",
    "\n",
    "Instead of a file path per image we are going to store the image directly. The table schema is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    'img': pxt.ImageType(nullable=False),\n",
    "    'tag': pxt.StringType(nullable=False),\n",
    "    'metadata': pxt.JsonType(nullable=False),\n",
    "    'ground_truth': pxt.JsonType(nullable=True),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nullable=False` means the values in this column can't be `None`, which Pixeltable will check at data insertion time (`nullable=False` is the default, so we'll leave that out from now on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table `imageops`.\n"
     ]
    }
   ],
   "source": [
    "pxt.drop_table('demo.imageops', ignore_errors=True)\n",
    "t = pxt.create_table('demo.imageops', schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, our table contains no data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "t.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inserting data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to populate `data` with what's in `records`, we call the [`insert()`](https://pixeltable.readthedocs.io/en/latest/api/_autosummary/pixeltable.MutableTable.insert_rows.html) function, which requires a list of rows, each of which is a dictionary mapping column names to column values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.insert(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Pixeltable, images are \"inserted\" as file paths. By default, Pixeltable only stores these paths, not the images themselves, so there is no duplication of storage.\n",
    "\n",
    "Let's look at the first 3 rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do this again, so we can demonstrate `revert()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.insert(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have now loaded our data twice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versioning in Pixeltable\n",
    "\n",
    "Pixeltable maintains a version history for data changes to tables (ie, inserting data and adding/dropping columns). The [`revert()`](https://pixeltable.readthedocs.io/en/latest/api/_autosummary/pixeltable.MutableTable.revert.html) method lets you go back to the preceding version.\n",
    "\n",
    "For our table `data`, since we don't want duplicates, we revert the last update:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.revert()\n",
    "t.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving data\n",
    "\n",
    "Pixeltable allows filtering with the `where()` function.\n",
    "\n",
    "For example, to only look at test data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.where(t.tag == 'test').show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or at data for images that are less than 640 pixels wide:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.where(t.metadata.width < 640).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pixeltable supports the standard comparison operators (`<`, `<=`, `>`, `>=`, `==`) and logical operators (`&` for `and`, `|` for `or`, `~` for `not`). Like in Pandas, logical operators need to be wrapped in parentheses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.where((data.tag == 'test') & (data.metadata.width < 640)).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operations on JSON data\n",
    "\n",
    "The previous example illustrates the use of path expressions against JSON-typed data: `width` is a field in the `metadata` column, which we can simply access as `t.metadata.width`.\n",
    "\n",
    "Another example: retrieve only the bounding boxes from the `ground_truth` column. This will come in handy later when we need to pass those bounding boxes (and not the surrounding dictionary) into a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.select(t.ground_truth.detections['*'].bounding_box).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The field `detections` contains a list, and the `'*'` index indicates that you want all elements in that list. You can also use standard Python list indexing and slicing operations, such as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.select(t.ground_truth.detections[0].bounding_box).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to select only the first bounding box, or"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.select(t.ground_truth.detections[::-1].bounding_box).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to select the bounding boxes in reverse."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Operations on image data\n",
    "\n",
    "Image data has properties `width`, `height`, and `mode`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.select(t.img.width, t.img.height, t.img.mode).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pixeltable also has a number of built-in functions for images (these are a subset of what is available for `PIL.Image.Image`): \n",
    "\n",
    "|Image function||\n",
    "|:---|:---|\n",
    "|[`convert()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.convert)|Returns a converted copy of this image|\n",
    "|[`crop()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.crop)|Returns a rectangular region from this image|\n",
    "|[`effect_spread()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.effect_spread)|Randomly spread pixels in an image|\n",
    "|[`entropy()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.entropy)|Calculates and returns the entropy for the image|\n",
    "|[`filter()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.filter)|Filters this image using the given filter|\n",
    "|[`getbands()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.getbands)|Returns a tuple containing the name of each band in this image|\n",
    "|[`getbbox()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.getbbox)|Calculates the bounding box of the non-zero regions in the image|\n",
    "|[`getchannel()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.getchannel)|Returns an image containing a single channel of the source image|\n",
    "|[`getcolors()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.getcolors)|Returns a list of colors used in this image|\n",
    "|[`getextrema()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.getextrema)|Gets the minimum and maximum pixel values for each band in the image|\n",
    "|[`getpalette()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.getpalette)|Returns the image palette as a list|\n",
    "|[`getpixel()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.getpixel)|Returns the pixel value at a given position|\n",
    "|[`getprojection()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.getprojection)|Get projection to x and y axes|\n",
    "|[`histogram()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.histogram)|Returns a histogram for the image|\n",
    "|[`point()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.point)|Maps this image through a lookup table or function|\n",
    "|[`quantize()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.quantize)|Convert the image to ‘P’ mode with the specified number of colors|\n",
    "|[`reduce()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.reduce)|Returns a copy of the image reduced factor times|\n",
    "|[`remap_palette()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.remap_palette)|Rewrites the image to reorder the palette|\n",
    "|[`resize()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.resize)|Returns a resized copy of this image|\n",
    "|[`rotate()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.rotate)|Returns a rotated copy of this image|\n",
    "|[`transform()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.transform)|Transforms this image|\n",
    "|[`transpose()`](https://pillow.readthedocs.io/en/stable/reference/Image.html#PIL.Image.Image.transpose)|Transpose image (flip or rotate in 90 degree steps)|\n",
    "\n",
    "These functions are invoked in the style of method calls and can be chained, as in this example, which rotates the image by 30 degrees and converts it to black-and-white:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.select(t.img.rotate(30).convert('L')).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Image similarity search\n",
    "\n",
    "When we created the `frame` column we specified `indexed=True`, which creates a vector index of CLIP embeddings for the images in that column. We can take advantage of that with the search function `nearest()`. First, let's get a sample image from `data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img = t.select(t.img).collect()['img'][10]\n",
    "sample_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`collect()` returns a result set, which is a two-dimensional structure you can access with standard Python indexing operations, similar to Pandas. In this case, we're selecting a typical row from the 'img' column, which is a `PIL.Image.Image`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sample_img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look for images like this one, use `nearest()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t.where(t.img.nearest(sample_img)).select(t.img).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the same function to look for images based on text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.where(data.img.nearest('car')).select(data.img).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## User-defined functions\n",
    "\n",
    "User-defined functions let you customize Pixeltable's functionality for your own data.\n",
    "\n",
    "In this example, we're going use a `torchvision` object detection model (Faster R-CNN) against the images in `data` with a user-defined function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torchvision\n",
    "\n",
    "model = torchvision.models.detection.fasterrcnn_mobilenet_v3_large_320_fpn(weights=\"DEFAULT\")\n",
    "_ = model.eval()  # switch to inference mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our function converts the image to PyTorch format and obtains a prediction from the model, which is a list of dictionaries with fields `boxes`, `labels`, and `scores` (one per input image). The fields themselves are PyTorch tensors, and we convert them to standard Python lists (so they become JSON-serializable data):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pxt.udf(return_type=pxt.JsonType(), param_types=[pxt.ImageType()])\n",
    "def detect(img):\n",
    "    t = torchvision.transforms.ToTensor()(img)\n",
    "    t = torchvision.transforms.ConvertImageDtype(torch.float)(t)\n",
    "    result = model([t])[0]\n",
    "    return {\n",
    "        'boxes': result['boxes'].tolist(), 'labels': result['labels'].tolist(), 'scores': result['scores'].tolist()\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [`pt.function`](https://pixeltable.readthedocs.io/en/latest/api/_autosummary/pixeltable.function.html#pixeltable.function) decorator creates a wrapper that tells Pixeltable what arguments the function takes and what it returns. In this case, it takes an image argument and returns a dictionary.\n",
    "\n",
    "We can now use `detect` in the Pixeltable index operator using standard Python function call syntax:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select(data.img, detect(data.img)).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When running this query, Pixeltable evalutes `detect(data.img)` only once per row."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computed columns\n",
    "\n",
    "Being able to run models against any image stored in Pixeltable is very useful, but the runtime cost of model inference makes it impractical to run it every time we want to do something with the model output. In Pixeltable, we can use computed columns to precompute and cache the output of a function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.add_column(detections=detect(data.img))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`detections` is now a column in `data` which holds the model prediction for the `img` column. Like any other column, it is persistent. Pixeltables runs the computation  automatically whenever new data is added to the table. Let's see what `data` looks like now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, the `computed_with` keyword argument can be any Pixeltable expression. In this example, we're making the first label recorded in `detections` available as a separate column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.add_column(first_label=data.detections.labels[0])\n",
    "data.select(data.detections.labels, data.first_label).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whether a computed image column is stored is controlled by the `stored` keyword argument of the `Column` constructor:\n",
    "- when set to `True`, the value is stored explicitly\n",
    "- when set to `False`, the value is always recomputed during a query (and never stored)\n",
    "- the default is `None`, which means that the Pixeltable decides (currently that means that the image won't be stored, but in the future it could take resource consumption into account)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
