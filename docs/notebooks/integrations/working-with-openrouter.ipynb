{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with OpenRouter in Pixeltable\n",
    "\n",
    "Pixeltable's OpenRouter integration enables you to access multiple LLM providers through a unified API via OpenRouter.\n",
    "\n",
    "### Prerequisites\n",
    "- An OpenRouter account with an API key (https://openrouter.ai)\n",
    "\n",
    "### Important Notes\n",
    "\n",
    "- OpenRouter usage may incur costs based on the models you use and your usage volume.\n",
    "- Be mindful of sensitive data and consider security measures when integrating with external services.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First you'll need to install required libraries and enter your OpenRouter API key.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -qU pixeltable openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "if 'OPENROUTER_API_KEY' not in os.environ:\n",
    "    os.environ['OPENROUTER_API_KEY'] = getpass.getpass('Enter your OpenRouter API key:')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a Pixeltable directory to hold the tables for our demo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Pixeltable database at: postgresql+psycopg://postgres:@/pixeltable?host=/Users/asiegel/.pixeltable/pgdata\n",
      "Created directory 'openrouter_demo'.\n"
     ]
    }
   ],
   "source": [
    "import pixeltable as pxt\n",
    "\n",
    "# Remove the 'openrouter_demo' directory and its contents, if it exists\n",
    "pxt.drop_dir('openrouter_demo', force=True)\n",
    "pxt.create_dir('openrouter_demo')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Completions\n",
    "\n",
    "Create a Table: In Pixeltable, create a table with columns to represent your input data and the columns where you want to store the results from OpenRouter.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table 'chat'.\n",
      "Added 0 column values with 0 errors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "No rows affected."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pixeltable.functions import openrouter\n",
    "\n",
    "# Create a table in Pixeltable and add a computed column that calls OpenRouter\n",
    "t = pxt.create_table('openrouter_demo.chat', {'input': pxt.String})\n",
    "\n",
    "messages = [{'role': 'user', 'content': t.input}]\n",
    "\n",
    "t.add_computed_column(output=openrouter.chat_completions(\n",
    "    messages=messages,\n",
    "    model='anthropic/claude-sonnet-4',\n",
    "    model_kwargs={\n",
    "        # Optional dict with parameters compatible with the model\n",
    "        'max_tokens': 300,\n",
    "        'temperature': 0.7\n",
    "    }\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 0 column values with 0 errors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "No rows affected."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Parse the response into a new column\n",
    "t.add_computed_column(response=t.output.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting rows into `chat`: 2 rows [00:00, 166.89 rows/s]\n",
      "Inserted 2 rows with 0 errors.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>input</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>How many species of felids have been classified?</td>\n",
       "      <td>There are currently 40 recognized species of felids (members of the family Felidae), which includes all wild cats as well as domestic cats. These range from small cats like the rusty-spotted cat to large cats like tigers and lions. This classification can occasionally be updated as new research provides better understanding of cat taxonomy.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Can you make me a coffee?</td>\n",
       "      <td>I cannot make you a coffee as I am an AI language model - I don&#x27;t have physical form or ability to interact with the physical world. I can, however, give you instructions on how to make different types of coffee, recommend coffee recipes, or provide information about coffee in general. Would you like any of those instead?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "                                              input  \\\n",
       "0  How many species of felids have been classified?   \n",
       "1                         Can you make me a coffee?   \n",
       "\n",
       "                                            response  \n",
       "0  There are currently 40 recognized species of f...  \n",
       "1  I cannot make you a coffee as I am an AI langu...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Start a conversation\n",
    "t.insert([\n",
    "    {'input': 'How many species of felids have been classified?'},\n",
    "    {'input': 'Can you make me a coffee?'}\n",
    "])\n",
    "t.select(t.input, t.response).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Different Models\n",
    "\n",
    "One of OpenRouter's key benefits is easy access to models from multiple providers. Let's create a table that compares responses from Anthropic Claude, OpenAI GPT-4, and Meta Llama.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table 'compare_models'.\n",
      "Added 0 column values with 0 errors.\n",
      "Added 0 column values with 0 errors.\n",
      "Added 0 column values with 0 errors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "No rows affected."
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a table to compare different models\n",
    "compare_t = pxt.create_table('openrouter_demo.compare_models', {'prompt': pxt.String})\n",
    "\n",
    "messages = [{'role': 'user', 'content': compare_t.prompt}]\n",
    "\n",
    "# Add responses from different models\n",
    "compare_t.add_computed_column(\n",
    "    claude=openrouter.chat_completions(\n",
    "        messages=messages,\n",
    "        model='anthropic/claude-sonnet-4',\n",
    "        model_kwargs={'max_tokens': 150}\n",
    "    ).choices[0].message.content\n",
    ")\n",
    "\n",
    "compare_t.add_computed_column(\n",
    "    gpt4=openrouter.chat_completions(\n",
    "        messages=messages,\n",
    "        model='openai/gpt-4o-mini',\n",
    "        model_kwargs={'max_tokens': 150}\n",
    "    ).choices[0].message.content\n",
    ")\n",
    "\n",
    "compare_t.add_computed_column(\n",
    "    llama=openrouter.chat_completions(\n",
    "        messages=messages,\n",
    "        model='meta-llama/llama-3.3-70b-instruct',\n",
    "        model_kwargs={'max_tokens': 150}\n",
    "    ).choices[0].message.content\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting rows into `compare_models`: 1 rows [00:00, 131.36 rows/s]\n",
      "Inserted 1 row with 0 errors.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>prompt</th>\n",
       "      <th>claude</th>\n",
       "      <th>gpt4</th>\n",
       "      <th>llama</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Explain quantum entanglement in one sentence.</td>\n",
       "      <td>Quantum entanglement is a phenomenon where two or more particles become so deeply connected that the quantum state of each particle cannot be described independently, even when separated by large distances.</td>\n",
       "      <td>Quantum entanglement is a phenomenon in which two or more particles become interconnected in such a way that the state of one particle instantaneously influences the state of the other, regardless of the distance separating them.</td>\n",
       "      <td>Quantum entanglement is a phenomenon in which two or more particles become interconnected in such a way that their properties, such as spin or polarization, are correlated, regardless of the distance between them.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "                                          prompt  \\\n",
       "0  Explain quantum entanglement in one sentence.   \n",
       "\n",
       "                                              claude  \\\n",
       "0  Quantum entanglement is a phenomenon where two...   \n",
       "\n",
       "                                                gpt4  \\\n",
       "0  Quantum entanglement is a phenomenon in which ...   \n",
       "\n",
       "                                               llama  \n",
       "0  Quantum entanglement is a phenomenon in which ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Insert a prompt and compare responses\n",
    "compare_t.insert([{'prompt': 'Explain quantum entanglement in one sentence.'}])\n",
    "compare_t.select(compare_t.prompt, compare_t.claude, compare_t.gpt4, compare_t.llama).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Features: Provider Routing\n",
    "\n",
    "OpenRouter allows you to specify provider preferences for fallback behavior and cost optimization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table 'routing'.\n",
      "Added 0 column values with 0 errors.\n",
      "Added 0 column values with 0 errors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "No rows affected."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a table with provider routing\n",
    "routing_t = pxt.create_table('openrouter_demo.routing', {'input': pxt.String})\n",
    "\n",
    "messages = [{'role': 'user', 'content': routing_t.input}]\n",
    "routing_t.add_computed_column(\n",
    "    output=openrouter.chat_completions(\n",
    "        messages=messages,\n",
    "        model='anthropic/claude-sonnet-4',\n",
    "        model_kwargs={'max_tokens': 300},\n",
    "        # Specify provider preferences\n",
    "        provider={\n",
    "            'order': ['Anthropic', 'OpenAI'],  # Try Anthropic first, then OpenAI\n",
    "            'allow_fallbacks': True\n",
    "        }\n",
    "    )\n",
    ")\n",
    "\n",
    "routing_t.add_computed_column(response=routing_t.output.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting rows into `routing`: 1 rows [00:00, 142.11 rows/s]\n",
      "Inserted 1 row with 0 errors.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>input</th>\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>What are the primary colors?</td>\n",
       "      <td>The primary colors are:\n",
       "\n",
       "1. Red\n",
       "2. Blue\n",
       "3. Yellow\n",
       "\n",
       "These are called primary colors because they cannot be created by mixing other colors together, and they can be combined to create all other colors. For example:\n",
       "- Red + Blue = Purple\n",
       "- Blue + Yellow = Green\n",
       "- Red + Yellow = Orange</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "                          input  \\\n",
       "0  What are the primary colors?   \n",
       "\n",
       "                                            response  \n",
       "0  The primary colors are:\\n\\n1. Red\\n2. Blue\\n3....  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "routing_t.insert([{'input': 'What are the primary colors?'}])\n",
    "routing_t.select(routing_t.input, routing_t.response).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Features: Context Window Optimization\n",
    "\n",
    "OpenRouter supports transforms like 'middle-out' to optimize handling of long contexts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created table 'transforms'.\n",
      "Added 0 column values with 0 errors.\n",
      "Added 0 column values with 0 errors.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "No rows affected."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a table with transforms for long context optimization\n",
    "transform_t = pxt.create_table('openrouter_demo.transforms', {'long_context': pxt.String})\n",
    "\n",
    "messages = [{'role': 'user', 'content': transform_t.long_context}]\n",
    "transform_t.add_computed_column(\n",
    "    output=openrouter.chat_completions(\n",
    "        messages=messages,\n",
    "        model='openai/gpt-4o-mini',\n",
    "        model_kwargs={'max_tokens': 200},\n",
    "        # Apply middle-out transform for better long context handling\n",
    "        transforms=['middle-out']\n",
    "    )\n",
    ")\n",
    "\n",
    "transform_t.add_computed_column(response=transform_t.output.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserting rows into `transforms`: 1 rows [00:00, 123.46 rows/s]\n",
      "Inserted 1 row with 0 errors.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>response</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>The main AI developments mentioned are:\n",
       "\n",
       "1. Machine learning algorithms that detect patterns in data.\n",
       "2. Deep learning, specifically in computer vision and natural language processing.\n",
       "3. Advancements in reinforcement learning.\n",
       "4. Developments in generative models.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "                                            response\n",
       "0  The main AI developments mentioned are:\\n\\n1. ..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example with longer context\n",
    "long_text = \"\"\"\n",
    "Artificial intelligence has transformed many industries. Machine learning algorithms \n",
    "can now detect patterns in data that humans might miss. Deep learning has revolutionized \n",
    "computer vision and natural language processing. The future of AI looks promising with \n",
    "developments in areas like reinforcement learning and generative models.\n",
    "\n",
    "Question: What are the main AI developments mentioned?\n",
    "\"\"\"\n",
    "\n",
    "transform_t.insert([{'long_context': long_text}])\n",
    "transform_t.select(transform_t.response).head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learn More\n",
    "\n",
    "To learn more about advanced techniques like RAG operations in Pixeltable, check out the [RAG Operations in Pixeltable](https://docs.pixeltable.com/notebooks/use-cases/rag-operations) tutorial.\n",
    "\n",
    "For more information about OpenRouter's features and available models, visit:\n",
    "- [OpenRouter Documentation](https://openrouter.ai/docs)\n",
    "- [Available Models](https://openrouter.ai/models)\n",
    "\n",
    "If you have any questions, don't hesitate to reach out.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
