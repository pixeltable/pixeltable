{
  "@context": "https://pixeltable.com/learn",
  "@type": "Tutorial",
  "@id": "anthropic-integration",
  "github_url": "https://github.com/pixeltable/pixeltable/blob/release/docs/notebooks/integrations/working-with-anthropic.ipynb",
  "title": "Working with Anthropic Claude in Pixeltable",
  "objective": "Master Anthropic Claude integration patterns focusing on conversational AI and advanced reasoning capabilities",
  "difficulty": "intermediate",
  "categories": ["api-integrations", "llm-chat", "conversational-ai", "anthropic-claude"],
  "prerequisites": ["pixeltable-basics", "api-key-management"],
  "imports_required": [
    "pixeltable as pxt",
    "pixeltable.functions.anthropic",
    "os",
    "getpass"
  ],
  "performance_notes": {
    "typical_runtime": "1-3 minutes for single queries",
    "resource_requirements": "Network connection, Anthropic API credits",
    "bottlenecks": ["API rate limits", "Claude processing time", "network latency"]
  },
  "key_learnings": [
    "Anthropic Claude uses 'messages' API similar to OpenAI but with unique parameters",
    "System prompts are configured via model_kwargs rather than message format",
    "Claude models excel at detailed reasoning and historical information",
    "Temperature, top_k, and top_p parameters provide fine control over responses",
    "Claude tends to be more verbose and detailed than other models",
    "Cost structure differs from OpenAI but follows similar token-based pricing",
    "Response parsing follows similar nested structure patterns"
  ],
  "relationships": {
    "builds_on": ["table-creation", "computed-columns", "api-key-management"],
    "enables": ["advanced-reasoning-workflows", "detailed-analysis"],
    "see_also": ["openai-integration", "gemini-integration"],
    "contrasts_with": ["local-model-serving", "cost-optimized-solutions"]
  },
  "steps": [
    {
      "number": 1,
      "section_title": "Installation and Dependencies",
      "intent": "Install Anthropic client and Pixeltable integration",
      "code": "%pip install -qU pixeltable anthropic",
      "imports_used": [],
      "explanation": "Install both Pixeltable and official Anthropic client library",
      "actual_output": "[Installation output]",
      "output_summary": "Libraries installed successfully",
      "output_type": "text",
      "learns": ["anthropic-client-installation"],
      "reinforces": ["dependency-management"],
      "gotchas": ["Must use official 'anthropic' package name"],
      "performance": {
        "execution_time": "30-60s",
        "scaling": "O(1) per installation",
        "optimization": "demo"
      },
      "alternatives": {
        "description": "Can install via conda or poetry with specific version pins",
        "when_to_use": "When version stability is critical"
      },
      "state_after": {
        "tables": [],
        "views": [],
        "variables": [],
        "models_loaded": []
      },
      "pattern_refs": ["secure-api-setup"]
    },
    {
      "number": 2,
      "section_title": "API Key Configuration",
      "intent": "Securely configure Anthropic API credentials",
      "code": "import os\nimport getpass\n\nif 'ANTHROPIC_API_KEY' not in os.environ:\n    os.environ['ANTHROPIC_API_KEY'] = getpass.getpass('Anthropic API Key:')",
      "imports_used": ["os", "getpass"],
      "explanation": "Uses identical secure pattern as OpenAI but with Anthropic-specific environment variable",
      "actual_output": "API key set in environment",
      "output_summary": "Anthropic credentials configured securely",
      "output_type": "none",
      "learns": ["anthropic-credentials"],
      "reinforces": ["secure-credential-handling", "environment-variables"],
      "gotchas": ["API key must be obtained from Anthropic Console"],
      "performance": {
        "execution_time": "<1s",
        "scaling": "O(1)",
        "optimization": "production"
      },
      "alternatives": {
        "description": "Can use .env files or cloud secret management services",
        "when_to_use": "In production environments with centralized secret management"
      },
      "state_after": {
        "tables": [],
        "views": [],
        "variables": ["ANTHROPIC_API_KEY"],
        "models_loaded": []
      },
      "pattern_refs": ["secure-api-setup", "environment-config"]
    },
    {
      "number": 3,
      "section_title": "Workspace Setup",
      "intent": "Create organized directory for Anthropic demo",
      "code": "import pixeltable as pxt\n\n# Remove the 'anthropic_demo' directory and its contents, if it exists\npxt.drop_dir('anthropic_demo', force=True)\npxt.create_dir('anthropic_demo')",
      "imports_used": ["pixeltable as pxt"],
      "explanation": "Clean workspace setup specific to Anthropic integration",
      "actual_output": "Connected to Pixeltable database at: postgresql+psycopg://postgres:@/pixeltable?host=/Users/asiegel/.pixeltable/pgdata\nCreated directory `anthropic_demo`.",
      "output_summary": "Anthropic demo directory created",
      "output_type": "text",
      "learns": ["provider-specific-organization"],
      "reinforces": ["directory-management", "workspace-organization"],
      "gotchas": ["Directory names should be descriptive for multi-provider projects"],
      "performance": {
        "execution_time": "<1s",
        "scaling": "O(1)",
        "optimization": "demo"
      },
      "alternatives": {
        "description": "Can use date-stamped directories for experiment tracking",
        "when_to_use": "When running comparative experiments across providers"
      },
      "state_after": {
        "tables": [],
        "views": [],
        "variables": ["ANTHROPIC_API_KEY"],
        "models_loaded": []
      },
      "pattern_refs": ["workspace-setup", "provider-organization"]
    },
    {
      "number": 4,
      "section_title": "Claude Messages Integration",
      "intent": "Configure Anthropic Claude with system prompts and parameters",
      "code": "from pixeltable.functions import anthropic\n\n# Create a table in Pixeltable and add a computed column that calls Anthropic\n\nt = pxt.create_table('anthropic_demo.chat', {'input': pxt.String})\n\nmsgs = [{'role': 'user', 'content': t.input}]\nt.add_computed_column(output=anthropic.messages(\n    messages=msgs,\n    model='claude-3-haiku-20240307',\n    max_tokens=300,\n    model_kwargs={\n        # Optional dict with parameters for the Anthropic API\n        'system': 'Respond to the prompt with detailed historical information.',\n        'top_k': 40,\n        'top_p': 0.9,\n        'temperature': 0.7\n    }\n))",
      "imports_used": ["pixeltable as pxt", "pixeltable.functions.anthropic"],
      "explanation": "Sets up Claude with system prompt and detailed parameter configuration",
      "actual_output": "Created table `chat`.\nAdded 0 column values with 0 errors.",
      "output_summary": "Claude chat table configured with system prompt",
      "output_type": "text",
      "learns": ["claude-messages-api", "system-prompt-configuration", "anthropic-parameters"],
      "reinforces": ["computed-columns", "api-integration"],
      "gotchas": ["System prompt goes in model_kwargs, not messages", "Model names include specific version dates"],
      "performance": {
        "execution_time": "<1s for setup",
        "scaling": "O(1) table creation",
        "optimization": "production"
      },
      "alternatives": {
        "description": "Can use claude-3-sonnet or claude-3-opus for different capabilities",
        "when_to_use": "Based on reasoning complexity vs cost requirements"
      },
      "state_after": {
        "tables": ["anthropic_demo.chat"],
        "views": [],
        "variables": ["ANTHROPIC_API_KEY"],
        "models_loaded": []
      },
      "pattern_refs": ["api-integration", "system-prompt-pattern", "claude-configuration"]
    },
    {
      "number": 5,
      "section_title": "Response Parsing",
      "intent": "Extract content from Anthropic's response structure",
      "code": "# Parse the response into a new column\nt.add_computed_column(response=t.output.content[0].text)",
      "imports_used": ["pixeltable as pxt", "pixeltable.functions.anthropic"],
      "explanation": "Navigates Anthropic's response structure to extract text content",
      "actual_output": "Added 0 column values with 0 errors.",
      "output_summary": "Response parsing configured",
      "output_type": "text",
      "learns": ["anthropic-response-structure"],
      "reinforces": ["response-parsing", "json-navigation"],
      "gotchas": ["Anthropic uses 'content[0].text' vs OpenAI's 'choices[0].message.content'"],
      "performance": {
        "execution_time": "<1s",
        "scaling": "O(1)",
        "optimization": "production"
      },
      "alternatives": {
        "description": "Can access role and other metadata from response",
        "when_to_use": "When analyzing conversation patterns or response metadata"
      },
      "state_after": {
        "tables": ["anthropic_demo.chat"],
        "views": [],
        "variables": ["ANTHROPIC_API_KEY"],
        "models_loaded": []
      },
      "pattern_refs": ["response-parsing", "anthropic-response-navigation"]
    },
    {
      "number": 6,
      "section_title": "Historical Query Execution",
      "intent": "Test Claude's detailed reasoning with historical question",
      "code": "# Start a conversation\nt.insert(input=\"What was the outcome of the 1904 US Presidential election?\")\nt.select(t.input, t.response).show()",
      "imports_used": ["pixeltable as pxt", "pixeltable.functions.anthropic"],
      "explanation": "Tests Claude's historical knowledge and detailed response capabilities",
      "actual_output": "Computing cells: 100%|████████████████████████████████████████████| 3/3 [00:01<00:00,  1.54 cells/s]\nInserting rows into `chat`: 1 rows [00:00, 149.28 rows/s]\nComputing cells: 100%|████████████████████████████████████████████| 3/3 [00:01<00:00,  1.53 cells/s]\nInserted 1 row with 0 errors.\n\n                                               input  \\\n0  What was the outcome of the 1904 US Presidenti...   \n\n                                            response  \n0  The outcome of the 1904 US presidential electi...",
      "output_summary": "Claude provides detailed historical analysis",
      "output_type": "table",
      "learns": ["claude-reasoning-capabilities", "historical-analysis"],
      "reinforces": ["insert-patterns", "computed-column-execution"],
      "gotchas": ["Claude responses tend to be longer and more detailed", "System prompt affects response style"],
      "performance": {
        "execution_time": "2-4s for API call",
        "scaling": "O(n) with complexity of reasoning required",
        "optimization": "production"
      },
      "alternatives": {
        "description": "Can adjust max_tokens for shorter responses",
        "when_to_use": "When conciseness is more important than detail"
      },
      "state_after": {
        "tables": ["anthropic_demo.chat"],
        "views": [],
        "variables": ["ANTHROPIC_API_KEY"],
        "models_loaded": []
      },
      "pattern_refs": ["claude-reasoning", "detailed-analysis-pattern"]
    }
  ],
  "patterns": [
    {
      "name": "system_prompt_pattern",
      "description": "Configuration of system-level instructions for AI behavior",
      "confidence": "high",
      "frequency": 2,
      "first_seen": "anthropic-integration",
      "code_template": "model_kwargs={\n    'system': 'System instruction here',\n    'temperature': 0.7,\n    'max_tokens': 300\n}",
      "parameters": {
        "system": "System-level instruction for AI behavior",
        "temperature": "Response creativity (0.0-1.0)",
        "max_tokens": "Maximum response length"
      },
      "variations": [
        {
          "name": "role_specific_system",
          "difference": "System prompt defines specific role or expertise",
          "code": "'system': 'You are a helpful research assistant specializing in historical analysis.'"
        }
      ],
      "prerequisites": ["api-integration"],
      "enables": ["consistent-ai-behavior", "specialized-responses"],
      "performance_impact": "Low - affects response quality not speed",
      "reusable": true,
      "production_ready": true
    },
    {
      "name": "claude_configuration",
      "description": "Anthropic-specific parameter configuration for optimal Claude performance",
      "confidence": "high",
      "frequency": 1,
      "first_seen": "anthropic-integration",
      "code_template": "anthropic.messages(\n    messages=msgs,\n    model='claude-3-haiku-20240307',\n    max_tokens=300,\n    model_kwargs={\n        'system': 'System prompt',\n        'top_k': 40,\n        'top_p': 0.9,\n        'temperature': 0.7\n    }\n)",
      "parameters": {
        "model": "Claude model version with date identifier",
        "top_k": "Top-k sampling parameter (1-100)",
        "top_p": "Nucleus sampling parameter (0.0-1.0)",
        "temperature": "Response randomness (0.0-1.0)"
      },
      "variations": [
        {
          "name": "reasoning_focused",
          "difference": "Lower temperature for more deterministic reasoning",
          "code": "'temperature': 0.1, 'top_p': 0.8"
        }
      ],
      "prerequisites": ["anthropic-api-setup"],
      "enables": ["fine-tuned-claude-responses", "consistent-reasoning"],
      "performance_impact": "Medium - affects response time and quality",
      "reusable": true,
      "production_ready": true
    },
    {
      "name": "anthropic_response_navigation",
      "description": "Pattern for extracting content from Anthropic's nested response format",
      "confidence": "high",
      "frequency": 1,
      "first_seen": "anthropic-integration",
      "code_template": "t.add_computed_column(response=t.output.content[0].text)",
      "parameters": {
        "output_column": "Column containing Anthropic API response",
        "content_index": "Index of content block (usually 0)"
      },
      "variations": [
        {
          "name": "multi_content_extraction",
          "difference": "Handle multiple content blocks in response",
          "code": "t.add_computed_column(all_content=[content.text for content in t.output.content])"
        }
      ],
      "prerequisites": ["anthropic-integration"],
      "enables": ["response-processing", "content-extraction"],
      "performance_impact": "Low - simple field access",
      "reusable": true,
      "production_ready": true
    },
    {
      "name": "detailed_analysis_pattern",
      "description": "Leveraging Claude's strength in detailed reasoning and analysis",
      "confidence": "medium",
      "frequency": 1,
      "first_seen": "anthropic-integration",
      "code_template": "model_kwargs={\n    'system': 'Provide detailed analysis with supporting evidence.',\n    'max_tokens': 500,  # Allow longer responses\n    'temperature': 0.3  # More focused reasoning\n}",
      "parameters": {
        "analysis_type": "Type of analysis requested (historical, technical, etc.)",
        "max_tokens": "Higher limit for detailed responses",
        "temperature": "Lower for more focused reasoning"
      },
      "variations": [
        {
          "name": "comparative_analysis",
          "difference": "System prompt focuses on comparison",
          "code": "'system': 'Compare and contrast the given topics with detailed evidence.'"
        }
      ],
      "prerequisites": ["system-prompt-configuration"],
      "enables": ["in-depth-research", "comprehensive-analysis"],
      "performance_impact": "High - longer responses and processing time",
      "reusable": true,
      "production_ready": true
    }
  ],
  "common_errors": [
    {
      "error_type": "Invalid API Key",
      "frequency": "common",
      "cause": "Missing or malformed ANTHROPIC_API_KEY environment variable",
      "symptoms": ["Authentication failed", "401 error"],
      "solution": {
        "quick_fix": "Verify API key format starts with 'sk-ant-'",
        "proper_fix": "Implement API key validation before making calls"
      },
      "prevention": "Test API key validity during setup",
      "example": "Using OpenAI format key instead of Anthropic format",
      "first_seen": "anthropic-integration#2"
    },
    {
      "error_type": "Model Not Found",
      "frequency": "occasional",
      "cause": "Incorrect model name or version",
      "symptoms": ["Model not found error", "Invalid model parameter"],
      "solution": {
        "quick_fix": "Verify exact model name with version date",
        "proper_fix": "Use model listing API to get current available models"
      },
      "prevention": "Reference official Anthropic documentation for model names",
      "example": "Using 'claude-3-haiku' instead of 'claude-3-haiku-20240307'",
      "first_seen": "anthropic-integration#4"
    },
    {
      "error_type": "System Prompt Formatting",
      "frequency": "occasional",
      "cause": "Placing system prompt in messages instead of model_kwargs",
      "symptoms": ["System prompt ignored", "Unexpected behavior"],
      "solution": {
        "quick_fix": "Move system prompt to model_kwargs dictionary",
        "proper_fix": "Create consistent system prompt configuration pattern"
      },
      "prevention": "Follow Anthropic-specific message formatting guidelines",
      "example": "Adding system message to messages array",
      "first_seen": "anthropic-integration#4"
    }
  ],
  "test_questions": [
    {
      "question": "How does Anthropic's system prompt configuration differ from OpenAI's?",
      "answer": "Anthropic uses 'system' parameter in model_kwargs, OpenAI uses system message in messages array",
      "difficulty": "intermediate"
    },
    {
      "question": "What are the key parameters for controlling Claude's reasoning style?",
      "answer": "temperature (creativity), top_k (vocabulary filtering), top_p (nucleus sampling), system prompt (behavior guidance)",
      "difficulty": "intermediate"
    },
    {
      "question": "Why might you choose Claude over other LLMs for certain tasks?",
      "answer": "Claude excels at detailed reasoning, analysis, and providing comprehensive explanations with supporting evidence",
      "difficulty": "advanced"
    },
    {
      "question": "How do you extract text content from Anthropic's API response?",
      "answer": "Access t.output.content[0].text (different from OpenAI's choices[0].message.content)",
      "difficulty": "beginner"
    }
  ],
  "production_tips": [
    {
      "tip": "Optimize system prompts for consistent behavior",
      "impact": "More predictable and useful responses",
      "implementation": "Create reusable system prompt templates for different use cases",
      "trade_offs": "May reduce response creativity",
      "example": "'system': 'You are a technical documentation expert. Provide clear, accurate explanations with examples.'"
    },
    {
      "tip": "Use appropriate Claude model for task complexity",
      "impact": "Better cost-performance balance",
      "implementation": "Haiku for simple tasks, Sonnet for balanced work, Opus for complex reasoning",
      "trade_offs": "Higher-tier models cost more but provide better quality",
      "example": "Use Haiku for summarization, Opus for complex analysis"
    },
    {
      "tip": "Configure max_tokens based on expected response length",
      "impact": "Prevents truncated responses and controls costs",
      "implementation": "Set higher limits for analysis tasks, lower for quick responses",
      "trade_offs": "Longer max_tokens = higher potential costs",
      "example": "300 tokens for summaries, 1000+ for detailed analysis"
    },
    {
      "tip": "Leverage Claude's strength in reasoning tasks",
      "impact": "Better results for complex analytical work",
      "implementation": "Design prompts that request step-by-step reasoning and evidence",
      "trade_offs": "Longer response times and higher token usage",
      "example": "Ask for 'detailed analysis with supporting evidence' rather than simple answers"
    }
  ],
  "pattern_maturity": {
    "novel_patterns": 1,
    "established_patterns": 3,
    "total_patterns": 4
  },
  "cookies": "🍪 Claude is like that overachieving student who always writes detailed essays - sometimes you get a PhD dissertation when you asked for a summary, but boy does it know its stuff!"
}