---
title: "Audio"
description: "Build an audio-to-text knowledge base with search capabilities"
icon: "microphone"
---

# Building a Audio Search Workflow

Pixeltable lets you build audio search workflows in two phases:
1. Define your processing workflow (once)
2. Query your knowledge base (anytime)

<Steps>
  <Step title="Install Dependencies">
    ```bash
    pip install pixeltable tiktoken openai openai-whisper spacy sentence-transformers
    python -m spacy download en_core_web_sm
    ```
  </Step>

  <Step title="Define Your Workflow" icon="diagram-project">
    Create `table.py`:
    ```python
    import pixeltable as pxt
    from pixeltable.functions import whisper
    from pixeltable.functions.huggingface import sentence_transformer
    from pixeltable.iterators.string import StringSplitter
    import spacy

    # Initialize spaCy
    nlp = spacy.load("en_core_web_sm")

    # Initialize app structure
    pxt.drop_dir("audio_search", force=True)
    pxt.create_dir("audio_search")

    # Create audio table
    audio_t = pxt.create_table(
        "audio_search.audio", 
        {"audio_file": pxt.Audio}
    )

    # Add transcription workflow
    audio_t.add_computed_column(
        transcription=whisper.transcribe(
            audio=audio_t.audio_file, 
            model="base.en"
        )
    )

    # Create sentence-level view
    sentences_view = pxt.create_view(
        "audio_search.audio_sentence_chunks",
        audio_t,
        iterator=StringSplitter.create(
            text=audio_t.transcription.text, 
            separators="sentence"
        )
    )

    # Configure embedding model
    embed_model = sentence_transformer.using(
        model_id="intfloat/e5-large-v2"
    )

    # Add search capability
    sentences_view.add_embedding_index(
        column="text", 
        string_embed=embed_model
    )
```
  </Step>

  <Step title="Use Your Workflow" icon="play">
    Create `app.py`:
    ```python
    import pixeltable as pxt

    # Connect to your tables and views
    audio_t = pxt.get_table("audio_search.audio")
    sentences_view = pxt.get_table("audio_search.audio_sentence_chunks")

    # Add audio files to the knowledge base
    audio_t.insert([{
        "audio_file": "https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/10-minute%20tour%20of%20Pixeltable.mp3"
    }])

    # Perform search
    query_text = "What are the key features of Pixeltable?"
    min_similarity = 0.8
    sim = sentences_view.text.similarity(query_text)
    result = (
        sentences_view.where(sim >= min_similarity)
        .order_by(sim, asc=False)
        .select(sentences_view.text, sim=sim)
        .collect()
    )

    # Print results
    for i in result:
        print(f"Similarity: {i['sim']:.3f}")
        print(f"Text: {i['text']}\n")
```
  </Step>
</Steps>

## What Makes This Different?

<CardGroup cols={1}>
  <Card title="Automatic Processing" icon="gears">
    Workflow handles transcription and embedding automatically:
    ```python
    audio_t.add_computed_column(
        transcription=whisper.transcribe(
            audio=audio_t.audio_file
        )
    )
    ```
  </Card>

  <Card title="Smart Chunking" icon="scissors">
    Intelligent sentence splitting using spaCy:
    ```python
    iterator=StringSplitter.create(
        text=audio_t.transcription.text,
        separators="sentence"
    )
    ```
  </Card>

  <Card title="Vector Search" icon="magnifying-glass">
    Fast search using E5 embeddings:
    ```python
    query_text = "What are the key features of Pixeltable?"
    min_similarity = 0.8
    sim = sentences_view.text.similarity(query_text)
    result = (
        sentences_view.where(sim >= min_similarity)
        .order_by(sim, asc=False)
        .select(sentences_view.text, sim=sim)
        .collect()
    )
    ```
  </Card>
</CardGroup>

## Workflow Components

<AccordionGroup>
  <Accordion title="Audio Transcription" icon="waveform">
    Uses OpenAI's Whisper for audio transcription:
    - Supports multiple audio formats
    - Automatic language detection
    - High accuracy transcription
    - Configurable model sizes (base.en, small.en, etc.)
  </Accordion>

  <Accordion title="Text Processing" icon="text-size">
    Splits transcriptions into units:
    - Sentence-level segmentation using spaCy
    - Maintains context boundaries
    - Natural language processing
    - Configurable chunking strategies
  </Accordion>

  <Accordion title="Vector Search" icon="database">
    Implements search using E5 embeddings:
    - High-quality vector representations
    - Fast similarity search
    - Configurable top-k retrieval
    - Similarity scores for ranking
  </Accordion>
</AccordionGroup>

## Advanced Usage

### Custom Search Functions

You can create custom search functions with different parameters:

```python
@pxt.query
def search_with_threshold(query_text: str, min_similarity: float):
    sim = sentences_view.text.similarity(query_text)
    return (
        sentences_view.where(sim >= min_similarity)
        .order_by(sim, asc=False)
        .select(sentences_view.text, sim=sim)
    )
```

### Batch Processing

Process multiple audio files in batch:

```python
audio_files = [
    "s3://your-bucket/audio1.mp3",
    "s3://your-bucket/audio2.mp3",
    "s3://your-bucket/audio3.mp3"
]

audio_t.insert({"audio_file": f} for f in audio_files)
```

### Different Embedding Models

You can use different sentence transformer models:

```python
# Alternative embedding models
embed_model = sentence_transformer.using(
    model_id="sentence-transformers/all-mpnet-base-v2"
)
# or
embed_model = sentence_transformer.using(
    model_id="sentence-transformers/all-MiniLM-L6-v2"
)
```