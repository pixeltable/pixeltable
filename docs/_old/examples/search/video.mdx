---
title: "Video"
description: "Build a multimodal video search workflow with Pixeltable"
icon: "video"
mode: "wide"
---

# Building a Multimodal Video Search Workflow

Pixeltable lets you build comprehensive video search workflows combining both audio and visual content:
1. Process both audio and visual content
2. Query your knowledge base by text or visual concepts

<Steps>
  <Step title="Install Dependencies">
    ```bash
    pip install pixeltable openai tiktoken openai-whisper spacy sentence-transformers
    ```
  </Step>

  <Step title="Define Your Workflow" icon="diagram-project">
    Create `table.py`:
    ```python
    import pixeltable as pxt
    from pixeltable.functions import openai
    from pixeltable.functions.huggingface import sentence_transformer
    from pixeltable.functions.video import extract_audio
    from pixeltable.iterators import AudioSplitter, FrameIterator
    from pixeltable.iterators.string import StringSplitter
    from pixeltable.functions.openai import vision

    # Define the embedding model once for reuse
    EMBED_MODEL = sentence_transformer.using(model_id='intfloat/e5-large-v2')

    # Set up directory and table name
    directory = 'video_index'
    table_name = f'{directory}.video'

    # Create video table
    pxt.create_dir(directory, if_exists='replace_force')

    video_index = pxt.create_table(
        table_name, 
        {'video': pxt.Video, 'uploaded_at': pxt.Timestamp}
    )

    video_index.add_computed_column(
        audio_extract=extract_audio(video_index.video, format='mp3')
    ) 

    # Create view for frames
    frames_view = pxt.create_view(
        f'{directory}.video_frames',
        video_index,
        iterator=FrameIterator.create(
            video=video_index.video,
            fps=1
        )
    )

    # Create a column for image description using OpenAI gpt-4o-mini
    frames_view.add_computed_column(
        image_description=vision(
            prompt="Provide quick caption for the image.",
            image=frames_view.frame,
            model="gpt-4o-mini"
        )
    )    

    # Create embedding index for image description
    frames_view.add_embedding_index('image_description', string_embed=EMBED_MODEL)    

    # Create view for audio chunks
    chunks_view = pxt.create_view(
        f'{directory}.video_chunks',
        video_index,
        iterator=AudioSplitter.create(
            audio=video_index.audio_extract,
            chunk_duration_sec=30.0,
            overlap_sec=2.0,
            min_chunk_duration_sec=5.0
        )
    )

    # Audio-to-text for chunks
    chunks_view.add_computed_column(
        transcription=openai.transcriptions(
          audio=chunks_view.audio_chunk, model='whisper-1'
        )
    )

    # Create view that chunks text into sentences
    transcription_chunks = pxt.create_view(
        f'{directory}.video_sentence_chunks',
        chunks_view,
        iterator=StringSplitter.create(text=chunks_view.transcription.text, separators='sentence'),
    )

    # Create embedding index for audio
    transcription_chunks.add_embedding_index('text', string_embed=EMBED_MODEL)
    ```
  </Step>

  <Step title="Use Your Workflow" icon="play">
    Create `app.py`:
    ```python
    from datetime import datetime
    import pixeltable as pxt

    # Constants
    directory = 'video_index'
    table_name = f'{directory}.video'

    # Connect to your tables and views
    video_index = pxt.get_table(table_name)
    frames_view = pxt.get_table(f'{directory}.video_frames')
    transcription_chunks = pxt.get_table(f'{directory}.video_sentence_chunks')

    # Insert videos to the knowledge base
    videos = [
        'https://github.com/pixeltable/pixeltable/raw/release/docs/resources/audio-transcription-demo/'
        f'Lex-Fridman-Podcast-430-Excerpt-{n}.mp4'
        for n in range(3)
    ]

    video_index.insert({'video': video, 'uploaded_at': datetime.now()} for video in videos[:2])

    query_text = "Summarize the conversation"

    audio_sim = transcription_chunks.text.similarity(query_text)
    audio_results = (
        transcription_chunks.order_by(audio_sim, transcription_chunks.uploaded_at, asc=False)
        .limit(5)
        .select(transcription_chunks.text, transcription_chunks.uploaded_at, similarity=audio_sim)
        .collect()
    )

    print(audio_results)
    ```
  </Step>
</Steps>

## What Makes This Different?

<CardGroup cols={1}>
  <Card title="True Multimodal Processing" icon="layer-group">
    Process both audio and visual content from the same videos:
    ```python
    # Extract audio from video
    video_index.add_computed_column(audio_extract=extract_audio(video_index.video, format='mp3'))
    
    # Extract frames from video
    frames_view = pxt.create_view(
        f'{directory}.video_frames',
        video_index,
        iterator=FrameIterator.create(
            video=video_index.video,
            fps=1
        )
    )
    
    # Optionally, create new videos from processed frames
    from pixeltable.functions.video import make_video
    processed_videos = frames_view.select(
        frames_view.video_id,
        make_video(frames_view.pos, frames_view.frame)  # Default fps is 25
    ).group_by(frames_view.video_id).collect()
    ```
  </Card>

  <Card title="AI-Powered Frame Analysis" icon="robot">
    Automatic image description using vision models:
    ```python
    frames_view.add_computed_column(
        image_description=vision(
            prompt="Provide quick caption for the image.",
            image=frames_view.frame,
            model="gpt-4o-mini"
        )
    )
    ```
  </Card>

  <Card title="Unified Embedding Space" icon="brain">
    Use the same embedding model for both text and image descriptions:
    ```python
    # Define once, use for both modalities
    EMBED_MODEL = sentence_transformer.using(model_id='intfloat/e5-large-v2')
    
    # Use for frame descriptions
    frames_view.add_embedding_index('image_description', string_embed=EMBED_MODEL)
    
    # Use for transcriptions
    transcription_chunks.add_embedding_index('text', string_embed=EMBED_MODEL)
    ```
  </Card>

  <Card title="Dual Search Capabilities" icon="magnifying-glass">
    Search independently across audio or visual content:
    ```python
    # Get similarity scores   
    audio_sim = transcription_chunks.text.similarity("Definition of happiness according to the guest")
    image_sim = frames_view.image_description.similarity("Lex Fridman interviewing a guest in a podcast setting")
    ```
  </Card>
</CardGroup>

## Workflow Components

<AccordionGroup>
  <Accordion title="Video Processing" icon="film">
    Extracts both audio and visual content:
    - Video file ingestion from URLs or local files
    - Automatic audio extraction with format selection
    - Frame extraction at configurable frame rates
    - Preserves timestamps for accurate retrieval
  </Accordion>

  <Accordion title="Visual Content Analysis" icon="image">
    Analyzes video frames with AI:
    - Extracts frames at 1 frame per second (configurable)
    - Generates natural language descriptions of each frame
    - Creates semantic embeddings of visual content
    - Enables search by visual concepts
  </Accordion>

  <Accordion title="Audio Processing" icon="waveform">
    Handles audio for efficient transcription:
    - Smart chunking to optimize transcription
    - Configurable chunk duration (30 sec default)
    - Overlap between chunks (2 sec default)
    - Minimum chunk threshold (5 sec default)
  </Accordion>

  <Accordion title="Speech-to-Text" icon="microphone">
    Uses OpenAI's Whisper for transcription:
    - High-quality speech recognition
    - Multiple language support
    - Sentence-level segmentation
    - Configurable model selection
  </Accordion>

  <Accordion title="Vector Search" icon="database">
    Implements unified embedding space:
    - Same embedding model for both modalities
    - High-quality E5 vector representations
    - Fast similarity search across content types
    - Configurable top-k retrieval
  </Accordion>
</AccordionGroup>
