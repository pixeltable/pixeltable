{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Transcribe audio files with Whisper\n",
        "\n",
        "Convert speech to text locally using OpenAI's open-source Whisper model—no API key needed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem\n",
        "\n",
        "You have audio or video files that need transcription. Long files are memory-intensive to process at once, so you need to split them into manageable chunks.\n",
        "\n",
        "| File | Duration | Challenge |\n",
        "|------|----------|-----------|\n",
        "| podcast.mp3 | 60 min | Too long to process at once |\n",
        "| interview.mp4 | 30 min | Need to extract audio first |\n",
        "| meeting.wav | 2 hours | Must chunk for memory efficiency |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solution\n",
        "\n",
        "**What's in this recipe:**\n",
        "- Transcribe audio files locally with Whisper (no API key)\n",
        "- Automatically chunk long files\n",
        "- Extract and transcribe audio from videos\n",
        "\n",
        "You create a view with AudioSplitter to break long files into chunks, then add a computed column for transcription. Whisper runs locally on your machine—no API calls needed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install -qU pixeltable openai-whisper\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pixeltable as pxt\n",
        "from pixeltable.iterators import AudioSplitter\n",
        "from pixeltable.functions import whisper\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load audio files\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created directory 'audio_demo'.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<pixeltable.catalog.dir.Dir at 0x37e827f20>"
            ]
          },
          "execution_count": 19,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create a fresh directory\n",
        "pxt.drop_dir('audio_demo', force=True)\n",
        "pxt.create_dir('audio_demo')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created table 'files'.\n"
          ]
        }
      ],
      "source": [
        "# Create table for audio files\n",
        "audio = pxt.create_table('audio_demo.files', {'audio': pxt.Audio})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inserting rows into `files`: 1 rows [00:00, 548.78 rows/s]\n",
            "Inserted 1 row with 0 errors.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "1 row inserted, 2 values computed."
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Insert a sample audio file (video files also work - audio is extracted automatically)\n",
        "audio.insert([\n",
        "    {'audio': 'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/audio-transcription-demo/Lex-Fridman-Podcast-430-Excerpt-0.mp4'}\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Split into chunks\n",
        "\n",
        "Create a view that splits audio into 30-second chunks with overlap:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Inserting rows into `chunks`: 2 rows [00:00, 909.04 rows/s]\n"
          ]
        }
      ],
      "source": [
        "# Split audio into chunks for transcription\n",
        "chunks = pxt.create_view(\n",
        "    'audio_demo.chunks',\n",
        "    audio,\n",
        "    iterator=AudioSplitter.create(\n",
        "        audio=audio.audio,\n",
        "        chunk_duration_sec=30.0,  # 30-second chunks\n",
        "        overlap_sec=2.0,          # 2-second overlap for context\n",
        "        min_chunk_duration_sec=5.0  # Drop chunks shorter than 5 seconds\n",
        "    )\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Created 2 chunks\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>start_time_sec</th>\n",
              "      <th>end_time_sec</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0.</td>\n",
              "      <td>30.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28.003</td>\n",
              "      <td>58.003</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "   start_time_sec  end_time_sec\n",
              "0          0.0000       30.0002\n",
              "1         28.0033       58.0034"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# View the chunks\n",
        "print(f\"Created {chunks.count()} chunks\")\n",
        "chunks.select(chunks.start_time_sec, chunks.end_time_sec).collect()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Transcribe with Whisper\n",
        "\n",
        "Add a computed column that transcribes each chunk:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/pierre/pixeltable/.venv/lib/python3.12/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n",
            "/Users/pierre/pixeltable/.venv/lib/python3.12/site-packages/whisper/transcribe.py:132: UserWarning: FP16 is not supported on CPU; using FP32 instead\n",
            "  warnings.warn(\"FP16 is not supported on CPU; using FP32 instead\")\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added 2 column values with 0 errors.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "2 rows updated, 2 values computed."
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Add transcription column (runs locally - no API key needed)\n",
        "chunks.add_computed_column(\n",
        "    transcription=whisper.transcribe(\n",
        "        audio=chunks.audio_chunk,\n",
        "        model='base.en'  # Options: tiny.en, base.en, small.en, medium.en, large\n",
        "    )\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Added 2 column values with 0 errors.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "2 rows updated, 2 values computed."
            ]
          },
          "execution_count": 25,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Extract just the text\n",
        "chunks.add_computed_column(text=chunks.transcription.text)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th>start_time_sec</th>\n",
              "      <th>end_time_sec</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0.</td>\n",
              "      <td>30.</td>\n",
              "      <td>of experiencing self versus remembering self. I was hoping you can give a simple answer of how we should live life. Based on the fact that our memories could be a source of happiness or could be the primary source of happiness, that an event when experienced bears its fruits the most when it&#x27;s remembered over and over and over and over.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>28.003</td>\n",
              "      <td>58.003</td>\n",
              "      <td>over and over and over and over and maybe there is some wisdom in the fact that we can control to some degree how we remember how we evolve our memory of it such that it can maximize the long-term happiness of that repeated experience. Okay, well first I&#x27;ll say I wish I could take you on the road with me. That was such a great description. Can I be your opening ax? Oh my God, no, I&#x27;m going to open for you dude. Otherwise it&#x27;s like, you know, everybody leaves.</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>"
            ],
            "text/plain": [
              "   start_time_sec  end_time_sec  \\\n",
              "0          0.0000       30.0002   \n",
              "1         28.0033       58.0034   \n",
              "\n",
              "                                                text  \n",
              "0   of experiencing self versus remembering self....  \n",
              "1   over and over and over and over and maybe the...  "
            ]
          },
          "execution_count": 26,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# View transcriptions with timestamps\n",
        "chunks.select(chunks.start_time_sec, chunks.end_time_sec, chunks.text).collect()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explanation\n",
        "\n",
        "**Whisper models:**\n",
        "\n",
        "| Model | Speed | Quality | Best for |\n",
        "|-------|-------|---------|----------|\n",
        "| `tiny.en` | Fastest | Basic | Quick tests |\n",
        "| `base.en` | Fast | Good | General use |\n",
        "| `small.en` | Medium | Better | Higher accuracy |\n",
        "| `medium.en` | Slow | Great | Professional quality |\n",
        "| `large` | Slowest | Best | Maximum accuracy |\n",
        "\n",
        "Models ending in `.en` are English-only and faster. Remove `.en` for multilingual support.\n",
        "\n",
        "**AudioSplitter parameters:**\n",
        "\n",
        "| Parameter | Description |\n",
        "|-----------|-------------|\n",
        "| `chunk_duration_sec` | Duration of each chunk in seconds |\n",
        "| `overlap_sec` | Overlap between chunks (helps with word boundaries) |\n",
        "| `min_chunk_duration_sec` | Drop the last chunk if shorter than this |\n",
        "\n",
        "**Video files work too:**\n",
        "\n",
        "When you insert a video file, Pixeltable automatically extracts the audio track.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## See also\n",
        "\n",
        "- [Iterators documentation](https://docs.pixeltable.com/datastore/iterators)\n",
        "- [Whisper library](https://github.com/openai/whisper)\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
