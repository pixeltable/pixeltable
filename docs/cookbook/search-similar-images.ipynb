{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Find similar images with CLIP\n",
        "\n",
        "Build visual similarity search to find images that look alike using OpenAI's CLIP model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem\n",
        "\n",
        "You have a collection of images and need to find visually similar onesâ€”for duplicate detection, content recommendations, or visual search.\n",
        "\n",
        "| Query | Expected matches |\n",
        "|-------|------------------|\n",
        "| sunset photo | Other sunset/beach images |\n",
        "| product image | Similar products |\n",
        "| user upload | Matching content in library |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solution\n",
        "\n",
        "**What's in this recipe:**\n",
        "- Create image embeddings with CLIP\n",
        "- Search by image similarity\n",
        "- Search by text description (cross-modal)\n",
        "\n",
        "You add an embedding index using CLIP, which understands both images and text. This enables finding similar images or searching images by text description.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -qU pixeltable sentence-transformers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pixeltable as pxt\n",
        "from pixeltable.functions.huggingface import clip\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load images\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a fresh directory\n",
        "pxt.drop_dir('image_search_demo', force=True)\n",
        "pxt.create_dir('image_search_demo')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "images = pxt.create_table('image_search_demo.images', {'image': pxt.Image})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Insert sample images\n",
        "images.insert([\n",
        "    {'image': 'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images/000000000036.jpg'},\n",
        "    {'image': 'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images/000000000090.jpg'},\n",
        "    {'image': 'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images/000000000106.jpg'},\n",
        "    {'image': 'https://raw.githubusercontent.com/pixeltable/pixeltable/main/docs/resources/images/000000000127.jpg'},\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create CLIP embedding index\n",
        "\n",
        "Add an embedding index using CLIP for cross-modal search:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Add CLIP embedding index (supports both image and text queries)\n",
        "images.add_embedding_index(\n",
        "    column='image',\n",
        "    image_embed=clip.using(model_id='openai/clip-vit-base-patch32'),\n",
        "    string_embed=clip.using(model_id='openai/clip-vit-base-patch32')\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Search by text description\n",
        "\n",
        "Find images matching a text query:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Search by text description\n",
        "query = \"people eating food\"\n",
        "sim = images.image.similarity(query)\n",
        "\n",
        "results = (\n",
        "    images\n",
        "    .order_by(sim, asc=False)\n",
        "    .select(images.image, score=sim)\n",
        "    .limit(2)\n",
        ")\n",
        "results.collect()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explanation\n",
        "\n",
        "**Why CLIP:**\n",
        "\n",
        "CLIP (Contrastive Language-Image Pre-training) understands both images and text in the same embedding space. This enables:\n",
        "- Image-to-image search (find similar photos)\n",
        "- Text-to-image search (find photos matching a description)\n",
        "\n",
        "**Index parameters:**\n",
        "\n",
        "| Parameter | Description |\n",
        "|-----------|-------------|\n",
        "| `image_embed` | Model for embedding images |\n",
        "| `string_embed` | Model for embedding text queries |\n",
        "\n",
        "**Both must use the same model** for cross-modal search to work.\n",
        "\n",
        "**New images are indexed automatically:**\n",
        "\n",
        "When you insert new images, embeddings are generated without extra code.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## See also\n",
        "\n",
        "- [Semantic text search](./search-semantic-text.ipynb)\n",
        "- [Vector database documentation](https://docs.pixeltable.com/datastore/vector-database)\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
