{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Summarize podcasts and audio\n",
        "\n",
        "Transcribe audio files and generate summaries automatically using Whisper and LLMs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Problem\n",
        "\n",
        "You have podcast episodes, meeting recordings, or interviews that need both transcription and summarization. Doing this manually is time-consuming and doesn't scale.\n",
        "\n",
        "| Content | Duration | Need |\n",
        "|---------|----------|------|\n",
        "| Podcast episodes | 60 min | Episode summary + key points |\n",
        "| Meeting recordings | 30 min | Action items + decisions |\n",
        "| Interviews | 45 min | Main topics + quotes |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Solution\n",
        "\n",
        "**What's in this recipe:**\n",
        "- Transcribe audio with Whisper (runs locally)\n",
        "- Generate summaries with an LLM\n",
        "- Chain transcription → summarization automatically\n",
        "\n",
        "You create a pipeline where audio is transcribed first, then the transcript is summarized. Both steps run automatically when you insert new audio files.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install -qU pixeltable openai-whisper openai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import getpass\n",
        "\n",
        "if 'OPENAI_API_KEY' not in os.environ:\n",
        "    os.environ['OPENAI_API_KEY'] = getpass.getpass('OpenAI API Key: ')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pixeltable as pxt\n",
        "from pixeltable.functions import whisper, openai\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a fresh directory\n",
        "pxt.drop_dir('podcast_demo', force=True)\n",
        "pxt.create_dir('podcast_demo')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create the pipeline\n",
        "\n",
        "Create a table with audio input, then add computed columns for transcription and summarization:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create table for audio files\n",
        "podcasts = pxt.create_table(\n",
        "    'podcast_demo.episodes',\n",
        "    {'title': pxt.String, 'audio': pxt.Audio}\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 1: Transcribe with local Whisper (uses GPU if available)\n",
        "podcasts.add_computed_column(\n",
        "    transcription=whisper.transcribe(podcasts.audio, model='base.en')\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract the text from transcription result\n",
        "podcasts.add_computed_column(\n",
        "    transcript_text=podcasts.transcription.text\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Step 2: Summarize the transcript with OpenAI\n",
        "summary_prompt = '''Summarize this transcript in 2-3 sentences, then list 3 key points.\n",
        "\n",
        "Transcript:\n",
        "''' + podcasts.transcript_text\n",
        "\n",
        "podcasts.add_computed_column(\n",
        "    summary_response=openai.chat_completions(\n",
        "        messages=[{'role': 'user', 'content': summary_prompt}],\n",
        "        model='gpt-4o-mini'\n",
        "    )\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Extract summary text from response\n",
        "podcasts.add_computed_column(\n",
        "    summary=podcasts.summary_response.choices[0].message.content\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Process audio files\n",
        "\n",
        "Insert audio files and watch the pipeline run automatically:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Insert sample audio (using a short sample for demo)\n",
        "audio_url = 'https://github.com/pixeltable/pixeltable/raw/main/docs/resources/audio/jfk_rice_moon_speech_excerpt.mp3'\n",
        "\n",
        "podcasts.insert([{\n",
        "    'title': 'JFK Moon Speech Excerpt',\n",
        "    'audio': audio_url\n",
        "}])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View transcript\n",
        "podcasts.select(podcasts.title, podcasts.transcript_text).collect()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# View summary\n",
        "for row in podcasts.select(podcasts.title, podcasts.summary).collect():\n",
        "    print(f\"Title: {row['title']}\")\n",
        "    print(f\"\\nSummary:\\n{row['summary']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Explanation\n",
        "\n",
        "**Pipeline architecture:**\n",
        "\n",
        "```\n",
        "Audio → Whisper transcription → Transcript text → LLM summarization → Summary\n",
        "```\n",
        "\n",
        "Each step is a computed column that depends on the previous one. When you insert a new audio file, all steps run automatically in sequence.\n",
        "\n",
        "**Whisper model options:**\n",
        "\n",
        "| Model | Size | Speed | Accuracy |\n",
        "|-------|------|-------|----------|\n",
        "| `tiny.en` | 39M | Fastest | Good for clear speech |\n",
        "| `base.en` | 74M | Fast | Balanced |\n",
        "| `small.en` | 244M | Medium | Better accuracy |\n",
        "| `medium.en` | 769M | Slow | High accuracy |\n",
        "\n",
        "For production with varied audio quality, use `small.en` or larger.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## See also\n",
        "\n",
        "- [Transcribe audio](./audio-transcribe.ipynb) - Basic audio transcription\n",
        "- [Summarize text](./text-summarize.ipynb) - Text summarization patterns\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
